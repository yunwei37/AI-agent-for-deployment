中文版翻译如下：

《Xen与虚拟化的艺术》
保罗·巴哈姆, 博里斯·德拉戈维奇, 凯尔·弗雷泽, 史蒂文·汉德, 蒂姆·哈里斯,
亚历克斯·霍, 罗尔夫·纽格鲍尔, 伊恩·普拉特, 安德鲁·沃菲尔德
剑桥大学计算机实验室
剑桥, 英国, CB3 0FD
firstname.lastname@cl.cam.ac.uk

摘要 1. 引言
众多系统已被设计利用虚拟化技术来细分现代计算机的放大资源。某些系统需要专用硬件，或无法支持普通的操作系统。一些系统以牺牲性能为代价，追求100%的二进制兼容性。其他系统则为提升速度牺牲安全性或功能性。很少有系统提供资源隔离或性能保证；大多数系统仅提供尽力而为的服务，这可能导致拒绝服务的风险。本文介绍了Xen，这是一个x86虚拟机监控器，允许多个普通操作系统安全且经过资源管理的共享传统硬件，同时不牺牲性能或功能。这是通过提供理想化的虚拟机抽象实现的，使得Linux、BSD和Windows XP等操作系统可以以最小的努力进行移植。

我们的设计旨在同时在现代服务器上托管多达100个虚拟机器实例。Xen采取的虚拟化方法非常高效：我们允许操作系统（如Linux和Windows XP）同时运行，且性能额外开销微乎其微——最多也仅为未经虚拟化情况下的几个百分点。我们在一系列微基准测试和系统范围的测试中显著超越了竞争的商业和免费可用解决方案。本文描述并评估的原型能够支持多个并发的XenoLinux客操作系统实例；每个实例导出与非虚拟化Linux 2.4相同的应用程序二进制接口。我们对Windows XP的移植尚未完成，但已经能够运行简单的用户空间进程。同时，NetBSD的移植工作也在进行中。

分类和主题描述
D.4.1 [操作系统]: 进程管理；D.4.2 [操作系统]: 存储管理；D.4.8 [操作系统]: 性能

一般术语
设计、测量、性能

关键词
虚拟机监控器、超级监视器、半虚拟化

有许多方法可以构建系统，以在共享机器上托管多个应用程序和服务器。或许最简单的方法是部署一个或多个运行标准操作系统（如Linux或Windows）的主机，然后允许用户安装文件并启动进程——应用程序之间的保护由传统操作系统技术提供。经验表明，由于声称不具有复杂配置相互作用，系统管理可能很快变成一项耗时的任务。更重要的是，此类系统并未充分支持性能隔离；一个进程的调度优先级、内存需求、网络流量和磁盘访问都会影响其他进程的性能。当存在足够的资源分配和闭合用户组（如在某些情况下）时，这可能是可以接受的。

授权在不收取费用的情况下使用本作品的全部或部分，用于个人或课堂用途，只要不制作或传播以获取商业利益，并且副本带有此通知和完整的引文。在其他情况下复制、重新出版、在服务器上发布或再分发到列表上，则需事先获得特定许可和/或支付费用。 

SOSP'03, 2003年10月19日至22日, 纽约州博尔顿登陆, 美国. 
版权所有2003年ACM💻1›58113›757›5/03/0010...$5.00.
在传统的虚拟机监控器（VMM）中，所暴露的虚拟硬件在功能上与底层机器完全相同[38]。尽管完全虚拟化显然允许未修改的操作系统被托管，但它也具有一些缺点。这一点尤其适用于x86架构，或者实验性平台PlanetLab[33]。然而，当资源被过度订阅或用户不合作时，有关全面虚拟化的其他论点也随之产生。

解决这一问题的一种方式是为操作系统回归支持性能隔离。这在资源容器[3]、Linux/RK[32]、QLinux[40]以及SILK[4]中已在不同程度上得到验证。此类方法的一个难点在于确保所有的资源使用都被准确归属到正确的进程——例如，考虑到由于缓冲缓存或页面替换算法导致的应用程序之间复杂的相互作用。这实质上是操作系统内部的“QoS串扰”问题[41]。在低层次上进行多路复用可以减轻该问题，正如Exokernel[23]和Nemesis[27]操作系统所展示的那样，意外或不必要的任务间的相互作用被最小化。

我们采用相同的基本方法构建Xen，它以整个操作系统的粒度对物理资源进行多路复用，并且能够提供各个操作系统之间的性能隔离。与进程级多路复用相比，这还允许多种客户操作系统优雅地共存，而非强制要求特定的应用程序二进制接口。对这种灵活性是需要付出代价的——运行完整的操作系统在初始化（例如，引导或恢复）和资源消耗方面，远比运行一个进程要更重。

针对我们目标的最多100个托管OS实例，我们相信这个代价是值得的；它允许个别用户以受控的方式运行未修改的二进制程序或二进制程序集合（例如，一个Apache服务器和一个PostgreSQL后端）。此外，它还提供了非常高的灵活性，因为用户可以根据其软件所需动态创建精确的执行环境。不同服务和应用程序之间的不可预见的配置交互被避免（例如，每个Windows实例都有自己的注册表）。

本文的其余部分结构如下：在第二节中，我们解释了我们对虚拟化的看法，并概述了Xen的工作原理。第三节描述了我们设计和实现的关键方面。第四节使用行业标准基准评估XenoLinux在与独立Linux、VMware Workstation和用户模式Linux（UML）比较中的性能。第五节回顾相关工作，最后在第六节中讨论未来的工作并进行总结。鉴于这些截然不同的目标，对Denali设计选择与我们自己原则之间的对比是极具启发性的。

2. Xen：方法与概述

Denali首先不定位于现有的应用程序二进制接口（ABI），因此可以从其虚拟机界面中省略某些架构特性。例如，虽然Denali在NetBSD、Linux和Windows XP的ABI中被导出且广泛使用，但它并不完全支持x86分段。

其次，Denali的实现未解决某些关键问题，使其设计选择与我们建立的原则大相径庭。例如，Denali的实现没有考虑如何处理复杂的服务器配置，因此它的设计旨在支持数千个运行网络服务的虚拟机，其中绝大多数是小规模且不流行的应用。然而，Xen的目标是扩展到大约100个运行行业标准应用程序和服务的虚拟机。
以下是您提供文本的中文翻译，保持学术风格和技术术语：

对于普遍的 IA-32 或 x86 架构。支持应用多路复用或在单个客户操作系统内的多个地址空间的问题并不属于 x86 架构设计的范畴。某些监控指令必须由虚拟机监控器（VMM）处理，以实现正确的虚拟化，但以权限不足的方式执行这些指令会静默失败，而不是引起便利的异常。因此，每个虚拟机本质上都是单用户单应用程序保护操作系统。在 Xen 中，相比之下，单个虚拟机托管一个真实的操作系统，该系统本身可以安全地多路复用成千上万的经过修改的用户级进程。尽管已经开发出一种原型虚拟内存管理单元（MMU），可能对 Denali 在这方面有所帮助，但我们没有看到任何已发布的技术细节或评估。

第三，在 Denali 架构中，VMM 处理所有与磁盘的分页。这可能与虚拟化层缺乏内存管理支持有关。内存分页内部更新的成本较高，因此在对系统结构（如页表）的管理上，ESX Server 实现了系统结构的影子版本并通过捕获每次更新尝试来维护与虚拟表的一致性。来宾操作系统对硬件页表有直接读访问，但更新被批处理并由虚拟机监控器验证。一个域可能被分配不连续的机器页。 

操作系统保护来宾操作系统必须在低于 Xen 的权限级别下运行。来宾操作系统必须为异常处理程序向 Xen 注册描述符表。除了页面错误处理程序之外，处理程序保持不变。来宾操作系统可以为系统调用安装“快速”处理程序，允许应用程序直接调用其来宾操作系统，从而避免通过 Xen 间接调用每个系统调用。硬件中断被替换为轻量级事件系统。每个来宾操作系统都有一个定时器接口，并且能识别“真实”和“虚拟”时间。 

关于设备 I/O，网络、磁盘等虚拟设备访问优雅且简单。数据使用异步 I/O 环环进行传输。事件机制取代硬件中断以进行通知。

表1：半虚拟化 x86 接口。性能隔离的目标与恶意来宾操作系统相悖：恶意虚拟机可能会促使抖动行为，通常会不公平地剥夺其他虚拟机的 CPU 时间和磁盘带宽。在 Xen 中，我们期望每个来宾操作系统使用其自身的内存保留和磁盘分配（这一点早先被自分页思想采纳）来执行其自身的分页。

最后，Denali 虚拟化了所有机器资源的“命名空间”，认为如果虚拟机无法命名资源分配，则不能访问另一虚拟机的资源（例如，虚拟机对硬件地址毫不知情，仅知道由 Denali 为其创建的虚拟地址）。相反，我们认为在虚拟机监控器内进行安全访问控制就足以确保保护；此外，如前所述，直接让物理资源对来宾操作系统可见在正确性和性能上都有强有力的论证。

在接下来的部分中，我们描述由 Xen 导出的虚拟机抽象，并讨论了来宾操作系统必须如何进行修改以符合这一要求。请注意，在本文中我们保留“来宾操作系统”这一术语。
以下是将您提供的文本翻译成中文的版本：

参考Xen可以托管的操作系统之一，并且存在于每个地址空间顶部的64MB区域，因此我们使用术语“域”来指代正在运行的虚拟机，以避免在进入和离开虚拟机监控器时发生TLB失效。每当一个客户操作系统需要新的页表，比如因为一个新进程正在被创建时，它将从自己的内存保留区域分配并初始化一个页面，并将其注册到Xen。此时操作系统必须放弃对页表内存的直接写入权限：所有后续更新必须经过Xen的验证。这在多方面限制了更新，包括仅允许操作系统映射其拥有的页面，并且不允许对页表的可写映射。客户操作系统可以批处理更新请求，以摊薄进入虚拟机监控器的开销。每个地址空间的顶部64MB区域是为Xen保留的，客户操作系统无法访问或重新映射。尽管我们的实现中某些部分，如内存管理，特定于x86，但许多方面（如我们的虚拟CPU和I/O设备）可以轻松应用于其他机器架构。此外，x86在其与RISC风格处理器显著不同的领域代表了一个最坏情况——例如，有效地虚拟化硬件页表比虚拟化软件管理的TLB要困难得多。

2.1 虚拟机接口
表1概述了半虚拟化x86接口，分为系统的三个广泛方面：内存管理、CPU和设备I/O。在下面的讨论中，我们逐一解决每个机器子系统，并讨论我们如何在我们的半虚拟化架构中呈现每个子系统。需要说明的是，尽管我们的实现中某些部分特定于x86架构，许多机制（例如，虚拟CPU和I/O设备）也可用于其他机器架构。进一步补充的是，x86在其与RISC风格处理器显著不同的领域代表了一个最坏情况——例如，有效地虚拟化硬件页表比虚拟化软件管理的TLB要困难得多。

2.1.1 内存管理
虚拟化内存无疑是半虚拟化架构中最困难的部分，包括虚拟机监控器中的机制以及对每个客户操作系统的修改。首先，虚拟机监控器的插入使其处于操作系统之下，这违反了操作系统是系统中最特权实体的通常假设。为了保护虚拟机监控器免受操作系统的不当行为（以及域之间的相互影响），客户操作系统必须被修改为在较低权限级别运行。许多处理器架构仅提供两个权限级别。在这些情况下，客户操作系统将与应用程序共享较低的权限级别。客户操作系统将通过在与应用程序分离的地址空间中运行来保护自己，并通过虚拟机监控器间接传递控制权，以设置虚拟权限级别并更改当前地址空间。如果处理器的TLB支持地址空间标签，那么可以避免昂贵的TLB失效。有效的权限级别虚拟化在x86上是可能的，因为它在硬件中支持四个不同的权限级别。x86的权限级别通常描述为环，从零（特权最高）到三（特权最低）。操作系统代码通常在环0中执行，因为其他环无法执行特权指令，而环3通常用于应用程序代码。就我们所知，环1和环2自OS/2以来未被任何著名的x86操作系统使用。任何遵循这一常见安排的操作系统都可以通过修改其以在环1中执行，从而移植到Xen。这防止了客户操作系统直接执行特权指令，同时使其安全地与在环3中运行的应用程序隔离。为确保在目标地址的安全性，特权指令通过要求在Xen内进行验证和执行来进行半虚拟化——这适用于操作。
在安装新的页表或在操作系统（OS）系统调用时放弃处理器时，故障以常规方式进行虚拟化。任何来宾操作系统直接执行特权指令的尝试都将被处理器失败，无论是静默地还是通过引发故障，因为只有 Xen 在足够特权的级别下执行。 

**2.1.3 设备I/O**

在x86架构上，内存故障和软件陷阱等异常的虚拟化非常直接。Xen 注册了描述每种异常类型处理程序的表格以供验证。此表中指定的处理程序通常与真实的 x86 硬件相同；这是可能的，因为异常堆栈帧在我们的半虚拟化架构中未被修改。唯一的修改是页故障处理程序，正常情况下该处理程序会从特权处理器寄存器（CR2）读取故障地址；由于无法访问该寄存器，我们将其写入扩展堆栈帧。发生异常时，如果在执行外圈0时，Xen 的处理程序会在来宾操作系统的堆栈上创建异常堆栈帧的副本，并将控制权返回给相应的注册处理程序。

通常，只有两种类型的异常足够频繁地发生，以影响系统性能：系统调用（通常通过软件异常实现）和页故障。我们通过允许每个来宾操作系统注册一个可由处理器直接访问的“快速”异常处理程序，从而提高了系统调用的性能，而无需通过外圈0进行间接访问；该处理程序在将其安装到硬件异常表格中之前经过验证。不幸的是，无法对页故障处理程序应用同样的技术，因为只有外圈0中执行的代码才能从寄存器CR2读取故障地址；因此，页故障必须始终通过Xen传递，以便此寄存器值可以在外圈1中保存。

通过验证在提交给Xen时的异常处理程序来确保安全。唯一需要检查的是处理程序的代码段不指定在外圈0中执行。由于没有来宾操作系统可以创建这样的段，因此可以将指定的段选择器与由Xen保留的少量静态值进行比较。除此之外，任何其他处理程序问题都在异常传播期间解决——例如，如果处理程序的代码在处理期间未正确执行，相关的处理逻辑将得到修正。

表2展示了将商品操作系统迁移到Xen的半虚拟化x86环境所需的成本（代码行数）。请注意，我们的NetBSD移植处于早期阶段，因此未给出数字。XP移植更为先进，但仍在进展中；它能够从RAM磁盘执行多个用户空间应用程序，但目前缺乏任何虚拟I/O驱动程序。因此，不提供XP虚拟设备驱动程序的数字。和Linux一样，我们预计这些驱动程序由于Xen提供的理想化硬件抽象，将是小型且简单的。

Windows XP需要对其架构独立的操作系统代码进行意外数量的修改，因为它使用了多种结构和联合来访问页表项（PTE）。每个页表访问都必须单独修改，尽管可以创建某些只读VBD，或者VIF可以过滤IP数据包以防止源地址欺骗。

**控制接口**

该控制接口以及关于系统当前状态的配置统计信息被导出到运行于Domain0的应用级管理软件套件中。这组管理工具允许方便地管理整个服务器：当前工具可以创建和销毁域、设置网络过滤器和路由规则、监控每个域的网络活动等。
驱动程序的分组和流粒度，并创建和删除虚拟网络接口及虚拟块设备。我们预计将开发更高级别的工具，以进一步自动化管理政策的应用。

硬件（SMP x86，物理内存，网络，以太网，SCSI/IDE）

3. 详细设计

图1：运行Xen虚拟机监控器的机器结构，宿主多个不同的来宾操作系统，包括在XenoLinux环境中运行控制软件的Domain0。为了清晰的说明，我们在每种情况下同时介绍Xen和来宾操作系统的功能。目前对来宾操作系统的讨论集中在XenoLinux上，因为这是最成熟的；尽管如此，我们正在进行的Windows XP和NetBSD的移植工作让我们对Xen的来宾操作系统无关的能力充满信心。

该过程是通过脚本自动化的。相比之下，Linux需要对其通用内存系统进行的修改要少得多，因其使用预处理器宏来访问页表项（PTE）——宏定义为添加翻译和虚拟机监控器调用提供了一个便利的地方，以满足半虚拟化的需求。

在这两个操作系统中，架构特定的部分实际上是x86代码向我们半虚拟化架构的移植。这涉及对使用特权指令的例程进行重写，并移除大量低级系统初始化代码。Windows XP需要更多的修改，主要是由于存在遗留的16位仿真代码以及对略微不同的引导加载机制的需求。请注意，Windows XP中的x86特定代码库显著大于Linux，因此应预期有更大的移植工作量。

3.1 控制转移：超调用和事件

对于Xen与上层域之间的控制交互，有两种机制存在：来自域到Xen的同步调用可以通过超调用进行，而通知则通过异步事件机制从Xen传递给域。

超调用接口允许域表现为异步软件陷阱到虚拟机监控器，以执行特权操作，类似于传统操作系统中的系统调用。超调用的一个示例是请求一组页表更新，其中Xen验证和应用更新列表，完成后将控制权返回给调用域。

Xen向域的通信是通过一种异步事件机制提供的，这种机制取代了传统的设备中断交付机制，允许对重要事件（如域终止请求）进行轻量级通知。类似于传统Unix信号，这里仅有少量事件，每个事件都标记特定类型的发生。例如，事件用于指示新数据已经通过网络接收，或虚拟磁盘请求已完成。

待处理事件存储在每个域的位掩码中，由Xen在调用由来宾操作系统指定的事件回调处理程序之前进行更新。回调处理程序负责重置待处理事件集，并以适当的方式响应通知。

一个域可以通过设置Xen可读的软件标志来显式推迟事件处理，这类似于在真实处理器上禁用中断。

3.2 数据传输：I/O环

随着Xen的设计和实现，遵循的一个目标是尽可能将策略与机制分离。尽管虚拟机监控器必须参与数据路径方面（例如，在域之间调度CPU、在传输前过滤网络数据包，或在读取数据块时实施访问控制），但并不需要它参与，甚至需要意识到更高层级问题，例如如何共享CPU或各个域可以传输哪种类型的数据包。

最终形成的架构是一种虚拟机监控器本身仅提供基本控制操作的架构。这些通过可由授权域访问的接口进行导出；潜在的复杂政策决策，如入境控制，最好是由在来宾操作系统上运行的管理软件来执行。虚拟机监控器的存在意味着在来宾操作系统和I/O设备之间存在额外的保护域，因此至关重要的是提供一种数据传输机制，允许数据以尽可能少的开销竖直移动通过系统。

我们的I/O传输机制设计受到两个主要因素的影响：资源管理和事件通知。为了资源责任追溯，我们努力最大限度减少在从设备收到中断时将数据多路复用到特定域所需的工作。
在管理缓冲区的背景下，本研究除了处理器和内存资源外，控制接口还支持虚拟网络接口（VIF）和块设备（VBD）的创建与删除。这些虚拟 I/O 设备具有关联的访问控制信息，确定哪些域可以访问它们及其访问限制（例如，性能要求可以通过在 Xen 中固定底层页面框架来实现）。BVT 的设计旨在利用虚拟时间扭曲技术，以促进最近唤醒的域，从而提供低延迟的调度服务。然而，其他调度算法可以很容易地在我们通用调度器的抽象上进行实现。每个域的调度参数可以由运行在 Domain 0 中的管理软件进行调整。

在 Xen 中，提供了对实时、虚拟时间以及壁钟时间的概念。实时通过自机器启动以来经过的纳秒数来表示，并且精确到处理器的周期计数器，还可以通过外部时间源（例如通过 NTP）进行频率锁定。一个域的虚拟时间仅在执行时前进，这通常由来宾操作系统调度器使用，以确保应用进程之间的时间片的正确共享。最后，壁钟时间被指定为添加到当前实时的偏移量，这允许在不影响实时进展的情况下调整壁钟时间。

图 2 显示了我们 I/O 描述符环的结构。环是由域分配的圆形描述符队列，但可以在 Xen 内部访问。描述符不直接包含 I/O 数据；相反，I/O 数据缓冲区由来宾操作系统在带外分配，并通过 I/O 描述符间接引用。访问每个环的机制基于两组生产者-消费者指针：域将请求放置在环上，推进请求生产者指针，Xen 则移除这些请求进行处理，推进相关的请求消费者指针。响应以类似方式放回环中，Xen 作为生产者，来宾操作系统作为消费者。请求的处理不需要按序进行：来宾操作系统为每个请求关联一个唯一标识符，该标识符在关联的响应中再现。这使 Xen 能够因调度或优先级考虑而不明确地重排 I/O 操作。

这种结构足够通用，能够支持多种不同的设备范式。例如，一组“请求”可以为网络数据包接收提供缓冲区；随后“响应”则信号这些数据包的到达。处理磁盘请求时，重排操作尤其有用，因为这有助于提高 Xen 内部的调度效率，而使用带外缓冲区的描述符则使得实现零拷贝传输变得简单。我们将请求或响应的生成与另一方的通知解耦：在请求的情况下，一个域可以在调用超调用以警示 Xen 之前排队多个条目；在响应的情况下，域则可以延迟交付响应。

与全虚拟化强制使用阴影页表以给出连续物理内存的幻觉不同，Xen 在某种程度上不那么受限。实际上，Xen 只需在页表更新时介入，以防止来宾操作系统做出不可接受的更改。因此，我们避免了与阴影页表使用相关的开销和额外复杂性。
在 Xen 中的通知事件的处理方法是通过指定响应的阈值数量来实现的。这允许每个域在延迟与吞吐量需求之间进行权衡，类似于 ArseNIC 千兆以太网接口中的流感知中断调度 [34]。为了确保安全，更新请求在应用之前需要进行验证。

为帮助验证，我们为每个机器页面框分配一个类型和引用计数。一个框架在任何时间点只能具有以下互斥类型之一：页面目录（PD）、页表（PT）、本地描述符表（LDT）、全局描述符表（GDT）或可写（RW）。请注意，来宾操作系统可以始终创建对其自身页面框的可读映射，无论其当前类型如何。只有当其引用计数为零时，才能安全地重新分配一个框架。该机制用于维护安全所需的不变性；例如，一个域不能对页表的任何部分具有可写映射，因为这将要求相关框架同时为 PT 和 RW 类型。

类型系统也被用于跟踪哪些框架已经被验证用于页表。为此，来宾操作系统需要指示何时分配框架用于页表使用——这需要 Xen 对框架中的每个条目进行一次性验证，此后其类型将被固定为 PD 或 PT，直至来自来宾操作系统的后续解除固定请求。

Xen 当前按照借用虚拟时间（BVT）调度算法 [11] 调度域。我们选择这种特定算法，因为它既节约工作，又具有低延迟唤醒（或调度）域的特殊机制，当域接收到事件时，快速调度尤为重要，以最小化虚拟化对设计为及时运行的操作系统子系统的影响；例如，TCP 依赖于对框架中每个条目的单次验证，之后其类型被固定为 PD 或 PT，合适时直至来宾操作系统的后续解除固定请求。这在更改页表基指针时尤其有用，因为它避免了在每次上下文切换时验证新的页表的需要。

在网络方面，Xen 提供了虚拟防火墙路由器（VFR）的抽象，每个域都有一个或多个逻辑上附加到 VFR 的网络接口（VIFs）。一个 VIF 看起来有些像现代网络接口卡：有两个输入/输出环用于缓存描述符，一个用于传输，一个用于接收。每个方向也都有一个相关规则列表，形式为（<模式>，<动作>）——如果模式匹配，则应用相关动作。域 0 负责插入和移除规则。在典型情况下，将安装规则以防止 IP 源地址欺骗，并确保基于目标 IP 地址和端口的正确解复用。规则也可以与 VFR 上的硬件接口相关联。特别地，我们可以安装规则以执行传统的防火墙功能，例如阻止在不安全端口上的传入连接尝试。

为了传输数据包，来宾操作系统只需将一个缓冲区描述符排入传输环。Xen 复制描述符，并为了确保安全，随后复制数据包头并执行任何匹配的过滤规则。数据包有效负载不会被复制，因为我们使用了散射-聚集直接内存访问（DMA）；但是请注意，相关页面框必须在传输完成之前保持固定。为了确保公平性，Xen 实现了一个简单的轮询时间片调度程序。

初始内存分配或保留在每个域创建时指定；因此，内存在域之间静态分区，提供强隔离。也可以指定最大可允许保留：如果某域内的内存压力增加，则可以尝试...（文本未完待续）
The translation of the provided technical text into Chinese, preserving its academic style and terminology, is as follows:

pttoclaimadditional 及时检查接收规则的集合，以确定从Xen到达此预留限制的目标内存页面。相反，VIF， 并在释放时交换数据包缓冲区和页面帧，如果域希望节省资源，可能是为了避免不必要的接收成本。如果没有可用的帧，则数据包将被丢弃。

3.3.6 磁盘
XenoLinux 实现了一个气球驱动程序 [42]，它通过在Xen和XenoLinux的页面分配器之间反复转移内存页面，来调整域的内存使用情况。尽管我们可以直接修改Linux的内存管理例程，但气球驱动程序通过利用现有的操作系统功能来进行调整，从而简化了Linux的移植工作。然而，半虚拟化可以用于扩展气球驱动程序的功能；例如，访客操作系统中的内存不足处理机制可以被修改，以通过向Xen请求更多内存来自动缓解内存压力。大多数操作系统假定内存最多由几个大块连续的扩展组成。由于Xen不保证分配连续的内存区域，访客操作系统通常会为自己创建一个连续物理内存的幻觉，即使它们底层的硬件内存分配是稀疏的。从物理地址映射到硬件地址完全是访客操作系统的责任，后者可以简单地维护一个由物理页面帧号索引的数组。Xen通过提供一个对所有域都可直接读取的共享转换数组，以支持高效的硬件到物理映射——对此数组的更新由Xen进行验证，以确保相关操作系统拥有必要的硬件页面帧。

请注意，即使访客操作系统选择忽略硬件地址，在大多数情况下，它在访问其页面表时必须使用转换表（这些表必然使用硬件地址）。硬件地址也可能被暴露给操作系统内存管理系统的有限部分，以优化内存访问。例如，访客操作系统可能分配特定的硬件页面，以便在物理索引缓存 [24] 中优化位置，或自然地对齐硬件内存的连续部分，使用超级页面 [30]。Xen以简单的轮询方式批处理来自竞争域的请求；这些请求随后被传递给标准提升调度器，然后才到达磁盘硬件。域可以显式地传递重新排序屏障，以防在必要时维护更高层次的语义（例如在使用写前日志时）。低级调度为我们提供了良好的吞吐量，而请求的批处理提供了合理公正的访问。未来的工作将研究提供更可预测的隔离和差异化服务，也许使用现有的技术和调度器 [39]。

3.4 构建新域
为新域构建初始访客操作系统结构的任务主要委托给Domain0，其通过使用特权控制接口（第2.3节）访问新域的内存并通知Xen有关初始寄存器状态。这种方法与其他方法相比具有多个优点。
在本节中，我们对Xen进行了全面的性能评估。我们首先对Xen进行了基准测试，与多种替代虚拟化技术进行比较，然后比较在单一本地操作系统上同时运行多个应用程序的总系统吞吐量与在各自虚拟机中运行每个应用程序的情况。接着，我们评估了Xen在客操作系统之间的性能隔离，并评估了在同一硬件上运行大量操作系统的总开销。

所有实验均在Dell 2650双处理器2.4GHz Xeon服务器上完成，配备2GB RAM、Broadcom Tigon 3 Gigabit以太网网卡，以及单个Hitachi DK32EJ 146GB 10k RPM SCSI硬盘。整个过程中使用了Linux 2.4.21版本，针对本地和VMware客操作系统进行了i686架构的编译，针对在Xen上运行时的xeno-i686编译，以及在UML上运行时的um架构编译。机器中的Xeon处理器支持超线程（“超线程”），但由于当前没有任何内核支持超线程感知调度程序，因此此功能被禁用。我们确保所有客操作系统和它们的虚拟机监控器(VMM)可用的内存总量等于本地Linux可用的内存总量。整个过程中使用了Red Hat 7.2发行版，安装在ext3文件系统上。虚拟机被配置为在“持久原始模式”下使用相同的磁盘分区，这样获得了最佳性能。使用相同的文件系统映像也消除了潜在的磁盘寻址时间和传输速率差异。

在这些测量中，我们使用了我们的XenoLinux移植（基于Linux 2.4.21），因为这是我们最成熟的客操作系统。我们预计Windows XP和NetBSD移植的相对开销会相似，但尚未进行完整的评估。当前有若干现有解决方案能够在同一台机器上运行多个Linux副本。VMware提供了若干商业产品，可以在不修改的Linux副本上启动虚拟x86机器。最常用的版本是VMware Workstation，由一组对“主机”操作系统的特权内核扩展组成。它支持Windows和Linux主机。VMware还提供了一种名为ESX Server的增强产品，它用专用内核替换了主机操作系统。通过这样做，它相较于工作站产品获得了一些性能提升。ESX Server还支持一种可以通过在客操作系统中安装特殊设备驱动程序（vmxnet）访问网络的半虚拟化接口，其中允许的部署环境支持这种接口。

我们对ESX Server进行了上述基准测试套件的评估，但由于产品最终用户许可协议的条款限制，遗憾的是无法报告定量结果。相反，我们呈现...
以下是翻译后的文本，保持了学术风格和技术术语：

来自VMware Workstation 3.2的结果，运行和编译器质量。该测试套件进行的I/O操作较少，并且对Linux宿主操作系统的交互也很有限，因为这是最新的VMware产品。几乎所有CPU时间都用于执行无该基准公开限制的用户空间代码，所有三个虚拟机监控程序（VMM）均表现出低开销。相对于其本地架构，ESX Server在构建默认配置的Linux 2.4.21内核时，也能与VMware Workstation及其托管架构相等或超越其性能，而Xen则要求客体操作系统的移植，但利用半虚拟化显著超越ESX Server。

在这种情况下，原生Linux在操作系统中大约耗费7%的CPU时间，主要用于文件I/O、调度和内存管理。对于VMM而言，这种“系统时间”必须在超过320Kb/s的聚合带宽下进行处理，且请求的序列会被扩展到不同程度：而Xen则只产生约3%的开销，其余VMM则面临更显著的减速。一个预热阶段被允许，在阶段内，客户端的数量被逐渐增加，从而允许服务器预加载其缓存。

我们进行了两个实验，使用了PostgreSQL 7.1.3数据库，借助于开源数据库基准测试套件（OSDB）在其默认配置中进行考察。我们展示了多用户信息检索（IR）和在线事务处理（OLTP）工作负载的结果，均以每秒元组数（tup/s）进行测量。对套件的测试工具进行了一些小修改，这是由于UML的一个漏洞，该漏洞在高负载下丢失了虚拟定时器中断，导致无法产生正确的结果。基准测试通过Unix域套接字上的PostgreSQL原生API（可调用SQL）驱动数据库。PostgreSQL对操作系统施加了相当大的负载，这在VMware和UML所经历的显著虚拟化开销中得到了体现。特别是OLTP基准测试需要进行许多同步磁盘操作，导致了多个保护域的频繁转换。

dbench程序是一个文件系统基准测试，来源于行业标准的“NetBench”。它模拟了Windows 95客户端对文件服务器施加的负载。在这里，我们检查了单个客户端处理大约90,000个文件系统操作的吞吐量。XenoLinux表现良好，其性能在1%以内接近于原生Linux。VMware和UML则均显得乏力，支持的客户端数量不到原生Linux系统的三分之一。

SPECWEB99是一个复杂的应用级基准测试，用于评估Web服务器及其所承载的系统。工作负载是页面请求的复杂组合：30%需要动态内容生成，16%是HTTP POST操作，0.5%执行CGI脚本。随着服务器的运行，它生成访问和POST日志，因此磁盘工作负载并非完全是只读的。因此，测量结果反映了一般的操作系统性能，包括文件系统和网络性能，除了Web服务器本身外，我们还进行了许多小规模实验，以更精确地测量Xen及其他VMM中的开销。

我们使用了多台客户端机器来生成对待测服务器的负载，每台机器模拟一组用户同时访问该网站。基准测试在不同数量的模拟用户下反复运行，以确定开销面积。在虚拟化开销的测量中，我们使用了McVoy的lmbench程序。我们使用了版本3.0-a3，因为该版本解决了Seltzer的hbench所提出的工具精确度问题。OS性能的子集被系统评估。
mbenchsuite由37个微基准组成，这是可以支持的最大数量。SPECWEB99定义了需要模拟用户必须接收的最低服务质量，以使其为“合规”，从而计入得分：在许多情况下，用户会被SMP系统中额外锁定所带来的性能开销所惊讶。

|          | L-SMP  | L-UP  | Xen     | VMW      | UML        |
|----------|--------|-------|---------|----------|------------|
| 0K       | 0.53   | 0.45  | 0.46    | 0.73     | 24.7       |
| 16K      | 0.81   | 0.50  | 0.50    | 0.83     | 25.1       |
| 64K      | 2.10   | 1.28  | 1.22    | 1.88     | 36.1       |
| 16K      | 3.51   | 1.92  | 1.88    | 2.99     | 62.8       |
| 64K      | 23.2   | 5.70  | 5.69    | 11.1     | 39.9       |
| 0.83     | 0.68   | 0.69  | 1.02    | 1.00     | 26.0       |
| 2.94     | 2.49   | 1.75  | 4.63    | 4.00     | 46.0       |
| 143      | 110    | 198   | 874     | 21k      | 33k        |
| 601      | 530    | 768   | 2k3     | 10k      |            |
| 4k2      | 4k0    | 4k8   | 291(-68%)| 615(-31%)| 130(-82%)  |
| 897      | 897    | 516(-14%) | 467(-14%)   | 65.7   |            |

表6：ttcp：带宽（Mb/s）

在原生Linux情况下，我们展示了单处理器（L-UP）和对称多处理（L-SMP）内核的图形，因为我们在许多情况下对SMP系统的额外锁定的性能开销有所惊讶。

为了评估虚拟化网络的开销，我们在千兆以太网局域网中检验TCP性能。在所有实验中，我们使用配置相似的运行本地Linux的SMP机器作为一个端点。这使我们能够独立地测量接收和传输性能。ttcp基准被用来执行这些测量。发送方和接收方应用程序均配置为128kB的套接字缓冲区，因为我们发现这为所有测试系统提供了最佳性能。结果提供的是在9次实验中转移400MB数据的中位数。

表6展示了两组结果，一组使用默认的以太网MTU值1500字节，另一组使用500字节的MTU（通常由拨号PPP客户端使用）。结果表明，XenoLinux虚拟网络驱动程序采用的页面切换技术避免了数据复制的开销，因此实现了每字节的开销极低。当MTU为500字节时，包开销占主导地位。传输分包和接收去复用的额外复杂性对吞吐量产生不利影响，但只有约14%。

在37个微基准中的24个中，XenoLinux的性能与原生Linux类似，紧密追踪单处理器Linux内核的性能，并超越SMP内核。在表3至表5中，我们展示了表现出有趣性能变化的结果；对Xen的明显较大惩罚以粗体显示。

在微基准的处理过程（表3）中，Xen的fork、exec和sh性能低于原生Linux是可以预期的，因为这些操作需要大量的页表更新，这些更新都必须经过Xen的验证。然而，半虚拟化的方法允许XenoLinux批量处理更新请求。创建新的页表呈现出理想情况：由于没有理由提前提交待处理更新，XenoLinux可以在2048个更新（其批量缓冲区的最大尺寸）之间分摊每个超调用。因此，每个更新超调用构建8MB的地址空间。

表4展示了不同工作集大小的不同数量进程之间的上下文切换时间。Xen在切换页表基址时引入1微秒至3微秒的额外开销。然而，较大工作集大小（可能更能代表真实应用）所显示的上下文切换结果表明，与缓存效果相比，这一开销是微小的。

在本节中，我们比较在各自的客操作系统中运行多个应用程序的性能，与在同一原生操作系统中运行它们的性能相对比。我们的重点是Xen的结果，但在适用的情况下，我们也评论其他虚拟机监控器（VMM）的性能结果。图4显示了在两台CPU机器上并行运行1、2、4、8和16份SPEC WEB99基准的结果。原生Linux配置为SMP；在其上我们运行了多个Apache实例作为并发进程。在Xen的情况中，每个SPEC WEB99实例都在其自己的单处理器Linux客操作系统中运行（同时还有一个sshd和其他管理进程）。每个Web服务器使用不同的TCP端口号，以便能够并行运行副本。请注意，
以下是该文本的中文翻译，保持了学术风格和技术术语：

---

不寻常的是，VMware Workstation 在这些必需的 SPEC 数据集上，对于同时连接的数量而言，较 UML 的性能要逊色（c 0:66），约为 4.88MBytes 或 1000 个连接大约为 3.3GB。这对全面评估磁盘和缓冲缓存子系统来说是足够大的。

表 5 中显示的 mmap 延迟和缺页故障延迟结果相当有趣，因为它们需要进入 Xen 页的两次转换：第一次是处理硬件故障并将细节传递给客户操作系统，第二次是在客户操作系统上安装更新的页表条目。尽管如此，开销仍然相对适中。表 3 中的一小异常是，XenoLinux 的信号处理延迟低于本地 Linux。该基准测试根本不需要调用 Xen，而 0.75 秒（30%）的加速是预期的。

在单个 Apache 实例的情况下，增加第二个 CPU 可以使本地 Linux 在第 4.1 节报告的结果上提高 28%，达到 662 个符合标准的客户端。然而，最佳的总吞吐量是在运行两个 Apache 实例时达到的，这表明 Apache 1.3.27 可能存在一些对称多处理（SMP）可扩展性问题。

在运行单一域时，Xen 受限于缺乏对 SMP 客户操作系统的支持。然而，Xen 的中断负载均衡器识别出闲置 CPU，并将所有中断处理转移到它上面，使单 CPU 的评分提高了 9%。随着域数的增加，Xen 的性能在一定程度上接近本地情况。

接下来，我们进行了多实例的 PostgreSQL 运行实验，使用 OSDB 套件进行测试。在单个 Linux 操作系统上运行多个 PostgreSQL 实例证明困难，因为通常需要运行单个 PostgreSQL 实例以支持多个数据库。然而，这会阻止不同用户拥有独立的数据库配置。我们不得不使用 chroot 和软件补丁的组合，以避免不同 PostgreSQL 实例之间的 SysV IPC 名称空间冲突。相比之下，Xen 允许每个实例在其自己的域中启动，从而简化了配置。

在图 5 中，我们展示了 Xen 在运行 1、2、4 和 8 个 OSDB-IR 和 OSDB-OLTP 实例时所达到的总吞吐量。当添加第二个域时，第二个 CPU 的完全利用几乎使总吞吐量翻倍。进一步增加域数会造成总吞吐量的轻微减少，这可以归因于上下文切换和磁盘头移动的增加。在单个 Linux 操作系统上运行多个 PostgreSQL 实例的总分比使用 Xen 的等效分低 25-35%。原因尚未完全理解，但似乎 PostgreSQL 存在 SMP 可扩展性问题，并结合了对 Linux 块缓存的低利用率。

图 5 还演示了 8 个域间的性能差异。Xen 的调度器被配置为为每个域分配 1 到 8 之间的整数权重。每个域的结果吞吐量得分反映在条形图的不同带状中。在 IR 基准测试中，权重对吞吐量具有明确的影响，每个区段的大小都在其预期大小的 4% 内。

然而，在 OLTP 情况下，赋予域更大份额的资源并未实现成比例的高分：高水平的同步磁盘活动突显了我们当前磁盘调度算法的弱点，导致其性能不佳。

4.4 性能隔离

为了展示 Xen 提供的性能隔离，我们希望进行一次“烘烤测试”，比较 Xen 和其他基于操作系统的性能隔离技术实现，例如资源容器。然而，目前似乎没有可供下载的基于 Linux 2.4 的实现。QLinux 2.4 还未发布，目标是为多媒体应用提供服务质量（QoS），而不是在服务器环境中提供完整的防御性隔离。Ensim 基于 Linux 的私人虚拟服务器产品似乎是最完整的实现， reportedly 包含了对 CPU、磁盘、网络和物理内存资源的控制[14]。我们正在与 Ensim 进行讨论，并希望能够在稍后的日期报告比较评估的结果。

在缺乏并行比较的情况下…… 

--- 

以上是文本的翻译，希望对您有帮助！
在本研究中，我们展示了Xen的性能隔离在恶意负载存在时仍然如预期般有效的结果。我们配置了四个域，均分资源，其中两个域运行了之前测量的工作负载（PostgreSQL/OSDB-IR和SPEC WEB99），两个域则分别运行了一对极端反社会的进程。第三个域同时运行一个磁盘带宽占用程序（持续的dd）以及一个以大目录中的大量小文件创建为目标的文件系统密集型工作负载。第四个域在同一时间运行一个“分叉炸弹”，同时运行一个虚拟内存密集型应用程序，该应用程序试图分配和触碰3GB的虚拟内存，并在失败时释放每一页，然后重新启动。

我们的结果表明，OSDB-IR和SPEC WEB99的结果仅受到运行干扰进程的两个域的边际影响——分别比之前报告的结果低了4%和2%。我们将这一现象归因于额外上下文切换和缓存效应的开销。当配置为其默认的5毫秒最大调度“切片”时，我们认为在此极端负载下，Xen表现相对良好：在运行128个同时的计算密集型进程时，相较于Linux，我们仅损失了7.5%的总吞吐量。

在这种极端负载下，我们测量了与运行SPEC CINT2000子集的某个域之间的用户对用户的UDP延迟。我们测得的平均响应时间为147毫秒（标准差为97毫秒）。在对129个空闲的域重复实验时，我们记录的平均响应时间为5.4毫秒（标准差为16毫秒）。尽管有相当大的背景负载，交互式域仍然保持响应能力。

为了确定7.5%性能下降的原因，我们将Xen的调度“切片”设置为50毫秒（ESXServer使用的默认值）。结果显示吞吐量曲线紧密追踪本地Linux，几乎消除了性能差距。然而，正如预期，交互性能在高负载下受到这些设置的负面影响。

这些数据十分鼓舞人心——尽管背景负载很大，交互式域仍然保持响应能力。我们在本节中考察Xen在扩展到其目标系统时的能力，即100个域。我们讨论了运行多个Linux实例及其关联应用程序的内存需求，同时测量了它们执行时的CPU性能开销。

我们评估了一个使用XenoLinux启动并运行一组默认的RH7.2守护进程、一个sshd和Apache网络服务器的域的最小物理内存需求。该域在启动时被分配了64MB的保留内存，限制了其最大增长大小。在没有配置交换空间的情况下，该域能够将其内存占用缩减至6.2MB；使用交换设备后，这一数值进一步降低至4.2MB。一个处于安静状态的域能够在接收到HTTP请求或周期性地保持在这一缩减状态上。 

目前，我们知道还有两个其他系统采用了半虚拟化的方法：IBM正在为其z系列主机支持一个半虚拟化版本的Linux，允许大量Linux实例同时运行。Denali是一个现代隔离内核，具有能力支持承载大量虚拟化操作系统。
Here's the translated text in Chinese, maintaining the academic style and technical terminology:

---

间接服务会导致更多内存的需求。在这种情况下，客户操作系统将从Xen请求页面，增长其占用空间，达到其配置上限。这表明，内存使用开销在现代服务器上运行100个域时不太可能成为问题，通常会分配更多内存给应用数据和缓冲缓存，而不是操作系统或应用文本页面。Xen本身每个域仅维护固定的20KB状态，这与其他虚拟机监控器（VMM）需要维护的影子页表等不同。最后，我们考察了在大量域之间的上下文切换开销，而不仅仅是在进程之间的切换。图6展示了在我们的双处理器服务器上同时运行SPECCINT2000测试套件的小子集时，在1到128个域或进程之间获得的标准化聚合吞吐量。表示原生Linux的线几乎是平的，表明在这种基准下，当在如此多进程之间调度时没有聚合性能损失；Linux将它们识别为计算密集型，并以长时间片（50毫秒或以上）进行调度。相比之下，较低的线表示Xen的吞吐量。当运行在Xen之上时，没有必要检查“安全”代码或保证确定性——在这两种情况下唯一受影响的是有关客户端。因此，Xen提供了一个更通用的解决方案：不需要通过受信任的编译器数字签名托管代码（如SPIN所示），也不需要附上安全证明（如与PCC相关），或要求用特定语言编写（例如在安全网络或任何Java基础系统中），或依赖特定中间件（例如移动代理系统）。当然，这些其他技术仍然可以继续与在Xen上运行的客户操作系统一起使用。这在于工作负载中具有更多瞬态任务的情况尤为有用，这些任务不会提供机会来摊销启动新域的成本。

关于语言级虚拟机方法，可以做出类似的论证：尽管资源管理的JVM显然能够托管不受信任的应用程序，但这些应用程序必然需要编译为Java字节码并遵循该特定系统的安全模型。同时，Xen可以轻松支持作为在客户操作系统上运行的应用程序的语言级虚拟机。这种方案满足了低配置成本下短时间内托管瞬态服务器的需求。通过让100个操作系统在单台服务器上运行，我们将相关成本降低了两个数量级。此外，通过将每个操作系统的设置和配置转变为软件问题，我们能够在托管上实现更小粒度的时间尺度。

6. 讨论与结论

我们介绍了Xen超监视器，它在运行客户操作系统的领域之间划分计算机资源。我们的半虚拟化设计特别强调性能和资源管理。我们还描述了并评估了XenoLinux，这是一个完整特性的Linux 2.4内核在Xen上运行的移植版本。我们正在进行的工作是将BSD和Windows XP内核移植到Xen上，进一步确认Xen所暴露接口的普适性。

6.1 未来工作

我们认为Xen和XenoLinux已足够完整，以满足未来的需求。

--- 

Please let me know if you need anything else!
致谢

为了更广泛的听众，我们打算在不久的将来公开发布我们的软件。一个beta版本现已由选定的方进行评估；一旦此阶段完成，将在我们的项目页面上宣布一般1.0版的发布。此工作得到了英国工程与自然科学研究委员会（EPSRC）拨款GR/S01894/01及微软的支持。我们要感谢Evangelos Kotsovinos、Anil Madhavapeddy、Russ Ross和James Scott对本项目的贡献。

在初步发布后，我们计划对Xen进行多项扩展和改进。为了提高虚拟块设备的效率，我们打算实现一个基于块内容索引的共享通用缓存。这将为我们的设计增加受控的数据共享而不牺牲隔离性。将写时复制语义添加到虚拟块设备将允许它们在不同域之间安全共享，同时仍然允许不同的文件系统。

为了提供更好的物理内存性能，我们计划实现最后机会页面缓存（LPC），有效地作为一个系统范围的空闲页面列表，在机器内存不足时，仅在长度非零的情况下使用。LPC会在客户操作系统选择驱逐一个干净页面时使用；而不是完全丢弃此页面，它可以被添加到空闲列表的末尾。在该页面被Xen重新分配之前，发生的故障可以因此无需磁盘访问而解决。

Xen的一个重要角色是作为XenoServer项目的基础，该项目超越了单个机器，正在构建支持互联网规模计算基础设施所需的控制系统。我们设计的关键在于资源使用可以被精确计量并由该工作的赞助者支付——如果以真实现金支付，我们可以使用拥塞定价策略来处理超额需求，并利用超额收入支付额外的机器。这需要准确而及时的I/O调度，以增强对有害工作负载的弹性。我们还计划在我们的块存储架构中整合会计功能，通过为虚拟块设备创建租约来实现。

为了更好地支持XenoServer的管理与管理，我们正在整合更全面的审计和取证日志支持。我们还在开发对XenoServer的管理及其运行的更深入支持。

参考文献

[1] A. Awadallah 和 M. Rosenblum. The vMatrix: 一个虚拟机监视器网络，用于动态内容分发. 在第七届国际网络内容缓存与分发研讨会（WCW2002）论文集，2002年8月.

[2] A. Bakre 和 B. R. Badrinath. I-TCP: 移动主机的间接TCP. 在第十五届国际分布式计算系统会议（ICDCS1995）论文集，1995年6月，第136-143页.

[3] G. Banga, P. Druschel, 和 J. C. Mogul. 资源容器: 服务器系统中资源管理的新设施. 在第三届操作系统设计与实现研讨会（OSDI1999）论文集，1999年2月，第45-58页.

[4] A. Bavier, T. Voigt, M. Wawrzoniak, L. Peterson, 和 P. Gunningberg. SILK: 在Linux内核中的Scout路径. Uppsala大学信息技术系技术报告2002-009, 2002年2月.

[5] B. N. Bershad, S. Savage, P. Pardyak, E. G. Sirer, M. Fiuczynski, D. Becker, S. Eggers, 和 C. Chambers. SPIN操作系统中的扩展性、安全性和性能. 在第十五届ACM SIGOPS操作系统原则研讨会论文集，1995年12月，第267-284页.

[6] A. Brown 和 M. Seltzer. 在lmbench之后的操作系统基准测试: 有关NetBSD在Intel x86架构上性能的案例研究. 在1997年ACM SIGMETRICS会议论文集，1997年6月.

[7] E. Bugnion, S. Devine, K. Govil, 和 M. Rosenblum. Disco: 在可扩展多处理器上运行商品操作系统. 在第十六届ACM SIGOPS操作系统原则研讨会论文集，1997年10月，第143-156页.

[8] Connectix. 产品概述: Connectix虚拟服务器, 2003. http://www.connectix.com/products/vs.html.

[9] G. Czajkowski 和 L. Dayne's. 无妥协的多任务: 虚拟机的进化. ACM SIGPLAN通讯, 2001年11月，第125-138页.

[10] S. Devine, E. Bugnion, 和 M. Rosenblum. 包括用于具有分段架构的计算机的虚拟机监视器的虚拟化系统. 美国专利6397242, 1998年10月.

[11] K. J. Duda 和 D. R. Cheriton. 借用虚拟时间(BVT)调度: 在通用调度器中支持延迟敏感线程. 在第十七届ACM SIGOPS操作系统原则研讨会论文集.  

[28] J. MacKie-Mason 和 H. Varian. 拥塞网络资源的定价. IEEE杂志，选定的通信领域, 1995年7月，第1141-1149页.

[29] L. McVoy 和 C. Staelin. lmbench: 可移植的性能分析工具. 在USENIX年会技术会议论文集，1996年1月，第279-294页. Usenix协会.

[30] J. Navarro, S. Iyer, P. Druschel, 和 A. Cox. 超级页面的实际透明操作系统支持. 在...
Proceedings of the 5th Operating Systems Principles, Volume 33 (5) of ACM Operating Systems Design and Implementation (OSDI) Systems Review, pages 261–276, Kiawah Island Resort, SC, USA, 2002. ACM Operating Systems Review, Winter 2002 Special Issue, Dec. 1999, pages 89–104, Boston, MA, USA, Dec. 2002.

[12] G.W. Dunlap, S.T. King, S. Cinar, M. Basrai, and P.M. Chen. [31] G.C. Necula. Proof-carrying code. In Conference Record of ReVirt: Enabling Intrusion Analysis through Virtual-Machine Logging and Replay. In Proceedings of the 5th Symposium on Principles of Programming Languages, pages 106–119, Jan. 1997. Operating Systems Design and Implementation (OSDI 2002), ACM Operating Systems Review, Winter 2002 Special Issue, pages 211–224, Boston, MA, USA, Dec. 2002.

[13] D. Engler, S.K. Gupta, and F. Kaashoek. A VM: Application-level virtual memory. In Proceedings of the 5th Workshop on Hot Topics in Operating Systems, pages 72–77, May 1995.

[14] Ensim. Ensim Virtual Private Servers, 2003. http://www.ensim.com/products/materials/datasheet_vps_051003.pdf.

[15] K.A. Fraser, S.M. Hand, T.L. Harris, I.M. Leslie, and I.A. Pratt. The Xenoserver computing infrastructure. Technical Report UCAM-CL-TR-552, University of Cambridge, Computer Laboratory, Jan. 2003.

[16] T. Garfinkel, M. Rosenblum, and D. Boneh. Flexible OS Support and Applications for Trusted Computing. In Proceedings of the 9th Workshop on Hot Topics in Operating Systems, Kauai, Hawaii, May 2003.

[17] J. Gelinas. Virtual Private Servers and Security Contexts, 2003. http://www.solucorp.qc.ca/miscprj/s_context.hc.

[18] K. Govil, D. Teodosiu, Y. Huang, and M. Rosenblum. Cellular Disco: Resource management using virtual clusters on shared-memory multiprocessors. In Proceedings of the 17th ACM SIGOPS Symposium on Operating Systems Principles, Volume 33 (5) of ACM Operating Systems Review, pages 154–169, Dec. 1999.

[19] P.H. Gum. System/370 extended architecture: facilities for virtual machines. IBM Journal of Research and Development, 27(6): 530–544, Nov. 1983.

[20] S. Hand. Self-paging in the Nemesis operating system. In Proceedings of the 3rd Symposium on Operating Systems Design and Implementation (OSDI 1999), pages 73–86, Oct. 1999.

[21] S. Hand, T.L. Harris, E. Kotsovinos, and I. Pratt. Controlling the XenoServer Open Platform, April 2003.

[22] A. Jeffrey and I. Wakeman. A Survey of Semantic Techniques for Active Networks, Nov. 1997. http://www.cogs.susx.ac.uk/projects/safetynet/.

[23] M.F. Kaashoek, D.R. Engler, G.R. Granger, H.M. Bricenço, R. Hunt, D. Mazieres, T. Pinckney, R. Grimm, J. Jannotti, and K. Mackenzie. Application performance and flexibility on Exokernel systems. In Proceedings of the 16th ACM SIGOPS Symposium on Operating Systems Principles, Volume 31 (5) of ACM Operating Systems Review, pages 52–65, Oct. 1997.

[24] R. Kessler and M. Hill. Page placement algorithms for large real-indexed caches. ACM Transactions on Computer Systems, USA, Dec. 2002. 
10(4):338-359，1992年11月。[43] A. Whitaker, M. Shaw, 和 S. D. Gribble. Denali: 轻量级虚拟机的操作系统支持。[25] S. T. King, G. W. Dunlap, 和 P. M. Chen. 为分布式和网络应用提供虚拟机的操作系统支持。在2003年美国USENIX技术大会年会的会议记录中，华盛顿大学，2002年，六月。[44] A. Whitaker, M. Shaw, 和 S. D. Gribble. Denali隔离内核中的可扩展性和性能。[26] M. Kozuch 和 M. Satyanarayanan. 互联网挂起/恢复。在第五届操作系统设计与实现研讨会上（OSDI 2002）发表的会议记录，纽约卡利科，2002年六月。[27] I. M. Leslie, D. McAuley, R. Black, T. Roscoe, P. Barham, 和 D. Evers. 操作系统评论，2002年冬季特别期，第195-210页，马萨诸塞州波士顿，美国，2002年12月。
Xen与虚拟化艺术  
保罗·巴姆（Paul Barham）、博里斯·德拉戈维奇（Boris Dragovic）、基尔·弗雷泽（Keir Fraser）、史蒂文·汉德（Steven Hand）、蒂姆·哈里斯（Tim Harris）、亚历克斯·霍（Alex Ho）、罗尔夫·诺伊格鲍尔（Rolf Neugebauer）、伊恩·普拉特（Ian Pratt）、安德鲁·沃菲尔德（Andrew Warfield）  
剑桥大学计算机实验室  
地址：英国剑桥，JJ汤姆森大道15号，CB3 0FD  
电子邮件：firstname.lastname@cl.cam.ac.uk  

**摘要**  
**1. 引言**  
众多系统被设计为利用虚拟化来细分现代计算机的丰富资源。有些需要专门硬件，或无法支持商品级操作系统。有些在性能上牺牲了100%的二进制兼容性。另一些则为提高速度而牺牲安全性或功能性。 很少有系统能提供资源孤立或性能保证；大多数只能提供最佳努力的配置，这可能导致服务拒绝。  
本文介绍了Xen，这是一个x86虚拟机监控程序（VMM），它允许多个商品级操作系统以安全且经过资源管理的方式共享常规硬件，而不牺牲性能或功能。通过提供一种理想化的虚拟机器抽象，使得操作系统如Linux、BSD和Windows XP可以实现较小的移植工作。  

我们的设计旨在使现代服务器能够同时托管多达100个虚拟机实例。Xen的虚拟化方法极其高效：我们允许操作系统如Linux和Windows XP在可忽略的性能开销（最多几个百分点，相较于未虚拟化情况）下同时运行。我们在一系列微基准测试和系统范围测试中，显著优于市售竞争产品和免费可用解决方案。  
本文描述并评估的原型能够支持多个并发的XenoLinux客户操作系统实例；每个实例导出一个与未虚拟化Linux 2.4完全相同的应用程序二进制接口。我们对Windows XP的Xen移植工作尚未完成，但能够运行简单的用户空间进程。关于NetBSD的移植工作也在进行中。  

Xen使用户能够动态实例化操作系统以执行他们所需的任何任务。在XenoServer项目中，我们在ISP的经济战略位置或互联网交换中心，部署标准服务器硬件上的Xen。我们在启动新虚拟机时执行入驻控制，并期望每个虚拟机以某种方式为所需的资源付费。  
我们在其他地方讨论了我们的思想和方法；本文重点关注虚拟机监控程序。  

**关键词**  
虚拟机监控程序、虚拟机管理程序、半虚拟化  

**类别与主题描述**  
D.4.1【操作系统】：进程管理；D.4.2【操作系统】：存储管理；D.4.8【操作系统】：性能  

**一般术语**  
设计、测量、性能  

构建一个系统以在共享机器上托管多个应用程序和服务有多种方式。也许最简单的方法是部署一个或多个运行标准操作系统（如Linux或Windows）的主机，然后允许用户安装文件和启动进程——应用程序之间的保护由传统操作系统技术提供。经验表明，系统管理可以迅速变成一项耗时的任务，因为假设的配置交互可能非常复杂。  
更重要的是，这些系统并未充分支持性能隔离；一个进程的调度优先级、内存需求、网络流量和磁盘访问会影响其他进程的性能。这在有足够的配置和封闭用户组的情况下（例如在商业用途中）可能是可接受的。
在传统的虚拟机监控器（VMM）中，所暴露的虚拟硬件与底层机器功能上是完全相同的[38]。尽管完全虚拟化具有让未修改操作系统得以托管的显著优点，但它也存在一系列缺陷。这一点尤其在资源过度订阅或用户不合作的情况下变得突出。解决这一问题的一种方式是将性能隔离的方法倒退到操作系统领域。这已通过资源容器[3]、Linux/RK [32]、QLinux [40] 和 SILK [4] 等方式在不同程度上得以验证。这种方法的一大难点在于确保所有资源使用均能归因于正确的进程—例如，考虑因缓冲缓存或页面替换算法而产生的应用之间复杂交互的问题。这实际上就是操作系统内“服务质量串扰”（QoS crosstalk）的问题[41]。在低层次实现多路复用可以缓解这一问题，正如 Exokernel [23] 和 Nemesis [27] 操作系统所表明的那样，从而最小化任务之间的意外或不必要的交互。

我们使用相同的基本思路来构建 Xen，该系统以整个操作系统为单位对物理资源进行多路复用，并能够在它们之间提供性能隔离。与进程级多路复用相比，这种方法还允许多种客户操作系统优雅地共存，而不是强制要求特定的应用二进制接口。为了这种灵活性，我们付出了代价——运行完整的操作系统相较于运行单个进程而言，无论是在初始化（例如，启动或恢复与 fork 和 exec）还是在资源消耗方面，都是更加沉重的负担。

针对我们目标的多达100个托管操作系统实例，我们认为这种代价是值得支付的；它允许个别用户以资源受控的方式运行未修改的二进制文件或二进制集合（例如，一个 Apache 服务器与一个 PostgreSQL 后端）。此外，它还提供了一种极高的灵活性，因为用户可以动态创建所需的精准执行环境，避免了各服务及应用之间不幸的配置交互（例如，每个 Windows 实例都维护自己的注册表）。

本文的其余部分结构如下：在第 2 节中，我们解释了我们的虚拟化方法并概述了 Xen 的工作方式。第 3 节描述了我们设计与实现的关键方面。第 4 节使用工业标准基准测试来评估运行在 Xen 上的 XenoLinux 的性能，并与独立的 Linux、VMware Workstation 和用户模式 Linux（UML）进行比较。第 5 节回顾了相关工作，最后第 6 节讨论未来的工作并作出总结。考虑到这些非常不同的目标，比较 Denali 的设计选择与我们自己的原则具有指导意义。

2. XEN: 方法与概述

首先，Denali 并不针对现有的应用二进制接口（ABI），因此可以从其虚拟机接口中省略某些架构特性。例如，Denali 没有完全支持 x86 段机制，尽管它在 NetBSD、Linux 和 Windows XP 的 ABI 中被导出（并广泛使用[1]）。
Here's the translated text:

关于广泛使用的IA-32或x86架构的虚拟化支持问题，特别是在单一客户操作系统内支持应用程序多路复用和多个地址空间的能力，从未成为x86架构设计的一部分。某些超级用户指令必须由虚拟机监控器（VMM）处理以确保正确的虚拟化，但这些指令在执行时处于不足的特权下安静地失败，而不会造成便捷的异常。这使得每个虚拟机本质上只能托管单用户单应用程序受保护的操作系统。相比之下，在Xen中，单个虚拟机托管一个真实的操作系统，该系统本身可以安全地多路复用成千上万个修改过的用户级进程。尽管已经开发出一种原型虚拟内存管理单元（MMU），可能在这一领域帮助Denali，但我们未能找到任何已发布的技术细节或评估。第三，在Denali架构中，VMM执行所有的分页操作，读写磁盘。这可能与虚拟化层缺乏内存管理支持有关。在分页时，每次更新尝试（例如，创建新的应用程序进程）都需要捕获并处理所有的非陷入特权指令。ESX Server实现了系统结构的影像版本，如页表，并通过捕获每次更新尝试以保持与虚拟表的一致性。此方法在更新密集型操作中成本较高，例如创建新的应用程序进程。对于内存管理而言，分页客户操作系统可以直接读取硬件页表，但更新将由Hypervisor批处理并验证。每个域可能被分配不连续的机器页面。客户操作系统必须在低于Xen的特权级别下运行，并且必须在Xen中注册异常处理程序的描述符表。除了页面错误外，其他处理程序保持不变。客户操作系统可以为系统调用安装一个“快速”处理程序，允许应用程序直接调用其客户操作系统，避免通过Xen间接调用的开销。硬件中断被替代为轻量级事件系统。每个客户操作系统都拥有一个计时器接口，并意识到“实际”时间和“虚拟”时间。网络、磁盘等虚拟设备优雅且简单地访问。数据通过异步I/O环传输，事件机制替代硬件中断以进行通知。

表1：半虚拟化的x86接口。VMM与我们性能隔离的目标相反：恶意的客户操作系统。若架构提供软件管理的TLB，任务将更容易，因为这些可以以简单的方式高效地虚拟化。带标记的TLB是大多数服务器级RISC架构（包括Alpha、MIPS和SPARC）支持的另一项有用特性。将地址空间标识符标签与每个TLB条目相关联，允许Hypervisor与每个客户操作系统高效共存于独立的地址空间，因此在转移执行时不必完全刷新整个TLB。不幸的是，x86没有软件管理的TLB；相反，TLB未命中由处理器自动处理，通过硬件遍历页表结构。因此，为了实现最佳性能，当前地址空间的所有有效页面翻译都应存在于硬件可访问的页表中。此外，因为TLB未标记，地址空间切换通常需要完全清除TLB。鉴于这些限制，我们做出了两个决定：（i）客户操作系统负责分配和管理硬件页表，Xen仅提供最小的参与以确保安全和隔离；（ii）Xen的客户操作系统定于。
引用 Xen 主机操作系统之一并存在于每个地址空间顶部的 64MB 区域，因此我们使用“域”（domain）一词来指代运行中的虚拟机，避免在进入和退出 hypervisor 时出现 TLB（翻译后备缓冲）缺失。客体操作系统（guest OS）在这里执行；这一区分类似于传统系统中的程序和进程之间的区别。我们称 Xen 为 hypervisor，因为它在所宿主的客体操作系统的监督代码之上操作，具有更高的特权级别。

### 2.1 虚拟机接口

表 1 提供了准虚拟化 x86 接口的概述，该接口可以分为系统的三个广泛方面：内存管理、CPU 和设备 I/O。接下来，我们逐一讨论每个机器子系统，并讨论它们在我们准虚拟架构中的呈现方式。需要注意的是，尽管我们的实现某些部分（例如内存管理）是特定于 x86 的，但许多方面（如我们的虚拟 CPU 和 I/O 设备）也可以轻松地应用于其他机器架构。此外，x86 在与 RISC 风格处理器显著不同的方面中代表了最坏的情况——例如，有效地虚拟化硬件页表比虚拟化软件管理的 TLB 要困难得多。

2.1.1 内存管理  
虚拟化内存无疑是准虚拟化架构中最困难的部分，无论是在 hypervisor 中所需的机制方面，还是在对每个方面的移植所需的修改方面。每当客体操作系统需要新的页表时，可能是因为正在创建一个新进程，它将从自身的内存保留区分配和初始化一个页，并将其注册到 Xen。此时，操作系统必须放弃对页表内存的直接写入特权；所有后续更新必须经过 Xen 的验证。这在多种方式上限制了更新，包括仅允许操作系统映射它拥有的页，并不允许对页表的可写映射。

2.1.2 CPU  
虚拟化 CPU 对客体操作系统有几个影响。首要的是，在操作系统下插入 hypervisor 违背了通常的假设：操作系统是系统中最特权的实体。为了保护 hypervisor 免受操作系统的不当行为（以及隔离各个域），客体操作系统必须被修改为在较低的特权级别下运行。许多处理器架构仅提供两个特权级别。在这样的情况下，客体操作系统将与应用程序共享较低的特权级别。客体操作系统随后通过在与应用程序不同的地址空间中运行来保护自己，并通过 hypervisor 间接传递控制，以设定虚拟特权级别并改变当前地址空间。

值得注意的是，x86 上的特权级别虚拟化是可行的，因为其硬件支持四个不同的特权级别。这些特权级别通常被描述为环（rings），并从零（最高特权）编号到三（最低特权）。操作系统代码通常在环 0 中执行，因为其他环不能执行特权指令，而环 3 通常用于应用程序代码。根据我们的了解，环 1 和环 2 自 OS/2 以来未被任何知名的 x86 操作系统使用。任何遵循此常见安排的操作系统都可以通过修改为在环 1 中执行而移植到 Xen。这防止了客体操作系统直接执行特权指令，同时安全地与在环 3 中运行的应用程序隔离。
在安装新页表时，或在操作系统系统调用时使处理器让步，因此故障以常规方式进行虚拟化。任何客户操作系统尝试直接执行特权指令时，都会被处理器以静默或出错的方式拒绝，因为只有 Xen 以足够特权的级别执行。  
2.1.3 设备 I/O  
包括内存故障和软件陷阱在内的异常可以在 x86 上非常直接地进行虚拟化。一个描述每种异常类型处理程序的表格会注册到 Xen 进行验证。该表中指定的处理程序通常与实际的 x86 硬件相同；这是可能的，因为在我们的半虚拟化架构中，异常堆栈帧并未被修改。唯一的修改是页故障处理程序，该处理程序通常会从特权处理器寄存器（CR2）读取故障地址；由于这并不可行，我们将其写入扩展堆栈帧。当在执行外部 ring 0 时发生异常，Xen 的处理程序会在客户操作系统堆栈上创建异常堆栈帧的副本，并将控制权返回给适当的已注册处理程序。  
通常只有两种类型的异常发生频率足够高，以影响系统性能：系统调用（通常通过软件异常实现）和页故障。我们通过允许每个客户操作系统注册一个直接由处理器访问的“快速”异常处理程序，来改善系统调用的性能；这个处理程序在将其安装到硬件异常表之前会先进行验证。不幸的是，不能将同样的技术应用于页故障处理程序，因为只有在 ring 0 中执行的代码才能从寄存器 CR2 读取故障地址；因此，页故障必须始终通过 Xen 来传递，以便在 ring 1 中访问此寄存器值。  
通过在将异常处理程序呈现给 Xen 时进行验证，可以确保安全性。唯一的必要检查是处理程序的代码段没有指定在 ring 0 中执行。由于没有客户操作系统能够创建这样的段，因此只需将指定段选择器与一小部分由 Xen 保留的静态值进行比较即可。除此之外，任何其他处理程序问题都可以在异常传播过程中解决——例如，如果处理程序的代码不符合要求。  
Windows XP 在其架构独立的操作系统代码中需要进行大量修改，因为它使用多种数据结构和联合体来访问页表条目（PTE）。每次页面表访问都必须单独修改，尽管某些只读虚拟块设备（VBD）可能会被创建，或者虚拟网络接口（VIF）可能会过滤 IP 数据包以防止源地址欺骗。  
控制 用户 用户 用户  
软件 软件 软件  
此控制接口以及关于系统当前状态的性能统计信息被导出到运行在域 0 中的一套应用级管理软件。该管理工具集合便利了整个服务器的管理：当前应用工具能够创建和销毁域、设置网络过滤器和路由规则、监控每个域的网络活动。  
客户操作系统 客户操作系统 客户操作系统 客户操作系统  
(XenoLinux) (XenoLinux) (XenoBSD) (XenoXP)  
Xeno 识别 Xeno 识别 Xeno 识别 Xeno 识别设备驱动设备驱动设备驱动设备驱动
驾驭网络包和流量粒度，创建和删除虚拟网络接口及虚拟块设备。我们预见到更高层次工具的开发，以进一步自动化管理政策的应用。

硬件（SMP x86，物理内存，网络，SCSI/IDE）
3. 详细设计
图1：运行Xen虚拟机监视器的机器结构，托管多个不同的客户操作系统，包括在XenoLinux环境中运行控制软件的Domain0。在每一种情况下，我们都将Xen和客户操作系统的功能并列呈现，以便于清晰阐述。目前对客户操作系统的讨论以XenoLinux为主，因为这是最成熟的；尽管如此，我们持续移植Windows XP和NetBSD的努力使我们有信心Xen是无客操作系统意识的。

这一过程通过脚本实现了自动化。相较之下，Linux对其通用内存系统的修改要少得多，因为它使用预处理器宏来访问页面表条目（PTEs）——这些宏定义提供了一个便捷的位置，以添加平行虚拟化所需的转换和虚拟机监视器调用。

在这两个操作系统中，架构特定部分实际上是将x86代码移植到我们平行虚拟化架构的结果。这涉及重写使用特权指令的例程，并删除大量低级系统初始化代码。由于存在遗留的16位仿真代码以及需要不同的引导加载机制，Windows XP所需的更改更为繁多。请注意，XP中的x86特定代码库显著大于Linux，因此应预期需要更大的移植工作。

2.3 控制与管理
在Xen的设计与实现过程中，始终有一个目标是尽可能地将政策与机制分离。尽管虚拟机监视器必须参与数据路径方面的工作（例如，在不同领域之间调度CPU、在发送前过滤网络数据包，或在读取数据块时实施访问控制），但并不需要它涉及，甚至知道如何共享CPU或每个领域可以传输哪种类型的数据包等更高层次的问题。

最终形成的架构是，虚拟机监视器本身仅提供基本的控制操作。这些操作通过可从授权域访问的接口导出；潜在的复杂政策决策，如入站控制，最好由运行在客户操作系统上的管理软件来执行，而不是由特权的虚拟机监视器代码来进行。

整体系统结构如图1所示。请注意，在启动时创建一个允许使用控制接口的域。这个初始域称为Domain0，负责托管应用级管理软件。控制接口提供了创建和终止其他域的能力，并控制它们的调度参数、物理内存分配以及它们访问机器物理磁盘和网络设备的权限。

3.2 数据传输：I/O环
因此，提供一种数据传输机制至关重要，允许数据在系统中垂直流动，同时尽可能减少开销。我们I/O传输机制的设计主要受两个因素的影响：资源管理和事件通知。为了进行资源问责，我们试图最小化在收到来自设备的中断时，将数据多路复用到指定域所需的工作量。
管理缓冲区的适应性

除了处理器和内存资源外，控制接口还支持虚拟网络接口（VIFs）和块设备（VBDs）的创建与删除。这些虚拟 I/O 设备具有相关的访问控制信息，决定了哪些域可以访问它们以及在什么限制下（例如，Xen 中通过固定基础页框的方式来进行 I/O 传输）。在数据传输过程中，I/O 缓冲区在共享缓冲池中防止串扰。为确保正确地估算网络往返时间，BVT 通过使用虚拟时间扭曲机制（这一机制临时违反了“理想”的公平共享，以偏向最近唤醒的域）提供低延迟调度。然而，其他调度算法可以轻松实现于我们的通用调度抽象之上。每个域的调度参数可以由在 Domain 0 中运行的管理软件进行调整。

3.3.2 时间与计时器

Xen 为来宾操作系统提供实时、虚拟时间和挂钟时间的概念。实时以自机器启动以来的纳秒数表示，并且维持与处理器周期计数器的准确性，可以锁定到外部时间源（例如，通过 NTP）。域的虚拟时间仅在执行过程中推进：这通常被来宾操作系统调度器用于确保其时间片在应用进程间的正确共享。最后，挂钟时间被指定为要加到当前实时上的偏移量。这使得挂钟时间可以在不影响实时前进进度的情况下进行调整。每个来宾操作系统可以编程一对报警计时器，一个用于实时，另一个用于虚拟时间。预计来宾操作系统将维护内部计时器队列，并使用 Xen 提供的报警计时器触发最早的超时。超时通过 Xen 的事件机制交付。

3.3.3 虚拟地址转换

与其他子系统一样，Xen 试图以尽可能少的开销虚拟化内存访问。如第 2.1.1 节所讨论，这一目标由于 x86 架构使用硬件页表而变得略微复杂。VMware 采用的方法是为每个来宾操作系统提供一个虚拟页表，而该页表对内存管理单元（MMU）不可见。然后，虚拟机监控器负责捕捉对虚拟页表的访问，验证更新，并在其与 MMU 可见的“影子”页表之间进行变更的来回传播。这大大提高了某些来宾操作系统操作的成本，例如创建新的虚拟地址空间，并且需要明确传播硬件更新到“已访问”和“脏”位。

尽管完全虚拟化强迫使用影子页表以提供连续物理内存的幻觉，但 XEN 并不那么受限。实际上，XEN 只需参与页表更新，以防止来宾操作系统进行不可接受的更改。因此，我们避免了与使用影子页表相关的开销和额外复杂性。
在Xen中的方法是通过指定一定数量的响应作为通知事件来进行的。这样允许每个域在延迟和吞吐量需求之间进行权衡，这与ArseNIC千兆以太网接口中的流感知中断调度类似。

为了方便验证，我们为每个机器页面框关联一个类型和引用计数。一个框可以在任何时刻具有以下任一互斥类型：页目录（PD）、页表（PT）、局部描述符表（LDT）、全局描述符表（GDT）或可写（RW）。请注意，无论其当前类型如何，客户操作系统始终可以创建对其自身页面框的可读映射。一个框只有在其引用计数为零时才能安全地重新分配。此机制用于维护安全所需的不变性；例如，一个域不能对页表的任何部分具有可写映射，因为这将要求相关框同时是PT和RW类型。

类型系统还用于跟踪哪些框已经通过验证可以用于页表。为此，客户操作系统会指示何时为页表使用分配框（这需要Xen对框中的每一条目进行一次性验证），其后类型被固定为PD或PT，直到接收到客户操作系统的后续解除固定请求。这在改变页表基指针时尤为有用，因为这样避免了在每次上下文切换时都需要验证新页表的需要。请注意，框只有在被解除固定且其引用计数减少为零后才能重新分配——这防止了客户操作系统使用解除固定请求来规避引用计数机制。

为了最小化所需的超级调用数量，客户操作系统可以在应用整个批次时本地排队更新，并通过单个超级调用进行应用——在创建新的地址空间时特别有利。然而，我们必须确保更新足够早地提交以保证正确性。幸运的是，客户操作系统通常会在第一次使用新映射前执行TLB刷新：这确保了任何缓存的翻译都被作废。因此，在TLB刷新之前立即提交待处理的更新通常足够保证正确性。然而，某些客户操作系统在确认TLB中不存在陈旧条目时会选择跳过刷新。在这种情况下，第一次尝试使用新映射时有可能会导致页面缺失错误。因此，客户操作系统的故障处理程序必须检查是否有未完成的更新；如果发现任何，则执行刷新并重试出错指令。

为了高效地实现数据包接收，我们要求客户操作系统在接收到每个数据包时交换一个未使用的页面框；这避免了Xen和客户操作系统之间需要复制数据包的情况，尽管这要求将页面对齐的接收缓冲区排队到网络接口。当接收到数据包时，Xen会立即将其处理，确保公正性，Xen会实现一个简单的轮转调度器。
Here's the translation of the provided text into Chinese, maintaining an academic style and technical terminology:

---

pttoclaimadditional随后检查接收规则集，以确定来自Xen的目标内存页，直到达到此保留限制。相反，VIF，并在释放域希望节省资源的情况下，可能会避免承担不必要的成本，通过交换数据包缓冲区获取页面帧。如果没有可用的帧，数据包将被丢弃。

3.3.6 磁盘
XenoLinux实现了一个气球驱动程序[42]，它通过向Xen和XenoLinux的页面分配器之间传递内存页面，调整域的内存使用情况。尽管我们可以直接修改Linux的内存管理例程，但气球驱动程序通过利用现有的操作系统功能进行调整，从而简化了Linux的移植工作。然而，可以使用准虚拟化扩展气球驱动程序的功能；例如，来宾操作系统中的内存不足处理机制可以被修改，以通过向Xen请求更多内存来自动缓解内存压力。

大多数操作系统假设内存最多由几个大的连续范围构成。由于Xen不保证分配连续的内存区域，因此来宾操作系统通常会为自己创建连续物理内存的幻象，即使它们底层的硬件内存分配是稀疏的。从物理地址到硬件地址的映射完全由来宾操作系统负责，来宾操作系统可以简单地维持一个由物理页面帧号索引的数组。Xen通过提供一个所有域均可直接读取的共享转换数组来支持高效的硬件到物理映射——对该数组的更新必须经Xen验证，以确保相关的操作系统拥有相关的硬件页面帧。

请注意，即使来宾操作系统选择在大多数情况下忽略硬件地址，它在访问自己的页面表时仍必须使用转换表（这必然使用硬件地址）。硬件地址也可能暴露给操作系统的内存管理系统的有限部分，以优化内存访问。例如，一个来宾操作系统可能分配特定的硬件页面以优化在物理索引缓存[24]中的放置，或者使用超级页映射自然对齐的连续硬件内存部分[30]。 

Xen简化地以循环方式从竞争域批处理请求；这些请求随后会传递给标准调度器，然后再达到磁盘硬件。当需要维护更高层次的语义（例如在使用写前日志时）时，域可能显式地传递重排序屏障以防止重排序。该低层次调度为我们提供了良好的吞吐量，同时请求的批处理提供了合理公平的访问。未来的工作将研究如何提供更可预测的隔离和不同的服务，或许采用现有的技术和调度器[39]。

我们还呈现了用户模式Linux（UML）的结果，这是一种越来越受欢迎的虚拟托管平台。UML是Linux的一个端口，作为用户空间进程在Linux主机上运行。与XenoLinux类似，所需的更改仅限于体系结构依赖代码基础。然而，UML代码由于执行环境的性质非常不同，与本机x86端口几乎没有相似之处。尽管UML可以在未修改的Linux主机上运行，但我们展示了利用补丁改进性能的“单内核地址空间”（skas3）变体的结果。

我们还研究了在同一x86机器上运行已移植版本的Linux的其他三种虚拟化技术。

---

请注意，部分文本可能由于其复杂性和技术性而需要进一步微调以符合特定上下文。
构建一个完全在Tix的Virtual PC和即将推出的Virtual Server产品中创建的域（现在被微软要求的ac-Xen）在设计上与VMware类似，提供了完整的x86虚拟化，减少了管理程序的复杂性，提高了稳健性（对特权接口的访问进行了合理性检查，这使我们能够在初始开发过程中捕捉到许多错误）。然而，最重要的是构建过程的扩展和专门化的便利性，以应对新的客户操作系统。例如，Linux内核假定的启动时地址空间比Windows XP预期的要简单得多。虽然可以为所有客户操作系统指定一个固定的初始内存布局，但这将需要在每个客户操作系统中添加额外的引导代码，以根据其余系统的要求进行布局。不幸的是，这类代码难以正确实现；因此，为了简单性和稳健性，最好在Domain0内实现，该域可以提供比引导环境更丰富的诊断和调试支持。

所有实验都在一台Dell 2650双处理器2.4GHz Xeon服务器（配备2GB RAM、Broadcom Tigon 3千兆以太网网卡和一台单一的Hitachi DK32EJ 146GB 10k RPM SCSI硬盘）上进行。整个过程中使用的是Linux 2.4.21版本，针对原生和VMware客户操作系统实验进行了i686架构的编译，在Xen上运行时使用xeno-i686架构，而在UML上运行时使用architettura um。试验机中的Xeon处理器支持超线程（SMT），但由于当前没有内核支持SMT感知调度程序，因此此功能被禁用。我们确保所有客户操作系统及其管理程序所能使用的总内存量等于原生Linux所能使用的总内存量。整个实验中使用了Red Hat 7.2发行版，安装在ext3文件系统上。虚拟机配置为使用相同的磁盘分区，以“持久原始模式”运行，这样可以获得最佳性能。使用相同的文件系统映像还消除了潜在的磁盘寻道时间和传输速率的差异。因此，我们期望我们的Windows XP和NetBSD移植的相对开销相似，但尚未进行全面评估。

有许多现有解决方案用于在同一台机器上运行多个Linux副本。VMware提供了几种可以在其上启动未修改的Linux副本的商业产品。最常用的版本是VMware Workstation，它由一组特权内核扩展构成，安装在“主机”操作系统上，同时支持Windows和Linux主机。VMware还推出了名为ESX Server的增强产品，它用专用内核替代了主机操作系统。通过这样做，它在工作站产品上获得了一些性能好处。ESX Server还支持通过安装特殊设备驱动程序（vmxnet）到客户操作系统中，访问网络的半虚拟化接口，这在部署情况下是允许的。我们已经将ESX Server置于下面描述的基准测试套件中，但遗憾的是，由于产品最终用户许可协议的条款，我们无法报告定量结果。相反，我们将提供新内容。
来自 VMware Workstation 3.2 的结果，运行和编译器质量。该套件进行的输入/输出操作较少，并且与操作系统的交互也很有限，作为最新的 VMware 产品，它是在 Linux 主机操作系统之上运行的。几乎所有的 CPU 时间都花在执行用户空间代码上，所有三个虚拟机监控器 (VMM) 显示出低开销。 ESX Server 利用其本地架构来平等或超越 VMware Workstation 及其托管架构。虽然 Xen 当然要求来宾操作系统被移植，但它利用了半虚拟化的优势，显著超过 ESX Server。接下来的柱形图显示了在本地 ext3 文件系统上使用默认配置构建 Linux 2.4.21 内核所用的总时间，其中使用 gcc 2.96。原生 Linux 在操作系统中花费约 7% 的 CPU 时间，主要用于文件输入/输出、调度和内存管理。

在 VMM 的情况下，这种“系统时间”必须在一系列请求中获得超过 320Kb/s 的聚合带宽。Xen 的额外开销约为 3%，其他 VMM 则经历了更显著的减速。预热阶段允许同时客户数量的逐步增加，从而使服务器得以预加载其缓冲缓存。

我们执行了两个实验，使用 PostgreSQL 7.1.3 数据库，通过开源数据库基准套件 (OSDB) 在其默认配置下进行操作。我们展示了针对大多数但并非所有动态内容生成情况的结果，这些情况包括多用户信息检索 (IR) 和在线事务处理 (OLTP) 工作负载，均以每秒元组数进行测量。须对测试工具进行小幅修改以产生正确的结果，这是由于 UML 中存在一个在高负载下丢失虚拟定时器中断的 bug。基准测试通过 PostgreSQL 的本地 API（可调用 SQL）通过 Unix 域套接字驱动数据库。PostgreSQL 对操作系统施加了相当大的负载，这在 VMware 和 UML 经验的相当大的虚拟化开销中得以反映。特别地，OLTP 基准测试要求许多同步磁盘操作，导致多个保护域的转换。

dbench 程序是来自行业标准“NetBench”的文件系统基准测试。它模拟了 Windows 95 客户端对文件服务器施加的负载。在这里，我们检查了单个客户端执行约 90,000 次文件系统操作所体验到的吞吐量。XenoLinux 表现良好，其性能在原生 Linux 性能的 1% 以内。 VMware 和 UML 都面临挑战，支持的客户端数量不到原生 Linux 系统的三分之一。

SPECWEB99 是一个复杂的应用程序级基准测试，用于评估 Web 服务器及其宿主系统。该工作负载是对页面请求的复杂混合：30% 需要动态内容生成，16% 是 HTTP POST 操作，0.5% 执行 CGI 脚本。随着服务器的运行，它生成访问和 POST 日志，因此磁盘的工作负载不仅限于只读操作。因此，测量还反映了包括文件系统和网络在内的一般操作系统性能。

为了更准确地测量 Xen 和其他 VMM 中的开销区域，我们除了 Web 服务器本身进行了多次较小的实验，针对特定子系统。我们检查了使用 McVoy 的 lmbench 程序 [29] 测量的虚拟化开销。我们使用的是 3.0-a3 版本，因为它解决了 Seltzer 的 hbench [6] 中提出的许多工具可靠性问题。操作系统性能子集为 l 的表现。
mbenchsuite由37个微基准组成，这是可支持的最大数量。SPECWEB99定义了模拟用户必须接收到的最小服务质量，以使其被视为“合规”，从而计入得分：在许多情况下，用户对SMP系统中额外锁定所产生的性能开销感到惊讶。

| 配置      | L-SMP   | L-UP   | Xen      | VMW      | UML     |
|-----------|---------|--------|----------|----------|---------|
| 0.53      | 0.45   | 0.46    | 0.73     | 165      |
| 0.81      | 0.50   | 0.50    | 0.83     | 203      |
| 2.10      | 1.28   | 1.22    | 1.88     | 61.1     |
| 3.51      | 1.92   | 1.88    | 2.99     | 91.4     |
| 23.2      | 5.70   | 5.69    | 11.1     | 39.9     |

表6：ttcp: 带宽 (Mb/s)  
表3：lmbench: 进程时间 (µs)

为了评估虚拟化网络的开销，我们检查了在千兆以太网局域网 (LAN) 上的 TCP 性能。在所有实验中，我们使用了运行原生 Linux 的配置相似的 SMP 机器作为终端之一。这使我们能够独立测量接收和发送性能。ttcp 基准用于执行这些测量。发送者和接收者应用程序配置了 128kB 的套接字缓冲区，因为我们发现这为所有测试的系统提供了最佳性能。结果展示了 9 次实验的中位数数据，每次传输 400MB。

表6展示了两组结果，一组使用默认的 1500 字节以太网 MTU，另一组使用 500 字节 MTU（因为它通常被拨号 PPP 客户端使用）。结果表明，XenoLinux 的虚拟网络驱动程序所采用的页面换入技术避免了数据复制的开销，从而实现了每字节极低的开销。随着 MTU 降低至 500 字节，按包开销成为主导。发射重墙和接收解复用的额外复杂性对吞吐量产生了不利影响，但仅降低了 14%。

在 37 个微基准中的 24 个中，XenoLinux 的性能与原生 Linux 相似，紧随单处理器 Linux 内核的性能，并超越了 SMP 内核。在表 3 到表 5 中，我们展示了表现出有趣性能差异的结果，特别是对于 Xen 的较大惩罚用粗体显示。

在过程微基准（表 3）中，Xen 的 fork、exec 和 sh 的性能较原生 Linux 低。这是意料之中的，因为这些操作要求大量页面表更新，必须由 Xen 验证。然而，半虚拟化的方式使得 XenoLinux 可以批量更新请求。创建新页面表是一个理想的案例：因为没有理由更快地提交待决的更新，XenoLinux 可以将每个超调用均摊到 2048 个更新（其批量缓冲区的最大大小）上。因此，每个更新超调用构建了 8MB 的地址空间。

表 4 显示了在不同工作集大小下，不同数量进程间的上下文切换时间。Xen 在切换页面表基准时产生了额外的 1µs 到 3µs 的开销。然而，对于更大的工作集大小（或许更能代表真实应用程序），与缓存效率相比，这种开销相对较小。

在 4.3 章节“并发虚拟机”中，我们比较了在各自的客户操作系统中运行多个应用程序与在同一个原生操作系统中运行它们的性能。我们的重点是 Xen 的结果，但在适用的情况下也对其他虚拟机监控程序的性能进行了评论。图 4 显示了在两处理器机器上并行运行 1、2、4、8 和 16 个 SPEC WEB99 基准的结果。原生 Linux 被配置为 SMP；在其上我们以并发进程的形式运行多个 Apache 实例。在 Xen 的情况下，每个 SPEC WEB99 都在其自己的单处理器 Linux 客户操作系统中运行（附带一个 sshd 和其他管理进程）。每个 Web 服务器使用了不同的 TCP 端口号，以便使各个副本并行运行。注意到...
Here is the translation of the provided text into Chinese, maintaining the academic style and technical terminology:

---

VMware Workstation 在这些需要可同时连接的 SPEC 数据集方面的性能不如 UML（约 25 个微基准测试）; 然而，这是一个领域，其中 ESX Server 的改进能够减少开销。 mmap 延迟和页面错误延迟的结果如表 5 所示，十分有趣，因为它们需要两次转换进入 Xen 页：一次是为了获取硬件故障并将细节传递给来宾操作系统，第二次是在来宾操作系统上安装更新的页表条目。尽管如此，开销相对较小。

表 3 中的一个小异常是 Xen Linux 的信号处理延迟低于本地 Linux。该基准测试根本不需要任何 Xen 的调用，并且 0.75（约 30%）的加速是显著的。为了获得良好的 SPECWEB99 分数，既需要高吞吐量，也需要有界延迟：例如，如果客户端的请求由于磁盘读取滞后而受到阻塞，则该连接将被归类为不合格，并不会对分数产生贡献。因此，VMM 需及时调度域是重要的。默认情况下，Xen 使用 5 毫秒的时间片。

在运行单个 Apache 实例的情况下，添加第二个 CPU 使得本地 Linux 的得分提高了 28%，达到 662 个符合标准的客户端。然而，最佳的总吞吐量是在运行两个 Apache 实例时实现的，这表明 Apache 1.3.27 可能存在一些 SMP 可伸缩性问题。

在运行单个域时，Xen 的表现受到对 SMP 客户端操作系统支持不足的影响。然而，Xen 的中断负载均衡器识别空闲 CPU并将所有中断处理分配给它，从而使单 CPU 的得分提高了 9%。随着域数量的增加，Xen 的性能在接近本地案例的情况下有所改善。

接下来，我们进行了多次实验，运行多个 PostgreSQL 实例，通过 OSDB 套件进行测试。在单个 Linux 操作系统上运行多个 PostgreSQL 实例证明是困难的，因为通常情况下，单个 PostgreSQL 实例支持多个数据库。然而，这会阻止不同用户拥有分开的数据库配置。我们借助 chroot 和软件补丁的组合来避免不同 PostgreSQL 实例之间的 SysVIPC 名称空间冲突。相比之下，Xen 允许每个实例在其自己的域中启动，便于配置。

在图 5 中，我们展示了运行 1、2、4 和 8 个 OSDB-IR 和 OSDB-OLTP 实例时 Xen 实现的总吞吐量。当添加第二个域时，第二个 CPU 的完全利用几乎使总吞吐量翻倍。进一步增加域的数量则会导致总吞吐量的某种降低，这可以归因于上下文切换和磁盘磁头移动的增加。在单个 Linux 操作系统上运行多个 PostgreSQL 实例的总得分比使用 Xen 的同等得分低 25-35%。原因尚未完全理解，但似乎 PostgreSQL 存在 SMP 可伸缩性问题，结合了 Linux 块缓存的低利用率。

图 5 还展示了 8 个域之间的性能差异。Xen 的调度器被配置为给每个域分配一个在 1 到 8 之间的整数权重。每个域的结果吞吐量得分反映在条形图的不同带状上。在 IR 基准测试中，权重对吞吐量有精确影响，每个区段的实际吞吐量在预期大小的 4% 以内。

然而，在 OLTP 的情况下，被赋予更大资源份额的域未能实现相应的高分：高水平的同步磁盘活动突显了我们当前磁盘调度算法的不足，导致它们表现不佳。

**4.4 性能隔离**

为了展示 Xen 提供的性能隔离，我们希望在 Xen 和其他基于操作系统的性能隔离技术的实现（如资源容器）之间进行一次“烘焙比拼”。然而，目前似乎没有基于 Linux 2.4 的实现可供下载。QLinux 2.4 尚未发布，且其目标是为多媒体应用提供服务质量，而不是在服务器环境中提供完全的防御隔离。Ensim 的基于 Linux 的私有虚拟服务器产品似乎是最完整的实现，据称涵盖了对 CPU、磁盘、网络和物理内存资源的控制。我们正在与 Ensim 进行讨论，希望在稍后能够报告比较评估的结果。

在缺乏直接比较的情况下...

--- 

请注意，由于原文本缺失和排版错误，一些内容可能无法准确翻译。希望翻译能符合您的要求。
在本文中，我们展示了结果，表明Xen的性能隔离在存在恶意负载的情况下同样有效。我们配置了四个领域，每个领域的资源分配相同，其中两个领域运行之前测量的负载（PostgreSQL/OSDB-IR和SPEC WEB99），另两个领域各自运行一对极具抗社交性的进程。第三个领域同时运行了一个磁盘带宽消耗者（持续的dd命令），以及一个针对大量小文件创建的大型文件系统密集型负载。第四个领域则同时运行一个“分叉炸弹”和一个虚拟内存密集型应用，该应用试图分配和访问3GB的虚拟内存，并在失败时释放每一页，然后重新启动。

我们发现，OSDB-IR和SPEC WEB99的结果仅受到运行扰动进程的两个领域的轻微影响——分别比之前报告的结果低4%和2%。我们将这种情况归因于额外上下文切换和缓存效应的开销。在配置其默认5ms最大调度“片段”时，这种情况的处理能力得到了进一步确认。尽管在单台服务器上同时操作128个计算密集型进程在我们的目标应用领域中不太常见，Xen的表现相对良好；运行128个领域时，我们与Linux相比，仅损失了7.5%的总吞吐量。

在这种极端负载下，我们测量了用户间的UDP延迟，对一个运行SPEC CINT2000子集的领域进行了测量。我们记录的平均响应时间为147毫秒（标准差97毫秒）。在对一个其他Linux系统几乎空闲的第129个领域重复实验时，我们记录的平均响应时间为5.4毫秒（标准差16毫秒）。这些数据是非常可喜的——尽管存在显著的背景负载，交互领域仍然保持响应能力。

为了确定7.5%性能下降的原因，我们将Xen的调度“片段”设置为50毫秒（这是ESX Server使用的默认值）。结果是一个吞吐量曲线几乎完全追踪原生Linux，几乎消除了性能差距。然而，如预期的那样，在高负载情况下，交互性能受到这些设置的不利影响，造成了性能的一定损失。

值得注意的是，虚拟化在操作系统中已被商业化和研究应用近三十年。IBM VM/370首次利用虚拟化为老旧代码提供二进制支持。VMware和Connectix都虚拟化了商品PC硬件，允许多个操作系统在单个主机上运行。所有这些例子都实现了对底层硬件（至少是一部分）进行了完全虚拟化，而不是对其进行半虚拟化，只是向客户操作系统呈现了经过修改的接口。如我们的评估所示，决定提供完全虚拟化，尽管能够更轻松地支持现成操作系统，但对性能产生了负面影响。

在这一部分，我们考察了Xen在其目标系统（100个领域）上的扩展能力。我们讨论了运行多个客户操作系统实例及相关应用程序的内存需求，并测量了其执行的CPU性能开销。我们评估了使用XenoLinux启动的领域的最小物理内存要求，运行默认设置的RH7.2守护进程，以及一个sshd和Apache网络服务器。该领域在启动时分配了64MB的保留内存，限制了其最大增长大小。客户操作系统被指示尽可能地最小化其内存占用，将所有可能的页面返回给Xen。在未配置任何交换空间的情况下，该领域能够将其内存占用减少到6.2MB；允许使用交换设备则将这一数据降低至4.2MB。当域处于这一减少状态时，可以保持直到有HTTP请求或周期性任务的到来。
以下是该文本的中文翻译，保持了学术风格和技术术语：

---
这证明了内存使用的开销不太可能成为在现代服务器分类机器上运行100个域的问题——通常会为应用数据和缓冲缓存使用分配更多内存，而不是操作系统或应用程序文本页。Xen本身每个域仅维持固定的20kB状态，这与必须维护阴影页表等的其他虚拟机监控程序不同。

最后，我们考察了在大量域之间的上下文切换开销，而不仅仅是在进程之间的上下文切换。图6显示了当在我们的双CPU服务器上同时运行SPECCINT2000基准测试的小子集时，在1到128个域或进程之间获得的标准化聚合吞吐量。代表本地Linux的曲线几乎是平的，这表明在调度如此众多的进程时，聚合性能没有损失；Linux将它们识别为计算密集型任务，并以50毫秒或更长的时间片进行调度。相比之下，下方的曲线则表明Xen的吞吐量。

因此，Xen提供了一个更通用的解决方案：无需通过受信任的编译器对托管代码进行数字签名（如在SPIN中），也无需附带安全证明（如使用PCC），写成特定语言（如在安全网络或任何Java基础系统中），或依赖于特定中间件（如移动代理系统）。这些其他技术当然可以继续与在Xen上运行的客户操作系统一起使用。这对于具有更多瞬态任务的工作负载特别有用，因为它们将不会提供摊销新域启动成本的机会。

关于语言级虚拟机方法也可以做类似的论证：虽然资源管理的JVM应该能够托管不受信应用程序，但这些应用程序必须编译成Java字节码，并遵循该系统的安全模型。与此同时，Xen能够轻松支持以应用程序形式运行的语言级虚拟机。

6.2 结论

Xen为部署广泛的网络中心服务提供了一个优秀的平台，如动态网页内容的本地镜像、媒体流转码和分发、多玩家游戏和虚拟现实服务器，以及提供瞬态连接设备的“智能代理”。Xen直接解决了部署此类服务的最大障碍：当前无法承载短时间内低实例化成本的瞬态服务器。通过允许100个操作系统在单个服务器上运行，我们降低了相关成本两个数量级。此外，通过将每个操作系统的设置和配置转变为软件关注点，我们使得托管小粒度时间规模成为可能。

我们已介绍Xen虚拟机监控程序，它在运行客户操作系统的域之间划分计算机的资源。我们的半虚拟化设计特别强调性能和资源管理。我们还描述并评估了XenoLinux，这是一个完全功能的Linux 2.4内核的移植版本，可以在Xen上运行。我们正在进行的工作是将BSD和Windows XP内核移植到Xen上，这证实了Xen所暴露接口的通用性。

6.1 未来工作

我们相信Xen和XenoLinux已足够完整。

--- 

希望这个翻译符合您的要求！如果需要进一步的帮助，请告诉我。
致谢

本工作的支持来自于ESPRC资助项目GR/S01894/01，以及来自微软的资助。我们感谢Evangelos Kotsovinos、Anil Madhavapeddy、Russ Ross和James Scott对本项目的贡献。我们计划在初始版本发布后推出一系列对Xen的扩展和改进。为提高虚拟块设备的效率，我们打算实现一个基于块内容的共享通用缓存，这将为我们的设计添加受控数据共享而不牺牲隔离性。通过在虚拟块设备中添加写时复制语义，将允许它们在不同行之间安全共享，同时仍然支持不同的文件系统。

为了提供更好的物理内存性能，我们计划实施最后机会页面缓存（LPC），该缓存在机器内存不足的情况下有效使用，只有在非零长度时才会成为系统范围内的空闲页面列表。LPC将在客户操作系统的虚拟内存系统选择驱逐一个干净页面时使用；而不是完全丢弃该页面，它可能会被添加到空闲列表的末尾。在此页面被Xen重新分配之前发生的故障因此可以不需要磁盘访问即可得到满足。

Xen的重要角色是XenoServer项目的基础，该项目超越了单个机器，构建支持互联网规模计算基础设施所需的控制系统。我们设计的关键思想是资源使用应精确计量并由该工作的赞助者支付——如果支付以现金形式进行，我们可以采用拥堵定价策略来处理超额需求，并利用超额收入支付额外机器的费用。这需要准确及时的输入输出调度，以提高对恶劣工作负载的弹性。我们还计划将会计纳入我们的块存储架构，通过为虚拟块设备创建租约来实现。

为了更好地支持XenoServer的管理与管理，我们正在整合更为全面的审计和取证日志支持。我们还在开发一个操作系统，以支持可扩展多处理器上的商品操作系统运行。

参考文献

[1] A. Awadallah 和 M. Rosenblum。vMatrix：用于动态内容分发的虚拟机器监控网络。发表于第七届国际网络内容缓存与分发研讨会（WCW2002），2002年8月。

[2] A. Bakre 和 B. R. Badrinath。I-TCP：移动主机的间接TCP。在第十五届国际分布式计算系统会议（ICDCS1995）论文集中，136-143页，1995年6月。

[3] G. Banga, P. Druschel 和 J. C. Mogul。资源容器：一种新的服务器系统资源管理设施。在第三届操作系统设计与实施研讨会（OSDI1999）论文集中，45-58页，1999年2月。

[4] A. Bavier, T. Voigt, M. Wawrzoniak, L. Peterson 和 P. Gunningberg。SILK：Linux内核中的侦察路径。技术报告2002-009，乌普萨拉大学信息技术系，2002年2月。

[5] B. N. Bershad, S. Savage, P. Pardyak, E. G. Sirer, M. Fiuczynski, D. Becker, S. Eggers 和 C. Chambers。SPIN操作系统中的可扩展性、安全性和性能。在第十五届ACM操作系统原则研讨会论文集中，卷29(5)，267-284页，1995年12月。

[6] A. Brown 和 M. Seltzer。Lmbench带来的操作系统基准测试：关于NetBSD在Intel x86架构上性能的案例研究。在1997年ACM SIGMETRICS计算机系统测量与建模会议论文集中，1997年6月。

[7] E. Bugnion, S. Devine, K. Govil 和 M. Rosenblum。Disco：在可扩展多处理器上运行商品操作系统。在第十六届ACM操作系统原则研讨会论文集中，卷31(5)，143-156页，1997年10月。

[8] Connectix。产品概述：Connectix虚拟服务器，2003年。http://www.connectix.com/products/vs.html。

[9] G. Czajkowski 和 L. Dayne·s。没有妥协的多任务：虚拟机演变。ACM SIGPLAN通知，36(11): 125-138，2001年11月。2001年ACM SIGPLAN对象导向编程、系统、语言及应用会议论文集。

[10] S. Devine, E. Bugnion 和 M. Rosenblum。包括计算机的虚拟化系统的虚拟机监控器。美国专利6397242，1998年10月。

[11] K. J. Duda 和 D. R. Cheriton。借用虚拟时间（BVT）调度：支持通用调度器中的低延迟敏感线程。在第十七届ACM操作系统原则研讨会论文集中。
以下是翻译后的中文文本，保持了学术风格和技术术语：

Proceedings of the 5th 
操作系统原理，第33卷第5期，ACM操作系统设计与实现研讨会（OSDI）会议记录，第261页（cid:150）276页，美国南卡罗来纳州基阿瓦岛度假村，2002年，ACM操作系统评论，2002年冬季特刊，1999年12月，第89页（cid:150）104页，美国马萨诸塞州波士顿，2002年12月。

[12] G.W. Dunlap, S.T. King, S. Cinar, M. Basrai, 和 P.M. Chen。 [31] G.C. Necula。可证明的代码。在第24届ACM SIGPLAN-SIGACT程序语言原理研讨会的会议记录中，POPL 1997：程序语言原则研讨会的第5届会议（OSDI 2002）的会议记录，第106页（cid:150）119页，1997年1月。

[32] S. Oikawa 和 R. Rajkumar。便携式 RK：用于保证和强制时间行为的可移植资源内核。在IEEE实时技术与应用研讨会的会议记录中，第111页（cid:150）120页，1999年6月。

[33] L. Peterson, D. Culler, T. Anderson 和 T. Roscoe。向互联网引入颠覆性技术的蓝图。在第一届网络热点话题研讨会（HotNets-I）的会议记录中，普林斯顿，美国新泽西州，2002年10月。

[34] I. Pratt 和 K. Fraser。砷化物：用户可访问的千兆以太网接口。在第20届IEEE计算机与通信学会联合会议（INFOCOM-01）的会议记录中，第67页（cid:150）76页，美国加利福尼亚州洛斯阿拉米托斯，2001年4月22日。

[35] D. Reed, I. Pratt, P. Menage, S. Early, 和 N. Stratford。XenoServers：未受信任代码的账户执行。在第7届热点操作系统研讨会的会议记录中，1999年。

[36] J.S. Robin 和 C.E. Irvine。分析Intel Pentium的支持安全虚拟机监控器的能力。在第9届USENIX安全研讨会的会议记录中，美国科罗拉多州丹佛，第129页（cid:150）144页，2000年8月。

[37] C.P. Sapuntzakis, R. Chandra, B. Pfaff, J. Chow, M.S. Lam, 和 M. Rosenblum。优化虚拟计算机的迁移。在OSDI 2002的第5届操作系统设计与实现研讨会的会议记录中，ACM操作系统评论，2002年冬季特刊，第377页（cid:150）390页，美国马萨诸塞州波士顿，2002年12月。

[38] L. Seawright 和 R. MacKinnon。VM/370：多样性和实用性的研究。IBM系统期刊，第4页（cid:150）17页，1979年。

[39] P. Shenoy 和 H. Vin。Cello：下一代操作系统的磁盘调度框架。在ACM SIGMETRICS’98，国际计算机系统测量与建模会议的会议记录中，第44页（cid:150）55页，1998年6月。

[40] V. Sundaram, A. Chandra, P. Goyal, P. Shenoy, J. Sahni 和 H.M. Vin。QLinux多媒体操作系统的应用性能。在第8届ACM多媒体会议的会议记录中，2000年11月。

[41] D. Tennenhouse。分层多路复用被认为有害。在Rudin和Williamson编辑的《高速网络协议》中，第143页（cid:150）148页，North-Holland，1989年。

[42] C.A. Waldspurger。VMware ESX服务器中的内存资源管理。在OSDI 2002的第5届操作系统设计与实现研讨会的会议记录中，ACM操作系统评论，2002年冬季特刊，第181页（cid:150）194页，美国马萨诸塞州波士顿，2002年12月。

[24] R. Kessler 和 M. Hill。大型真实索引缓存的页面放置算法。ACM计算机系统交易，第27卷第6期，第530页（cid:150）544页，1983年11月。
10(4):338-359, 1992年11月。[43] A. Whitaker, M. Shaw 和 S. D. Gribble. Denali: 轻量级虚拟机的操作系统支持。[25] S. T. King, G. W. Dunlap 和 P. M. Chen. 为分布式和网络应用程序提供的虚拟机操作系统支持。发表于2003年美国USENIX技术会议论文集，华盛顿大学，2002年。 [44] A. Whitaker, M. Shaw 和 S. D. Gribble. 在Denali隔离内核中的规模和性能。 [26] M. Kozuch 和 M. Satyanarayanan. 网络挂起/恢复。发表于第五届操作系统设计与实施研讨会（OSDI2002）论文集，纽约Calicoon，2002年6月。 [27] I. M. Leslie, D. McAuley, R. Black, T. Roscoe, P. Barham, D. Evers, 发表在《操作系统评论》，2002年冬季特刊，第195-210页，马萨诸塞州波士顿，美国，2002年12月。
**Xen与虚拟化的艺术**

保罗·巴哈姆，鲍里斯·德拉戈维奇，基尔·弗雷泽，史蒂文·汉德，蒂姆·哈里斯，亚历克斯·霍，罗尔夫·纽根鲍尔，伊恩·普拉特，安德鲁·沃菲尔德  
剑桥大学计算机实验室  
15 JJ汤姆森大道，剑桥，英国，CB3 0FD  
firstname.lastname@cl.cam.ac.uk

**摘要**

随着现代计算机的强大，许多系统被设计为使用虚拟化技术来细分计算机的资源。一些系统需要专用硬件，或无法支持商品操作系统。一些系统支持100%的二进制兼容性，尽管这可能会影响性能。还有一些系统为了速度而牺牲安全性或功能性。然而，提供资源隔离或性能保障的系统相对较少；大多数系统仅提供尽力而为的服务，容易导致服务拒绝。本文介绍了Xen，这是一个基于x86的虚拟机监控器 (VMM)，它允许多个商品操作系统在保证安全和资源管理的情况下共享常规硬件，而不会牺牲性能或功能性。这是通过提供理想化的虚拟机抽象来实现的，使得Linux、BSD和Windows XP等操作系统可以进行最小化的移植。

我们的设计旨在支持现代服务器上同时托管多达100个虚拟机实例。Xen所采用的虚拟化方法非常高效：我们允许Linux和Windows XP等操作系统同时运行，且其性能开销微乎其微——最多仅为未虚拟化情况下的几个百分点。我们在多个微基准测试和系统范围测试中表现远超竞争对手的商业解决方案和可自由获取的解决方案。本文描述和评估的原型能够支持多个并发的XenoLinux客户操作系统实例；每个实例都导出了一个与非虚拟化Linux 2.4完全相同的应用程序二进制接口。我们的Windows XP移植到Xen尚未完成，但能够运行简单的用户空间进程。NetBSD的移植工作也在持续推进。

**类别和主题描述符**

D.4.1 [操作系统]：进程管理；D.4.2 [操作系统]：存储管理；D.4.8 [操作系统]：性能

**一般术语**

设计，测量，性能

**关键词**

虚拟机监控器，超监视器，半虚拟化

在共享计算机上托管多个应用程序和服务器的系统有多种构建方式。也许最简单的方法是部署运行标准操作系统（如Linux或Windows）的一个或多个主机，然后允许用户安装文件并启动进程——应用程序之间的保护由传统操作系统技术提供。经验表明，系统管理可能迅速成为一项耗时的任务，因为被认为独立的配置之间会有复杂的交互。更重要的是，这些系统并未充分支持性能隔离；一个进程的调度优先级、内存需求、网络流量和磁盘访问会影响其他进程的性能。当有足够的资源配备并且用户组是封闭的（例如在某些商业情况下）时，这可能是可以接受的。

**版权声明**

本研究的全部或部分数字或纸质副本的复制用于个人或课堂使用，经允许不收取费用，前提是副本未制作或分发以用于盈利商业利益，并且副本上附有本通知和完整的引用。在其他情况下复印、再出版、发布到服务器或重新分发到列表，需事先获得特定的许可和/或支付费用。

SOSP’03，2003年10月19日至22日，纽约州博尔顿登陆，美国。  
版权2003 ACM 1›58113›757›5/03/0010...$5.00
在传统的虚拟机监控器（VMM）中，暴露的虚拟硬件在功能上与底层机器完全相同[38]。虽然完全虚拟化显然能够允许未修改的操作系统被托管，但它也有许多缺点。这尤其在当资源超额分配或用户不合作时显得尤为明显。有一种解决该问题的方法是为操作系统重工支持性能隔离。这已经通过资源容器[3]、Linux/RK [32]、QLinux [40] 和 SILK [4] 在不同程度上得到了验证。这种方法的一项困难在于确保所有的资源使用都能归账到正确的进程。例如，考虑由于缓冲缓存或页面替换算法导致的应用程序之间的复杂交互。这实际上是操作系统中“服务质量交叉干扰”（QoS crosstalk）[41] 的问题。在低级别上进行多路复用可以缓解这一问题，正如Exokernel [23] 和Nemesis [27]操作系统所示。任务之间的非意图或不期望的交互被最小化。

我们使用相同的基本方法构建了Xen，它以整个操作系统的粒度对物理资源进行多路复用，并能够在它们之间提供性能隔离。与进程级别的多路复用相比，这也允许一系列访客操作系统相对优雅地共存，而不是强制规定特定的应用程序二进制接口。为此，虽然需要在这方面付出代价（运行完整的操作系统比运行一个进程更加耗费资源），但我们认为这是值得的；这使个人用户能够以资源受控的方式运行未修改的二进制文件或二进制文件集合，例如，一个Apache服务器和一个PostgreSQL后端。

此外，它提供了极高的灵活性，因为用户可以动态创建其软件所需的确切执行环境。不幸的是，各种服务和应用程序之间的配置交互被避免（例如，每个Windows实例维持自己的注册表）。 

本文其余部分的结构如下：第二节我们解释了我们的虚拟化方法并概述了Xen的工作机制。第三节描述了我们设计和实施的关键方面。第四节使用工业标准基准测试评估XenoLinux在Xen之上运行的性能，与独立的Linux、VMware Workstation和用户模式Linux（UML）进行比较。第五节回顾相关工作，最后第六节讨论未来的工作并做出总结。鉴于这些截然不同的目标，将Denali的设计选择与我们的原则进行对比具有指导意义。

2. XEN：方法与概述

首先，Denali并不针对现有的应用程序二进制接口（ABI），因此可以省略其虚拟机接口中的某些架构特性。例如，Denali并没有完全支持x86段，而这是在NetBSD、Linux和Windows XP的ABI中被导出（并广泛使用）的。其次，Denali的实现并未解决上述问题… 

（注：翻译尽量保留了学术风格和专业术语，部分内容已进行了适度调整以确保语言通顺。）
The text you've provided seems to be a technical excerpt related to virtualization technologies, particularly focusing on the x86 architecture and the Denali and Xen virtualization systems. Here's the translation into Chinese, maintaining the academic style and technical terminology:

---

对于广泛使用的IA-32或x86架构，支持应用程序复用的问题，以及在单一来宾操作系统内的多个地址空间的支持，从未成为x86架构设计的一部分。某些监控指令必须由虚拟机监控器（VMM）处理，以确保正确的虚拟化，但是以不够特权执行这些指令会悄无声息地失败，而不是导致一个方便的陷阱。因此，每个虚拟机本质上托管一个单用户单应用程序受保护的“操作系统”。相对而言，在Xen中，一个单一的虚拟机托管一个真实的操作系统，后者本身可以安全地多路复用成千上万的修改过的用户级进程。尽管开发了一种原型虚拟内存管理单元（MMU），可能在这方面帮助Denali，但我们并不知道有任何已发布的技术细节或评估。

第三，在Denali架构中，虚拟机监控器（VMM）执行所有的分页操作，包括从磁盘的数据读取和写入。这可能与虚拟化层缺乏内存管理支持有关。内部分页通过捕获每个更新尝试（这一方法在更新密集的操作中成本较高）来维持与虚拟表的兼容性。来宾操作系统（Guest OS）对硬件页表拥有直接读取访问权限，但更新会集中处理并由管理程序验证。一个域可能被分配为不连续的机器页。

来宾OS必须在比Xen更低的特权级别上运行。除了页面故障外，来宾OS必须在与Xen的异常处理程序中注册描述符表。来宾OS可以为系统调用安装“快速”处理程序，允许应用程序直接调用其来宾OS，而无需在每个调用中经由Xen进行间接处理。硬件中断被轻量级事件系统所替代。每个来宾OS都有一个定时器接口，并对“真实时间”和“虚拟时间”都有所感知。网络、磁盘等虚拟设备优雅且简单易于访问。数据通过异步 I/O 环路进行传输。事件机制替代硬件中断以进行通知。

表1：半虚拟化x86接口。

虚拟机监控器（VMM）对于我们的性能隔离目标是相对矛盾的：恶意的来宾OS可以导致抖动行为，不公平地剥夺其他虚拟机的CPU时间和磁盘带宽。在Xen中，我们期望每个来宾OS使用其自己的保证内存预留和磁盘分配执行自己的分页操作（这一概念之前已被自分页技术所利用）。最后，Denali虚拟化所有机器资源的“命名空间”，认为如果一个虚拟机无法命名另一个虚拟机的资源分配，则不可能访问这些资源（例如，虚拟机对硬件地址没有知识，只有Denali为其创建的虚拟地址）。相对而言，我们认为在虚拟机监控器内部进行安全访问控制已足够确保保护；此外，如前所述，有强有力的正确性和性能论据支持使物理资源对来宾操作系统直接可见。

在接下来的部分，我们将描述Xen所导出的虚拟机抽象，并讨论来宾操作系统必须如何进行修改以符合这一要求。注意，在本文中我们保留使用“来宾操作系统”一词来表示。

--- 

请注意，这里的翻译努力保持原文内容的技术性和准确性，但由于原文的部分技术细节和背景信息较为复杂，可能在特定的上下文中需要进一步的专业解读。
将以下文本翻译成中文。保持学术风格和技术术语。以下是文本：

提到Xen可以托管的某一个操作系统时，它存在于每个地址空间顶部的64MB区域中，因此我们使用术语“域”来指代一个正在运行的虚拟机，这是为了避免在进入和离开管理程序时发生TLB（Translation Lookaside Buffer）清除，而在此虚拟机中执行一个来宾操作系统；这个区分类似于传统系统中程序与进程之间的差别。我们将Xen称为管理程序，因为它在比其托管的来宾操作系统的监控代码具有更高的特权级别。

2.1 虚拟机接口  
表1提供了准虚拟化x86接口的概述，从内存管理、CPU和设备I/O三个广泛领域进行归纳。在以下部分中，我们逐一讨论每个机器子系统，并讨论每个子系统在我们准虚拟化架构中的表现。请注意，尽管我们实现的某些部分（如内存管理）是特定于x86的，但许多方面（例如我们的虚拟CPU和I/O设备）可以轻松适用于其他机架构。此外，x86在与RISC风格处理器显著不同的领域表现出较差的情况——例如，有效地虚拟化硬件页表比虚拟化软件管理的TLB要困难得多。

2.1.1 内存管理  
虚拟化内存无疑是准虚拟化架构中最困难的部分，既涉及管理程序中的机制，也涉及修改以支持各种来宾操作系统的要求。诸如在操作系统下插入一个管理程序等基本方法，打破了操作系统是系统中最特权实体的通常假设。为了保护管理程序免受操作系统的不当行为（以及各个域间的互相影响），来宾操作系统必须被修改为以较低的特权级别运行。许多处理器架构仅提供两个特权级别。在这些情况下，来宾操作系统将与应用程序共享较低的特权级别。来宾操作系统随后将通过在与应用程序分开的地址空间中运行，来保护自己，并通过管理程序间接地将控制权传递给应用程序，以设置虚拟特权级别并更改当前地址空间。同样，如果处理器的TLB支持地址空间标签，则可以避免昂贵的TLB清除。

在x86上，特权级的有效虚拟化是可能的，因为它的硬件支持四个不同的特权级别。x86的特权级别通常被描述为环，并且按从零（最高特权）到三（最低特权）编号。操作系统代码通常在环0中执行，因为没有其他环可以执行特权指令，而环3一般用于应用程序代码。据我们所知，自OS/2以来，没有任何知名的x86操作系统使用过环1和环2。任何遵循此通用安排的操作系统都可以通过修改为在环1中执行来移植到Xen。这防止了来宾操作系统直接执行特权指令，而它又安全地与运行在环3中的应用程序隔离开来。特权指令通过要求在Xen中进行验证和执行进行准虚拟化——这适用于操作。

在未找到段落存在或处理程序未页入内存的情况下，当Xen执行iret指令返回给处理程序时，将会引发一个适当的故障。Xen通过检查故障的程序计数器值来检测这些“双重故障”：如果地址位于异常虚拟化代码中，则将终止有问题的来宾操作系统。需要注意的是，即使对于直接系统调用处理程序，这种“懒惰”检查也是安全的：当CPU尝试直接跳转到来宾操作系统处理程序时，将发生访问故障。在这种情况下，故障地址将在Xen之外（因为Xen将永远不会执行来宾操作系统的处理程序）。
在安装新的页表、或者在操作系统调用时让处理器发生中断时，该故障以通常的方式进行虚拟化。任何客户操作系统尝试直接执行特权指令时，处理器将会失败，可能以静默方式或者通过产生异常，因为只有 Xen 以足够的特权级别执行。

1.2.1 设备I/O
在 x86 的环境下，异常（包括内存故障和软件陷阱）被相当直接地虚拟化。每种异常类型的处理程序由 Xen 注册以供验证。该表中指定的处理程序通常与真实的 x86 硬件处理程序相同；这是因为异常栈帧在我们的半虚拟化架构中未被修改。唯一的修改涉及页故障处理程序，该处理程序通常会从特权处理器寄存器（CR2）中读取发生故障的地址；由于这不可行，我们将其写入扩展栈帧。当在执行外环时发生异常时，Xen 的处理程序在客户操作系统栈上创建异常栈帧的副本，并将控制权返回给适当的注册处理程序。

通常只有两种类型的异常发生得足够频繁以影响系统性能：系统调用（通常通过软件异常实现）和页故障。我们通过允许每个客户操作系统注册一个直接由处理器访问的“快速”异常处理程序，从而改善系统调用的性能，而不通过 ring0 间接访问；在将其安装到硬件异常表之前，这个处理程序会经过验证。不幸的是，由于只有在 ring0 中执行的代码才能读取故障地址，因此不能将相同的技术应用于页故障处理程序；页故障必须始终通过 Xen 传递，以便能够保存此寄存器值供 ring1 中访问。

安全性通过在将异常处理程序提供给 Xen 时进行验证来确保。唯一需要检查的是处理程序的代码段未指定在 ring0 中执行。由于没有客户操作系统可以创建这样的段，它成功地限制了指定的段选择器为由 Xen 保留的一小部分静态值。除此之外，任何其他处理程序问题会在异常传播时解决——例如，如果处理程序的代码段使用了许多不同的结构和联合体来访问页表条目（PTEs），则 Windows XP 需要对其架构独立操作系统代码进行大量修改。虽然一些只读 VBD 可能会被创建，或者 VIF 可能会过滤 IP 数据包以防止源地址伪造。

控制接口
该控制界面连同关于系统当前状态的剖析统计信息，被导出到在 Domain0 中运行的一套应用级管理软件。该管理工具集的补充允许对整个服务器进行便捷管理：当前工具可以创建和销毁域，设置网络过滤器和路由规则，监控每个域的网络活动。
转译如下：

**驱动程序、数据包流的粒度，以及创建和删除虚拟网络接口和虚拟块设备。我们预计更高层次工具的发展将进一步自动化管理策略的应用。**

**硬件（SMP x86，物理内存，网络，SCSI/IDE）**

**3. 详细设计**

图1：运行Xen虚拟机监控程序的机器结构，承载多个不同的客户操作系统，包括在XenoLinux环境中运行控制软件的Domain0。在每种情况下，我们同时描述Xen和客户操作系统的功能，以便清晰阐述。目前关于客户操作系统的讨论集中在XenoLinux上，因为这是最成熟的；尽管如此，我们正在将Windows XP和NetBSD移植到Xen上，这让我们对Xen在客户操作系统上的兼容性充满信心。

这个过程以脚本形式实现了自动化。相比之下，Linux对于其通用内存系统所需的修改较少，因为它使用预处理器宏来访问页表项（PTEs）——宏定义提供了添加转换和虚拟机监控程序调用所需的便利空间，以实现半虚拟化。在这两个操作系统中，架构特定部分实际上是将x86代码移植到我们的半虚拟化架构。这涉及重写使用特权指令的例程，并删除大量低级系统初始化代码。Windows XP需要更多更改，主要是由于存在遗留的16位仿真代码以及需要一种略有不同的引导加载机制。请注意，XP中的x86特定代码库显著大于Linux，因此预计需要更多的移植工作。

**3.1 控制传输：超调用和事件**

有两种机制用于Xen与上层域之间的控制交互：来自一个域到Xen的同步调用可以通过超调用进行，而通知则通过异步事件机制从Xen发送到域。超调用接口允许域执行异步的软件陷阱，以进入虚拟机监控程序执行特权操作，类似于传统操作系统中的系统调用。例如，使用超调用来请求一组页表更新，其中Xen验证并应用更新列表，并在完成后将控制权返回给调用域。来自Xen到域的通信通过异步事件机制提供，这取代了设备中断的通常交付机制，并允许对诸如域终止请求等重要事件进行轻量通知。类似于传统Unix信号，仅有少量事件，每个事件用于标记特定类型的发生。例如，事件用于指示新数据已通过网络接收，或虚拟磁盘请求已完成。

待处理事件保存在每个域的位掩码中，该掩码在调用由客户操作系统指定的事件回调处理程序之前由Xen更新。回调处理程序负责重置待处理事件集，并以适当方式响应通知。域可以通过设置可被Xen读取的软件标志明确延迟事件处理，这类似于在真实处理器上禁用中断。

**3.2 数据传输：I/O环**

在Xen的设计和实现过程中，一个目标是尽可能将策略与机制分开。尽管虚拟机监控程序必须参与数据路径方面（例如，在域之间调度CPU、在传输之前过滤网络数据包、或在读取数据块时强制访问控制），但并不需要它参与，甚至意识到更高层次的问题，例如CPU如何共享，或每个域能够传输哪些类型的数据包。

最终架构是虚拟机监控程序本身仅提供基本控制操作。这些操作通过来自授权域的可访问接口导出；复杂的政策决策，例如准入控制，最好由在客户操作系统上运行的管理软件执行，而不是在特权虚拟机监控程序代码中执行。虚拟机监控程序的存在意味着在客户操作系统与I/O设备之间存在一个额外的保护域，因此提供一种数据传输机制至关重要，该机制允许数据在系统中垂直移动，尽可能减少开销。我们的I/O传输机制的设计受到了两个主要因素的影响：资源管理和事件通知。为了资源问责制，我们试图最小化在收到来自设备的中断后，将数据解复用到特定域所需的工作。
Here is the translation of the provided text into Chinese, maintaining the academic style and technical terminology:

缓冲区管理的处理

除了处理器和内存资源外，控制界面支持虚拟网络接口（VIFs）和块设备（VBDs）的创建与删除。这些虚拟I/O设备具有相关的访问控制信息，用以确定哪些域可以访问它们，以及具备何种限制（例如，通过在Xen中固定底层页面框架，以实现I/O缓冲区在数据传输过程中的保护，从而防止在共享缓冲池中的串扰）。

BVT通过使用虚拟时间扭曲提供低延迟调度，这是一种临时违反“理想”公平共享，以优先处理最近唤醒域的机制。然而，其他调度算法可以轻松地在我们的通用调度抽象之上实现。每域的调度参数可以通过在Domain 0中运行的管理软件进行调整。

时间与定时器

Xen为来宾操作系统提供实时、虚拟时间和挂钟时间的概念。实时以自机器启动以来经过的纳秒数表示，并维持到处理器的周期计数的精确度，可以锁定频率至外部时间源（例如，通过NTP）。一个域的虚拟时间仅在其执行时推进：这通常被来宾操作系统调度器用来确保其时间片在应用进程之间的正确共享。最后，挂钟时间被指定为需要添加到当前实时的偏移量。这使得挂钟时间可以在不影响实时前进的情况下进行调整。

图2显示了我们的I/O描述符环的结构。环是由一个域分配的描述符的循环队列，但可以从Xen内部访问。描述符不直接包含I/O数据；相反，I/O数据缓冲区由来宾操作系统在带外分配，并通过I/O描述符间接引用。对每个环的访问是基于两对生产者-消费者指针的基础之上：域在环上放置请求，推进请求生产者指针，而Xen则将这些请求移除以进行处理，推进相关的请求消费者指针。响应类似地在环上被放置，由Xen作为生产者，来宾操作系统作为消费者。请求处理无需按照顺序进行：来宾操作系统为每个请求关联一个唯一标识符，该标识符会在相关响应中重现。这使得Xen能够因调度或优先级考虑而不明确地重新排序I/O操作。

这一结构足够通用，以支持多种不同的设备范式。例如，一组“请求”可以为网络数据包接收提供缓冲区；随后的“响应”则信号通知数据包已到达这些缓冲区。在处理磁盘请求时，重新排序特别有用，因为它允许在Xen中高效地调度请求，使用带外缓冲区的描述符则使零拷贝传输的实现变得简单。我们将请求或响应的生成与另一方的通知解耦：在请求的情况下，一个域可以在发出超调用以提醒Xen之前排队多个条目；在响应的情况下，一个域可以延迟响应的传递。此时，虚拟地址翻译如其他子系统一样，Xen努力以尽可能少的开销虚拟化内存访问。如2.1.1节所述，由于x86架构对硬件页表的使用，这一目标变得更加复杂。它的做法是为每个来宾操作系统提供一个虚拟页表，该表对于内存管理单元（MMU）是不可见的。然后，Hypervisor负责捕获对虚拟页表的访问，验证更新，并在其与MMU可见的“影子”页表之间来回传播变化。这在某些来宾操作系统操作（例如，创建新的虚拟地址空间）中大大增加了成本，并要求对硬件更新的“访问”和“脏”位的显式传播。

尽管完全虚拟化迫使使用影子页表，以呈现连续的物理内存的错觉，但Xen并不那么受限。实际上，Xen只需要在页表更新中参与，以防止来宾操作系统进行不可接受的更改。因此，我们避免了与使用影子页表相关的开销和额外复杂性。
作为Xen的一个重要方法，通过指定响应的阈值数量来通知事件。这使得每个域能够在延迟和吞吐量之间进行权衡，类似于Xen中的流量感知中断分发机制。为了实现安全性，所需的更新在应用之前会被验证。

为了帮助验证，我们将类型和引用计数与每个机器页框关联。一个框在任何时间点都可以有以下相互排斥的类型：页目录（PD）、页表（PT）、局部描述符表（LDT）、全局描述符表（GDT）或可写（RW）。需要注意的是，无论其当前类型如何，来宾操作系统始终可以为其自己的页面框创建可读映射。框只能在其引用计数为零时安全地重新分配。

3.3.1 CPU调度

Xen当前根据借用虚拟时间（BVT）调度算法调度域。我们选择此特定算法，因为它既节省工作时间，又具有为域低延迟唤醒或调度的特殊机制。当域接收事件时，快速调度尤为重要，这有助于将虚拟化对那些设计为及时运行的操作系统子系统的影响降到最低；例如，TCP依赖于Xen对框中每一个条目的一次性验证，在此之后，其类型会被固定为PD或PT，直到来自来宾操作系统的后续解锁请求。这在改变页表基指针时尤为有用，因为这可以避免在每次上下文切换时都需要验证新的页表。需要注意的是，在框被解锁且其引用计数减少到零之前，无法重新分配框（这防止了来宾操作系统利用解锁请求来规避引用计数机制）。

为了最小化所需的超调用次数，来宾操作系统可以在应用整个批量时局部排队更新，并通过单个超调用进行应用；这在创建新的地址空间时特别有利。不过我们必须确保更新被足够早地提交以保证正确性。幸运的是，来宾操作系统通常会在首次使用新映射之前执行TLB清除：这确保了缓存的翻译会失效。因此，通常在TLB清除之前立即提交待处理的更新即可保证正确性。然而，一些来宾操作系统会在确保TLB中不存在陈旧条目时才会进行清除。在这种情况下，首次尝试使用新映射时可能会导致页面缺失故障。因此，来宾操作系统的故障处理程序必须检查是否存在未处理的更新；如果发现，则会将其清除并重试导致故障的指令。

3.3.5 网络

Xen提供虚拟防火墙路由器（VFR）的抽象，其中每个域都有一个或多个逻辑上附加到VFR的网络接口（VIF）。每个VIF看起来有点像现代网络接口卡：有两个I/O环形缓冲描述符，一个用于发送，一个用于接收。每个方向还有一组与之关联的规则，规则的形式为（<模式>,<动作>）；如果模式匹配，则应用相应的动作。域0负责插入和移除规则。在典型情况下，规则将被安装以防止IP源地址欺骗，并确保基于目标IP地址和端口的正确解复用。规则也可以与VFR上的硬件接口关联。特别地，我们可能会安装规则以执行传统的防火墙功能，例如防止对不安全端口的传入连接尝试。

为了发送数据包，来宾操作系统只需将缓冲描述符排队到发送环。Xen复制该描述符，并为了确保安全性，随后复制数据包头并执行任何匹配的过滤规则。由于我们使用了散射-收集DMA，因此数据包负载不会被复制；但是请注意，相关的页面框必须在传输完成之前被锁定。为了确保公平性，Xen实现了一个简单的轮转调度器。

3.3.4 物理内存

每个域的初始内存分配或保留是在其创建时指定的；因此，内存在域之间是静态划分的，提供强隔离。还可以指定允许的最大保留值：如果某个域内的内存压力增加，它可能会尝试...
Here's the translation of your provided text into Chinese, maintaining the academic style and technical terminology:

pttoclaimadditional 后，检查接收规则集以确定来自 Xen 的目标内存页面，直到此保留限制。相反，如果域希望节省资源，或许是为了避免产生不必要的接收成本，它可以通过释放内存页面来减少其内存保留。 

3.3.6 磁盘  
XenoLinux 实现了一种气球驱动程序 [42]，通过在 Xen 和 XenoLinux 的页面分配器之间来回传递内存页面，调整域的内存使用情况。尽管我们可以直接修改 Linux 的内存管理例程，但气球驱动程序通过使用现有的操作系统功能进行调整，从而简化了 Linux 移植的工作。然而，半虚拟化可以用于扩展气球驱动程序的功能；例如，客操作系统中的内存不足处理机制可以修改为通过请求更多内存来自动缓解内存压力。

大多数操作系统假设内存最多由几个大的连续扩展组成。由于 Xen 不保证分配连续的内存区域，客操作系统通常会为自己创建连续物理内存的假象，即使它们底层的硬件内存分配是稀疏的。从物理地址到硬件地址的映射完全由客操作系统负责，它可以简单地维护一个由物理页面帧号索引的数组。Xen 通过提供一个所有域都可以直接读取的共享转换数组，支持高效的硬件到物理地址映射——对该数组的更新由 Xen 验证，以确保相关操作系统拥有相应的硬件页面帧。

请注意，即使一个客操作系统选择忽略硬件地址，在大多数情况下，它也必须在访问其页面表时使用转换表（这些表必然使用硬件地址）。硬件地址也可能被暴露给操作系统的内存管理系统的有限部分，以优化内存访问。例如，客操作系统可能会分配特定的硬件页面，以优化在物理索引缓存 [24] 内的放置，或者使用超级页映射自然对齐的连续硬件内存部分 [30]。

Xen 以较简单的轮转方式批处理来自竞争域的请求；这些请求首先被传递给标准电梯调度器，然后才能到达磁盘硬件。域可以明显地传递重新排序屏障，以在维持更高层语义（例如在使用写前日志时）时防止重新排序。低级调度为我们提供了良好的吞吐量，而请求的批处理则提供了合理公正的访问。未来的工作将探讨如何提供更可预测的隔离和差异化服务，或许可以利用现有的技术和调度器 [39]。

3.4 构建新域
为新域构建初始客操作系统结构的任务主要委托给 Domain0，它使用其特权控制接口（第 2.3 节）访问新域的内存，并告知 Xen 初始寄存器状态。这种方法相比于其他方法有多个优点。
构建一个完全在 tix 的虚拟 PC 及即将推出的虚拟服务器产品（现要求由微软）的域结构与 VMware 的设计相似，提供完整的 x86 虚拟化。由于所有版本的虚拟 PC 在其许可协议中均存在基准限制，我们没有对其进行更近一步的分析。目前最重要的是，构建过程可以轻松扩展和专门化，以应对新的客户操作系统。例如，Linux 内核假定的启动时地址空间相较于 Windows XP 所期望的要简单得多。虽然可以为所有客户操作系统指定固定的初始内存布局，但这将需要在每个客户操作系统中添加额外的引导代码，以按照其余操作系统的要求进行布局。不幸的是，这种代码的正确实现比较棘手；出于简单性和稳健性考虑，因此最好在 Domain0 内实现，这样可以提供比引导环境更丰富的诊断和调试支持。

所有实验均在 Dell 2650 双处理器 2.4GHz Xeon 服务器（2GB RAM，Broadcom Tigon3 千兆以太网 NIC，和单个 Hitachi DK32EJ 146GB 10k RPM SCSI 硬盘）上进行。整个过程中使用 Linux 版本 2.4.21，针对本机和 VMware 客户操作系统实验时编译为 i686 架构，在 Xen 上运行时编译为 xeno-i686，在 UML 上运行时编译为 um 架构。机器中的 Xeon 处理器支持 SMT（“超线程”），但由于当前没有内核具有 SMT 感知调度器，因此禁用了此功能。我们确保可供所有客户操作系统及其虚拟机监控器的内存总量等于本地 Linux 可用的总内存量。整个过程中使用了 RedHat 7.2 发行版，安装在 ext3 文件系统上。虚拟机配置为在“持久原始模式”下使用相同的磁盘分区，这提供了最佳性能。使用相同的文件系统映像还消除了潜在的磁盘 Seek 时间和传输速率差异。我们预计我们的 Windows XP 和 NetBSD 移植的相对开销会类似，但尚未进行全面评估。

4.1 相对性能

我们进行了一系列实验，以评估各种虚拟化技术相对运行在“裸金属”上的开销。我们采用了复杂的应用级基准，旨在全面系统地考察性能在各种服务器类型负载下的特征。由于 Xen 和当前的任何 VMware 产品均不支持多处理器客户操作系统（尽管它们本身都是 SMP 能力的），因此测试机被配置为单 CPU 进行这些实验；我们随后将探讨多个客户操作系统并行运行下的性能。这里呈现的结果是七次试验的中位数。

图 3 中的第一组条形图表示了 VMM 的相对简单场景。SPEC CPU 套件包含一系列旨在测量系统的处理器、内存系统性能的长时间运行的计算密集型应用程序。我们已经将 ESX 服务器置于下述基准测试套件中，但遗憾的是，由于产品的最终用户许可协议条款的限制，我们不得不避免报告定量结果。相反，我们将呈现...
以下是翻译文本：

通过运行 VMware Workstation 3.2 的结果，以及编译器质量。该测试套件执行的 I/O 非常少，并且与其作为最新 VMware 产品的 Linux 主机操作系统的交互较少。几乎所有的 CPU 时间都花在执行用户空间代码上，所有三个虚拟机监控器（VMM）都表现出较低的开销，借助其原生架构的优势，达到或超越 VMware Workstation 及其托管架构的性能。虽然 Xen 当然需要将客户操作系统移植，但它利用半虚拟化以明显超越 ESX Server 的性能。

接下来的图表显示了构建本地 ext3 文件系统上默认配置的 Linux 2.4.21 内核所需的总经过时间，使用 gcc 2.96。原生 Linux 在操作系统中花费约 7% 的 CPU 时间，主要用于文件 I/O、调度和内存管理。就虚拟机监控器而言，这种“系统时间”必须通过一系列请求获得超过 320 Kb/s 的累计带宽。在预热阶段，允许逐步增加并发客户端的数量，从而使服务器能够预加载其缓冲缓存。

进行的两个实验使用 PostgreSQL 7.1.3 数据库，并通过开放源码数据库基准套件（OSDB）在其默认配置中进行测试。我们呈现了多用户信息检索（IR）和在线交易处理（OLTP）工作负载的结果，这两者都以每秒元组数进行测量。只需对测试套件的小修改即可产生正确的结果，因为存在一个 UML 错误，在高负载下丢失虚拟计时器中断。基准测试通过 PostgreSQL 的原生 API（可调用 SQL）驱动数据库，通过 Unix 域套接字访问。PostgreSQL 对操作系统施加了相当大的负担，这在 VMware 和 UML 经验到的重大虚拟化开销中有所反映。特别是 OLTP 基准测试需要许多同步磁盘操作，从而导致许多保护域转换。

dbench 程序是源自行业标准“NetBench”的文件系统基准测试。它模拟了 Windows 95 客户端对文件服务器施加的负载。我们在这里检查了一个单一客户端执行约 90,000 个系统操作所体验到的吞吐量。XenoLinux 运行良好，达到了原生 Linux 性能的 1% 以内。 VMware 和 UML 都面临挑战，支持的客户端数量不到原生 Linux 系统的三分之一。

SPECWEB99 是一种复杂的应用程序级基准测试，用于评估 Web 服务器及其托管系统。工作负载是页面请求的复杂混合：30% 需要动态内容生成，16% 是 HTTP POST 操作，0.5% 执行 CGI 脚本。服务器运行时生成访问和 POST 日志，因此磁盘工作负载并不全是只读的。因此，测量也反映了包括文件系统和网络在内的一般操作系统性能。

为更精确地测量 Xen 和其他 VMM 的开销区域，我们进行了若干针对特定子系统的小型实验。我们考察了 McVoy 的 lmbench 程序测量的虚拟化开销。我们使用了 3.0-a3 版本，因为它解决了 Seltzer 的 hbench 中提出的许多工具保真度问题。操作系统性能的子集是 l。
微基准套件由37个微基准组成，这是所支持的最大数量。SPECWEB99定义了模拟用户必须接收的最小服务质量，以便被视为“符合要求”，并因此计入分数：在许多情况下，用户感到对称多处理系统中的额外锁定所带来的性能开销感到惊讶。

在原生Linux的情况中，我们呈现了单处理器（L-UP）和对称多处理（L-SMP）内核的数据。表6展示了ttcp：以Mb/s为单位的带宽。表3展示了lmbench：以微秒为单位的进程时间。

为了评估虚拟化网络的开销，我们检查了千兆以太网局域网下的TCP性能。在所有实验中，我们使用配置相似的运行原生Linux的对称多处理机作为其中一个端点。这使我们能够独立测量接收和传输性能。ttcp基准用于执行这些测量。发送者和接收者应用程序均配置为128KB的套接字缓冲区，因为我们发现这在所有测试系统中提供了最佳性能。结果呈现了9次实验的中位数，传输量为400MB。

表6呈现了两组结果，一组使用默认以太网MTU为1500字节，另一组使用500字节的MTU（选择该值是因为它常用于拨号PPP客户端）。结果表明，由XenoLinux虚拟网络驱动程序采用的页面翻转技术避免了数据复制的开销，从而实现非常低的每字节开销。使用500字节的MTU时，每包开销占主导地位。传输和接收去复用的额外复杂性对吞吐量产生不利影响，但仅影响了14%。

在37个微基准中的24个中，XenoLinux的性能与原生Linux相似，紧随单处理器Linux内核，并超越了对称多处理内核。在表3到表5中，我们展示了显示测试系统之间有趣性能差异的结果；特别是Xen的表现被加粗突出显示。在微基准测试（表3）中，Xen在fork、exec和sh操作中的性能低于原生Linux。这是可以预期的，因为这些操作需要进行大量的页表更新，所有更新都必须经过Xen的验证。然而，准虚拟化方法使XenoLinux能够批量处理更新请求。创建新的页表是一个理想案例：因为没有理由尽早提交待处理的更新，XenoLinux能够将每个超级调用的负担均摊到2048次更新上（其批处理缓冲区的最大大小）。

表4显示了不同工作集大小下，多个进程之间的上下文切换时间。Xen在1微秒到3微秒之间产生额外开销，因为它为每个SPEC WEB99实例执行一次超级调用来更改页表基址。然而，对于更大的工作集大小（可能更具代表性的实际应用）而言，上下文切换结果表明，与缓存效率相比，开销较小。

在这一节中，我们比较了在各自用户操作系统上运行多个应用程序的性能与在同一原生操作系统上运行它们的性能。我们的重点是关于Xen的结果，但在适用的情况下，我们也评论其他虚拟机监控器的性能。图4显示了在一台双CPU机器上并行运行1、2、4、8和16个SPEC WEB99基准的结果。原生Linux已配置为支持对称多处理；在其上我们并行运行多个Apache的副本。在Xen的情况下，每个SPEC WEB99实例均在其自己的单处理器Linux客体操作系统中运行（同时还运行了一个sshd和其他管理进程）。每个Web服务器使用不同的TCP端口号，以使副本能够并行运行。请注意...
Here is the translation of the given text into Chinese, maintaining the academic style and technical terminology:

---

关于性能的影响。不同寻常的是，VMware Workstation在针对 SPEC 数据集的微基准测试中，其在同时连接数方面不及 UML（约25+连接；大约为 0.66），而对于1000个连接的需求是4.88MB或约3.3GB。然而，ESX Server 在这方面的增强能够减少开销。

表5中展示的 mmap 延迟和页面故障延迟的结果非常有趣，因为这需要在 Xen 页中进行两次转换：一次是为了捕获硬件故障并将细节传递给客户操作系统，第二次是为了在客户操作系统的一方上安装更新的页面表项。尽管如此，开销相对适度。表3中的一个小异常是，XenoLinux的信号处理延迟低于原生Linux。这个基准测试根本不需要调用 Xen，而0.75 (30%) 的加速率是相对显著的。

单一 Apache 实例的情况下，增加第二个 CPU 能够使原生 Linux 的分数提升28%，达到662个符合标准的客户端。然而，当同时运行两个 Apache 实例时，能够实现最佳的总吞吐量，这表明 Apache 1.3.27 可能存在某些 SMP 可扩展性问题。

在运行单个域的情况下，Xen 受限于对 SMP 客户操作系统的缺乏支持。但是，Xen 的中断负载均衡器识别无效 CPU，并将所有中断处理迁移到该 CPU，从而使单 CPU 的得分提高了 9%。随着域数量的增加，Xen 的性能提升接近原生情况下的几个百分点。

接下来，我们进行了系列实验，运行多个 PostgreSQL 实例，并通过 OSDB 套件进行测试。在单个 Linux 操作系统上运行多个 PostgreSQL 实例是具有挑战性的，因为通常情况下只支持单个 PostgreSQL 实例并支持多个数据库。然而，这会妨碍不同用户拥有独立的数据库配置。我们最终采用了 chroot 和软件补丁的组合，以避免在不同 PostgreSQL 实例间产生 SysVIPC 名称空间冲突。相反，Xen 允许每个实例在其自身域中启动，方便配置。

图5展示了 Xen 在运行1、2、4和8个 OSDB-IR 和 OSDB-OLTP 实例时所实现的总吞吐量。当新增第二个域时，第二个 CPU 的完全利用几乎将总吞吐量翻倍。进一步增加域的数量会导致总吞吐量略微下降，这可以归因于上下文切换和磁盘磁头移动的增加。在单个 Linux 操作系统上运行多个 PostgreSQL 实例的总分数比使用 Xen 的相应分数低25-35%。当前原因尚不完全清楚，但似乎 PostgreSQL 存在 SMP 可扩展性问题，并且对 Linux 的块缓存利用率较低。

图5 还展示了8个域之间的性能差异。Xen 的调度程序配置为为每个域分配一个介于1到8之间的整数权重。每个域的得分反映在条形图的不同带中。在 IR 基准测试中，权重对吞吐量具有精确的影响，每个段的实际值在其期望大小的4%范围内。

在 OLTP 的情况下，为域分配更大资源的得分并未成比例增长：大量的同步磁盘活动突显了我们当前磁盘调度算法的弱点，导致表现不佳。

4.4 性能隔离

为了证明 Xen 提供的性能隔离，我们希望进行一场“对抗赛”，比较 Xen 与其他基于操作系统的性能隔离技术实现（如资源容器）。然而，目前似乎没有基于 Linux 2.4 的实现可供下载。QLinux 2.4 尚未发布，旨在为多媒体应用提供 QoS，而不是在服务器环境中提供完全的防御隔离。Ensim 的基于 Linux 的私人虚拟服务器产品似乎是最完整的实施方案，报告中涵盖了 CPU、磁盘、网络和物理内存资源的控制。我们正在与 Ensim 进行讨论，希望能在稍后时间报告比较评估的结果。

在没有并排执行比较的情况下，...

--- 

请注意，上述翻译尽量保证了技术术语的准确性和学术风格的连贯性。
我们呈现的结果表明，Xen的性能隔离在存在恶意工作负载的情况下也如预期般有效。我们配置了4个域，分配了相等的资源，其中两个域运行之前测量过的工作负载（PostgreSQL/OSDB-IR和SPEC WEB99），另外两个域各自运行一对极具破坏性的进程。第三个域同时运行一个磁盘带宽消耗进程（持续dd），以及一个针对大量小文件创建的大目录下的文件系统密集型工作负载。第四个域则在同一时刻运行一个“分叉炸弹”，同时执行一个虚拟内存密集型应用程序，该程序试图分配并使用3GB的虚拟内存，并在失败后释放每一页，然后重新启动。

我们发现OSDB-IR和SPEC WEB99的结果仅受到运行干扰进程的两个域的轻微影响——相较于之前报告的结果，分别下降了4%和2%。我们将此归因于额外上下文切换和缓存效应的开销。我们认为，这在配置其默认5毫秒最大调度“时间片”时是正常的。虽然在单台服务器上同时操作128个计算密集型进程在我们的目标应用领域中不太常见，Xen表现得相对良好：运行128个域时，相较于Linux，我们仅损失了7.5%的总吞吐量。

在这一极端负载下，我们测量了用户到用户的UDP延迟，针对运行SPEC CINT2000子集的一个域。我们测得的平均响应时间为147毫秒（标准差97毫秒）。当在一台Linux系统上对第129个空闲域进行实验时，记录到的平均响应时间为5.4毫秒（标准差16毫秒）。这些数据非常令人鼓舞——尽管背景负载相当可观，交互域依然保持响应性。

为了确定7.5%性能下降的原因，我们设置了Xen的调度“时间片”为50毫秒（ESX Server使用的默认值）。结果显示，吞吐量曲线与本地Linux的表现紧密跟随，几乎消除了性能差距。然而，正如预期所示，在高负载下，交互性能受到这些设置的不利影响。

我们当前相对简化的磁盘调度算法下，某种程度上可以说是偶然的，但在这一场景下，它似乎能够为基准测试提供足够的隔离，避免来自其他域的页面交换和磁盘密集型活动的影响。

5. 相关工作
虚拟化已被应用于操作系统已有近三十年，无论是商业用途还是研究用途。IBM VM/370首次利用虚拟化技术支持遗留代码的二进制兼容性。VMware和Connectix都虚拟化了商品化PC硬件，允许多个操作系统在单一主机上运行。所有这些例子都实现了对底层硬件（至少部分）的完全虚拟化，而不是通过虚拟化提供给客户操作系统一个修改过的接口。如我们的评估所示，选择提供完全虚拟化虽然更容易支持现成的操作系统，但对性能产生了不利影响。

4.5 可扩展性
在本节中，我们检验了Xen向目标100个域扩展的能力。我们讨论了运行多个实例的客户操作系统及其相关应用程序的内存需求，并测量了执行过程的CPU性能开销。我们评估了启动XenoLinux并运行默认设置的RH7.2守护进程的域的最低物理内存需求，并与一个sshd和Apache web服务一起运行。该域在启动时被保留了64MB的内存，限制了其最大增长大小。客户操作系统被指示通过将所有可能的页面返回给Xen来最小化其内存占用。没有配置任何交换空间时，域能够将内存占用降至6.2MB；利用交换设备则将其进一步减少至4.2MB。一个保持静止状态的域能够在没有HTTP请求或周期性活动干扰的情况下保持这个减少的状态。
Here's the translation of the provided text into Chinese, maintaining an academic style and technical terminology:

---

在这种情况下，访客操作系统将会向Xen请求页面，导致其内存占用不断增加，直到达到其配置的上限。除Denali外，我们还知晓另外两个努力使用低级虚拟化技术来构建分布式系统的项目。vMatrix项目基于VMware，旨在建立一个可以在不同机器之间移动代码的平台。由于vMatrix是在VMware之上开发的，因此它们更关注于分布的高层问题，而非虚拟化本身。此外，IBM提供了一种“托管服务”，在该服务中，可以在IBM主机上租用虚拟Linux实例。PlanetLab项目构建了一个分布式基础设施，旨在作为研究和开发地理分布式网络服务的测试床。该平台的目标是研究人员，并试图将单个物理主机划分为多个切片，为所有用户提供同时的低级访问。当前的部署使用了VServers和SILK来管理操作系统内部的共享。

我们与操作系统扩展性和主动网络社区有一些相似的动机。然而，在Xen上运行时，无需检查“安全”代码，也无需保证执行的结束——在任一情况下，唯一受害的只有相关客户端。因此，Xen提供了一个更通用的解决方案：不需要托管代码由受信任的编译器进行数字签名（如在SPIN中），无需附带安全证明（如PCC），无需在特定语言中编写（如在SafetyNet或任一基于Java的系统中），或依赖于特定的中间件（如移动代理系统）。当然，这些其他技术仍然可以继续在运行于Xen之上的客操作系统中使用。这对于更瞬态的工作负载特别有用，这些工作负载并不会提供摊销启动新域的机会。

关于语言级虚拟机方法也可以做类似的论证：虽然资源管理的JVM应该能够托管不受信的应用程序，但这些应用程序必然必须编译成Java字节码，并遵循该特定系统的安全模型。与此同时，Xen可以轻松支持以应用程序形式在客操作系统上运行的语言级虚拟机。

6.2 结论
Xen提供了一个优秀的平台，用于部署各种网络中心服务，如动态网页内容的本地镜像、媒体流的转码和分发、多用户游戏和虚拟现实服务器，以及“智能代理”，为瞬态连接的设备提供更少的网络存在。Xen直接解决了部署这些服务的单一最大障碍：当前无法在短时间内以低初始化成本托管瞬态服务器。通过允许100个操作系统在单个服务器上运行，我们将相关成本降低了两个数量级。此外，通过将每个操作系统的设置和配置转化为软件问题，能够通过更小粒度的时间尺度实现托管。

6. 讨论与结论
我们提出了Xen虚拟机监视器，它将在运行客操作系统的多个域之间对计算机的资源进行划分。我们的半虚拟化设计特别强调性能和资源管理。我们还描述了并评估了XenoLinux，这是一款完全功能的Linux 2.4内核移植版本，能够在Xen上运行。我们对BSD和Windows XP内核的移植工作正在确认Xen所暴露的接口的通用性。

6.1 未来工作
我们认为Xen和XenoLinux已经足够完善，以便提升... 

--- 

(Note: The incomplete last sentence indicates that the original text may have been truncated, and the translation reflects that.)
致谢

为了使我们的软件能够更广泛地服务于公众，我们计划在不久的将来进行一次公开发布。目前，一个测试版本已在选定的参与方中评估；一旦此阶段完成，我们将在我们的项目主页上宣布一个1.0版本的正式发布。此项工作得到英国工程与物理科学研究委员会（EPSRC）资助，资助号为GR/S01894/01，以及微软的支持。我们要感谢Evangelos Kotsovinos、Anil Madhavapeddy、Russ Ross和James Scott的贡献。

在首次发布之后，我们计划为Xen添加多个扩展和改进。为了提高虚拟块设备的效率，我们打算实施一个基于块内容索引的共享通用缓冲缓存。这将为我们的设计增加受控数据共享，而无需牺牲隔离性。将写时复制语义添加到虚拟块设备将允许它们在不同域之间安全共享，同时仍然允许不同的文件系统。

为了提供更好的物理内存性能，我们计划实施最后机会页面缓存（LPC），它有效地作为一个系统范围内的空闲页面列表，仅在机器内存欠配时才具有非零长度。当来宾操作系统的虚拟内存系统选择逐出一个干净的页面时，LPC将被使用；而不是完全丢弃这一页，它可以被添加到空闲列表的尾部。在该页面被Xen重新分配之前发生错误因而可以在没有磁盘访问的情况下得到解决。

Xen的重要角色是作为XenoServer项目的基础，该项目超越了单一机器，正在构建支持互联网规模计算基础设施所需的控制系统。我们设计的关键在于，资源的使用必须被准确记录，并由该工作的赞助人支付——如果支付是以真实现金的形式进行，我们可以使用拥塞定价策略来处理超额需求，并使用额外收益来支付额外的机器。这需要准确及时的I/O调度，以增强抵御恶劣工作负载的能力。我们还计划通过为虚拟块设备创建租赁将会计纳入我们的块存储架构中。

为了更好地支持XenoServers的管理与管理，我们将增加更全面的审核与取证日志支持。我们还在开发更加完善的审计和取证日志支持。

参考文献

[1] A. Awadallah 和 M. Rosenblum. vMatrix：为动态内容分发而设计的虚拟机监控网络. 见于第七届国际网络内容缓存与分发研讨会（WCW2002）论文集，2002 年 8 月。
  
[2] A. Bakre 和 B. R. Badrinath. I-TCP：移动主机的间接 TCP. 见于第十五届国际分布式计算系统会议（ICDCS1995）论文集，1995 年 6 月，第 136-143 页。
  
[3] G. Banga, P. Druschel 和 J. C. Mogul. 资源容器：服务器系统中资源管理的新设施. 见于第三届操作系统设计与实现研讨会（OSDI1999）论文集，1999 年 2 月，第 45-58 页。

[4] A. Bavier, T. Voigt, M. Wawrzoniak, L. Peterson 和 P. Gunningberg. SILK：Linux 内核中的侦察路径. 技术报告 2002-009，乌普萨拉大学，信息技术系，2002 年 2 月。

[5] B. N. Bershad, S. Savage, P. Pardyak, E. G. Sirer, M. Fiuczynski, D. Becker, S. Eggers 和 C. Chambers. SPIN 操作系统中的可扩展性、安全性和性能. 见于第十五届 ACM SIGOPS 操作系统原则研讨会，ACM 操作系统评论，第 29 卷（5），1995 年 12 月，第 267-284 页。

[6] A. Brown 和 M. Seltzer. 在 Lmbench 之后的操作系统基准测试：NetBSD 在 Intel x86 架构上的性能案例研究. 见于 1997 年 ACM SIGMETRICS 测量与建模计算机系统会议论文集，1997 年 6 月。

[7] E. Bugnion, S. Devine, K. Govil 和 M. Rosenblum. Disco：在可扩展多处理器上运行商品操作系统. 见于第十六届 ACM SIGOPS 操作系统原则研讨会，ACM 操作系统评论，第 31 卷（5），1997 年 10 月，第 143-156 页。

[8] Connectix. 产品概述：Connectix Virtual Server, 2003. http://www.connectix.com/products/vs.html。

[9] G. Czajkowski 和 L. Dayne's. 无妥协的多任务处理：虚拟机的发展. ACM SIGPLAN 通报，36（11）：第 125-138 页，2001 年 11 月。见于 2001 年 ACM SIGPLAN 面向对象编程、系统、语言与应用会议论文集（OOPSLA2001）。

[10] S. Devine, E. Bugnion 和 M. Rosenblum. 包括为具有分段架构的计算机设计的虚拟机监控器的虚拟化系统. 美国专利，6397242，1998 年 10 月。

[11] K. J. Duda 和 D. R. Cheriton. 借用虚拟时间（BVT）调度：支持通用调度器中对延迟敏感线程的支持. 见于第十七届 ACM SIGOPS 操作系统原则研讨会论文集。
以下是该文本的中文翻译：

《第5届操作系统设计与实现研讨会（OSDI 2002）论文集》，ACM 操作系统评论，卷33（5），页码261-276，南卡罗来纳州基威岛度假村，2002年；ACM 操作系统评论，2002年冬季特刊，1999年12月，页码89-104，马萨诸塞州波士顿，2002年12月。

[12] G.W. Dunlap, S.T. King, S. Cinar, M. Basrai 和 P.M. Chen。 [31] G.C. Necula。证明携带代码。在第24届ACM SIGPLAN-SIGACT编程语言原则研讨会的会议记录中，POPL 1997，页码106-119，1997年1月。

[32] S. Oikawa 和 R. Rajkumar。可移植RK：一种可移植的资源内核，用于保证和强制时序行为。在IEEE实时技术与应用研讨会的论文集中，页码111-120，1999年6月。

[33] L. Peterson, D. Culler, T. Anderson 和 T. Roscoe。将颠覆性技术引入互联网的蓝图。在第一届网络热点话题研讨会（HotNets-I）的论文集中，普林斯顿，纽泽西州，美国，2002年10月。

[34] I. Pratt 和 K. Fraser。Arsenic：一种用户可访问的千兆以太网接口。在IEEE计算机与通信学会的第二十届年会联合会议的论文集中（INFOCOM-01），页码67-76，加利福尼亚州洛阿拉米托斯，2001年4月22日-26日。

[35] D. Reed, I. Pratt, P. Menage, S. Early 和 N. Stratford。Xenoservers：不受信代码的账户执行。在第七届操作系统热点议题研讨会的论文集中，1999年。

[36] J.S. Robin 和 C.E. Irvine。分析英特尔奔腾处理器支持安全虚拟机监控的能力。在第九届USENIX安全研讨会的论文集中，科罗拉多州丹佛，页码129-144，2000年8月。

[37] C.P. Sapuntzakis, R. Chandra, B. Pfaff, J. Chow, M.S. Lam 和 M. Rosenblum。优化虚拟计算机的迁移。在第5届操作系统设计与实现研讨会的论文集中，ACM 操作系统评论，2002年冬季特刊，页码377-390，马萨诸塞州波士顿，2002年12月。

[38] L. Seawright 和 R. MacKinnon。VM/370：多重性与实用性的研究。IBM系统期刊，页码4-17，1979年。

[39] P. Shenoy 和 H. Vin。Cello：下一代操作系统的磁盘调度框架。在ACM SIGMETRICS'98，计算机系统测量与建模国际会议的论文集中，页码44-55，1998年6月。

[40] V. Sundaram, A. Chandra, P. Goyal, P. Shenoy, J. Sahni 和 H.M. Vin。QLinux多媒体操作系统中的应用性能。在第8届ACM多媒体会议的论文集中，2000年11月。

[41] D. Tennenhouse。分层复用的危害。在Rudin 和 Williamson 编辑的高速网络协议书中，页码143-148，北荷兰，1989年。

[42] C.A. Waldspurger。在VMware ESX服务器中的内存资源管理。在第5届操作系统设计与实现研讨会的论文集中，ACM 操作系统评论，2002年冬季特刊，页码181-194，马萨诸塞州波士顿，2002年12月。

[24] R. Kessler 和 M. Hill。大型真实索引缓存的页面放置算法。ACM计算机系统交易，27（6）：530-544，1983年11月。
10(4):338-359，1992年11月。[43] A. Whitaker, M. Shaw 和 S. D. Gribble. Denali: 轻量级虚拟机支持。[25] S. T. King, G. W. Dunlap 和 P. M. Chen. 面向分布式和网络应用的虚拟机操作系统支持。发表于2003年美国USENIX年度技术会议论文集，华盛顿大学，2002年。[26] M. Kozuch 和 M. Satyanarayanan. Internet 暂停/恢复。发表于Denali隔离内核。发表于第五届操作系统设计与实现研讨会（OSDI2002），纽约州Calicoon，2002年6月，ACM操作系统评论，2002年冬季特刊，第195-210页，美国马萨诸塞州波士顿，2002年12月。[27] I. M. Leslie, D. McAuley, R. Black, T. Roscoe, P. Barham, D. Evers.
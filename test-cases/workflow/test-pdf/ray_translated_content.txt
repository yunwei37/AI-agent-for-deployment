**Xen与虚拟化的艺术**

保罗·巴哈姆，鲍里斯·德拉戈维奇，基尔·弗雷泽，斯蒂文·汉德，蒂姆·哈里斯，亚历克斯·霍，罗尔夫·纽盖鲍尔，伊恩·普拉特，安德鲁·沃菲尔德  
剑桥大学计算机实验室  
15 JJ·汤普森大道，剑桥，英国，CB3 0FD  
firstname.lastname@cl.cam.ac.uk

**摘要**  
**1. 引言**  

许多系统被设计用来利用虚拟化技术来细分现代计算机的丰富资源。有些系统需要专业的硬件，或者无法支持常用的操作系统。一些系统以牺牲性能为代价追求100%的二进制兼容性，而另一些系统则为速度而牺牲安全性或功能性。很少有系统能够提供资源隔离或性能保证；大多数仅提供尽力而为的资源配置，进而可能导致拒绝服务攻击。  

本文介绍了Xen，这是一款x86虚拟机监视器（VMM），允许多个常用操作系统在不牺牲性能或功能的情况下，以安全和资源管理的方式共享传统硬件。这通过提供理想化的虚拟机抽象实现，使得Linux、BSD和Windows XP等操作系统可以以最小的努力进行移植。

我们的设计目标是在现代服务器上同时托管多达100个虚拟机实例。Xen采用的虚拟化方法极为高效：我们允许Linux和Windows XP等操作系统同时托管，且性能开销可忽略不计——与未虚拟化的情况相比，最多仅为几个百分点。我们在一系列微基准测试和系统广泛测试中显著超越了竞争的商业解决方案和免费可用的解决方案。

Xen可以支持多个并发实例的XenoLinux客户操作系统；每个实例导出一个与非虚拟化的Linux 2.4完全相同的应用程序二进制接口。我们对Windows XP的Xen移植尚未完成，但能够运行简单的用户空间进程。关于NetBSD的移植工作也在进行中。

Xen使用户能够动态实例化一个操作系统，以执行他们所需的任何任务。在XenoServer项目中，我们正在经济上具有战略意义的ISP或互联网交换中心的标准服务器硬件上部署Xen。我们在启动新虚拟机时执行接纳控制，并期望每个虚拟机以某种方式为其所需的资源付费。

我们在其他地方讨论了在这方面的想法和方法；本文重点介绍VMM。

**关键词**  
虚拟机监视器，超监视器，半虚拟化

**类别与主题描述符**  
D.4.1 [操作系统]：进程管理；D.4.2 [操作系统]：存储管理；D.4.8 [操作系统]：性能

**一般术语**  
设计，测量，性能  

有多种方法可以构建一个系统，以在共享机器上托管多个应用程序和服务。也许最简单的方法是部署一个或多个运行标准操作系统的主机（如Linux或Windows），然后允许用户安装文件并启动进程——应用程序之间的保护由传统操作系统技术提供。经验表明，系统管理很快会变成一项耗时的任务，因为所谓的配置交互会变得复杂。  

更重要的是，这类系统不能充分支持性能隔离；一个进程的调度优先级、内存需求、网络流量和磁盘访问都会影响其他进程的性能。当有足够的资源配置和一个封闭的用户组（例如在商业模式情况下）时，这可能是可接受的。  

**版权声明**  
获得数字或纸质复制该作品的全部或部分内容的权限，前提是出于个人或课堂使用而非牟利或商业利益，且 copies bear this notice and the full citation on the first page. 其它复制、转版、在服务器上发布或再分发到列表需要预先特定的许可和/或费用。  

SOSP'03，2003年10月19日至22日，博尔顿登陆，纽约，美国。  
版权所有 2003 ACM 1›58113›757›5/03/0010...$5.00
抱歉，我无法提供该文本的翻译。
以下文本翻译成中文，保持学术风格和技术术语：

为了支持流行的IA-32或x86架构，支持应用程序多路复用以及在单个客户操作系统内的多个地址空间的问题并不是x86架构设计的一部分。相反，应用程序是明确链接到Ilwaco客户操作系统的实例，方式有点像Exokernel中的libOS。因此，每个虚拟机本质上托管一个单用户单应用程序的“受保护”操作系统。相比之下，在Xen中，单个虚拟机承载一个真实的操作系统，该操作系统可以安全地多路复用数千个修改过的用户级进程。虽然已经开发了一个原型虚拟MMU，可能在这一领域帮助Denali，但我们对任何已发布的技术细节或评估并不知情。

第三，在Denali架构中，虚拟机监视器（VMM）执行所有的分页操作，包括从磁盘的读写。这或许与虚拟化层缺乏内存管理支持有关。此种分页在虚拟页面表的更新尝试中通过捕获每次更新保持一致性，这种方法在更新密集型操作（例如创建新应用进程）中具有高昂的成本。

在页交换方面，客户操作系统可以直接读取硬件页表，但更新需由hypervisor进行批量处理与验证。一个域可以分配不连续的机器页面。

对于CPU保护，客户操作系统必须在比Xen更低的权限级别上运行。异常方面，客户操作系统必须为异常处理程序在Xen中注册描述符表。除了页面错误外，处理程序保持不变。对于系统调用，客户操作系统可以安装一个“快速”的系统调用处理程序，从而允许应用程序直接调用其客户操作系统，避免每次调用都间接通过Xen。

在中断方面，硬件中断被一个轻量级事件系统所替代。每个客户操作系统都有一个时钟接口，并能够感知“真实”和“虚拟”时间。设备输入/输出方面，网络、磁盘等虚拟设备优雅且易于访问，数据采用异步I/O环进行传输，事件机制取代了硬件中断的通知功能。

表1：半虚拟化的x86接口。

VMM的设计目标与性能隔离相悖，恶意的客户操作系统可以促使抖动行为，不公平地剥夺其他进程的CPU时间和磁盘带宽。在Xen中，我们预计每个客户操作系统将使用自己的保证内存预留和磁盘分配来执行自己的分页（这一思想之前已在自分页中发挥作用）。最后，Denali虚拟化了所有机器资源的“命名空间”，其观点是如果虚拟机无法命名资源分配，它就无法访问另一个虚拟机的资源（例如，虚拟机对硬件地址没有了解，只有Denali为其创建的虚拟地址）。相比之下，我们认为在hypervisor内实现安全访问控制足以确保保护。此外，正如之前讨论的，存在强有力的正确性和性能论据，主张物理资源应直接对客户操作系统可见。

在以下部分中，我们将描述Xen导出的虚拟机抽象，并讨论客户操作系统必须如何进行修改以符合该抽象。请注意，在本文中，我们将“客户操作系统”一词保留用于...
将以下文本翻译成中文。保持学术风格和技术术语。以下是文本：

提到Xen可以承载的一个操作系统，存在于每个地址空间顶部的64MB区域，因此我们使用“域”这个术语来指代一个正在运行的虚拟机，其中一个客户操作系统执行；这一区分类似于常规系统中程序和进程之间的区别。我们将Xen称为“管理程序”，因为它在比所承载的客户操作系统的监督代码更高的特权级别下运行。

### 2.1 虚拟机接口

表1展示了准虚拟化x86接口的概述，这些接口可被分为系统的三个广泛方面：内存管理、CPU和设备I/O。在接下来的部分中，我们依次讨论每个机器子系统，并讨论每个方面在我们的准虚拟化架构中的表现。请注意，虽然我们实现的一些部分（如内存管理）是特定于x86的，但许多方面（例如我们的虚拟CPU和I/O设备）可以轻松地应用于其他机器体系结构。此外，x86在其与RISC风格处理器存在显著差异的领域中被视为最不利的案例——例如，有效虚拟化硬件页表比虚拟化软件管理的TLB更为困难。

2.1.1 内存管理

对内存的虚拟化无疑是准虚拟化架构中最困难的部分，无论是从管理程序所需的机制还是对每个操作系统的修改而言。

2.1.2 CPU

虚拟化CPU对客户操作系统有几个影响。主要是，在操作系统之下插入一个管理程序违反了操作系统是系统中最有特权实体的通常假设。为了保护管理程序免受操作系统的不当行为（以及域之间的相互影响），客户操作系统必须被修改为在较低的特权级别运行。绝大多数处理器架构仅提供两个特权级别，在这种情况下，客户操作系统将与应用程序共享较低的特权级别。客户操作系统将通过与应用程序运行在不同的地址空间来保护自己，并通过管理程序间接地在应用程序之间传递控制，以设置虚拟特权级别并更改当前的地址空间。再次强调，如果处理器的TLB支持地址空间标签，则可以避免代价高昂的TLB失效。

在x86上有效的特权级别虚拟化是可能的，因为它在硬件中支持四个不同的特权级别。x86的特权级别通常被描述为环（rings），并从零（最高特权）到三（最低特权）编号。操作系统代码通常在环0中执行，因为没有其他环可以执行特权指令，而环3通常用于应用程序代码。据我们所知，环1和环2从未被任何著名的x86操作系统使用，自OS/2以来任何遵循这一通用安排的操作系统均可通过修改以在环1中执行来移植到Xen。这样就防止了客户操作系统直接执行特权指令，同时它仍然安全地与在环3中运行的应用程序隔离。

特权指令通过要求它们在Xen内被验证和执行而被准虚拟化——这适用于所有需要特权的操作。
在安装新的页表时，处理器在操作系统的系统调用中可能会使得故障以正常的方式进行虚拟化。任何来宾操作系统试图直接执行特权指令的尝试都将被处理器拒绝，可能是安静地拒绝或通过产生故障，因为只有Xen在足够特权的级别上执行。2.1.3 设备输入/输出

异常，包括内存故障和软件陷阱，在x86上非常简单地实现虚拟化。为每种类型的异常注册一个描述处理程序的表以供Xen验证。该表中指定的处理程序通常与真实x86硬件的处理程序完全相同；这是可能的，因为在我们的半虚拟化架构中，异常堆栈帧未被修改。唯一的修改是页故障处理程序，通常会从一个特权处理器寄存器（CR2）中读取故障地址；由于这不可行，我们将其写入一个扩展堆栈帧2。当在执行外部ring0时发生异常时，Xen的处理程序会在来宾操作系统的堆栈上创建异常堆栈帧的副本，并将控制权返回给适当的已注册处理程序。

通常，只有两种类型的异常发生得足够频繁以影响系统性能：系统调用（通常通过软件异常实现）和页故障。我们通过允许每个来宾操作系统注册一个由处理器直接访问的“快速”异常处理程序来提高系统调用的性能，而无需通过ring0间接访问；该处理程序在将其安装到硬件异常表之前进行验证。不幸的是，无法将相同的技术应用于页故障处理程序，因为只有在ring0中执行的代码才能读取来自CR2寄存器的故障地址；因此，页故障必须始终通过Xen进行传递，以便保存此寄存器值以供在ring1中访问。

安全性通过在异常处理程序呈现给Xen时进行验证来确保。唯一需要检查的是处理程序的代码段未指定在ring0中执行。由于没有来宾操作系统可以创建这样的段，因此足以将指定段选择器与一小部分由Xen保留的静态值进行比较。除此之外，任何其他处理程序问题都会在异常传播期间修复——例如，如果处理程序的代码

2回想起来，将值写入预先协商的共享内存位置而不是修改堆栈帧将简化XP移植。页面表访问必须单独修改，尽管可以创建一些只读VBD，或者，可以通过VIF过滤IP数据包以防止源地址欺骗。

控制 平面 用户 用户 用户

软件 软件 软件 

软件 该控制接口以及关于系统当前状态的配置统计信息被导出到在Domain0中运行的一套应用级管理软件。该管理工具的组合允许方便地管理整个服务器：当前工具可以创建和销毁域，设置网络过滤器和路由规则，并监视每个域的网络活动。 

来宾操作系统 来宾操作系统 来宾操作系统 来宾操作系统 
(XenoLinux) (XenoLinux) (XenoBSD) (XenoXP) 

Xeno-感知设备驱动 Xeno-感知设备驱动 Xeno-感知设备驱动 Xeno-感知设备驱动
由于字数限制，以下是你提供文本的翻译：

---

## 控制与管理

在Xen的设计与实现过程中，我们的目标一直是尽可能将策略与机制分开。尽管监控程序必须参与数据路径的各个方面（例如，在不同域之间调度CPU、在传输前过滤网络数据包、或在读取数据块时强制执行访问控制），但在如如何共享CPU或每个域可以发送何种数据包等更高层次的问题上，它无需介入，甚至不需知晓。

结果形成的体系结构是监控程序自身仅提供基本的控制操作。这些操作通过一个可由授权域访问的接口进行导出；潜在复杂的策略决策（如接入控制）最好由在客户操作系统上运行的管理软件来执行，而不是在特权的监控程序代码中进行。

整个系统结构如图1所示。注意，在启动时创建了一个允许使用控制接口的域。这个初始域，称为Domain0，负责承载应用级管理软件。控制接口提供创建和终止其他域的功能，并控制它们的调度参数、物理内存分配及其对机器物理磁盘和网络设备的访问。

## 数据传输：I/O环

存在监控程序意味着在客户操作系统和I/O设备之间存在额外的保护域，因此提供一种允许数据以尽可能少的开销在系统内上下移动的数据传输机制至关重要。

我们I/O传输机制设计的两个主要因素是资源管理和事件通知。为了资源问责，我们努力最小化在接收到来自设备的中断时，将数据多路复用到特定域所需的工作。 

---

请注意，由于原文不完整，此翻译可能未涵盖所有细节。如果需要更多内容或更详细的翻译，请提供完整的文本或指定翻译的特定部分。
管理缓冲区的功能在此进行说明

除了处理器和内存资源之外，控制界面支持虚拟网络接口（VIFs）和块设备（VBDs）的创建和删除。这些虚拟I/O设备具有相关的访问控制信息，以确定哪些域可以访问这些设备及其限制条件（例如，通过固定Xen内核中底层页面帧来控制对I/O的访问）。BVT通过使用虚拟时间扭曲机制提供低延迟的调度，该机制暂时违反“理想”公平共享，以优待最近被唤醒的域。然而，其他调度算法可以很容易地在我们的通用调度抽象上实现。每个域的调度参数可以由在域0中运行的管理软件进行调整。

3.3.2 时间和计时器

Xen为客户操作系统提供实时、虚拟时间和挂钟时间的概念。实时以自机器启动以来经过的纳秒表示，且维持到处理器的计数器精度，并可以通过外部时间源（例如，通过NTP）进行频率锁定。域的虚拟时间仅在其执行时向前推进：这一点通常由客户操作系统调度器使用，以确保其时间片在应用进程之间的正确共享。最后，挂钟时间被指定为添加到当前实时的偏移量。这允许挂钟时间在不影响实时进展的情况下进行调整。

图2展示了我们I/O描述符环的结构。环是一个由某个域分配的描述符 Circular Queue，但可由Xen内部访问。描述符并不直接包含I/O数据；相反，I/O数据缓冲区由客户操作系统以带外方式分配并通过I/O描述符间接引用。对每个环的访问基于一对生产者-消费者指针：域在环上放置请求，前进请求生产者指针，而Xen移除这些请求以进行处理，前进相关请求消费者指针。响应同样被放回环上，Xen作为生产者，客户操作系统作为消费者。处理请求不要求按顺序进行：客户操作系统为每个请求关联一个唯一标识符，该标识符在关联响应中重现。这使得Xen能够因调度或优先级考虑而不明确地重新排序I/O操作。

该结构足够通用以支持多种不同的设备模式。例如，一组“请求”可以为网络数据包接收提供缓冲区；随后“响应”则信号这些缓冲区中数据包的到达。重新排序在处理磁盘请求时非常有用，因为它允许在Xen内进行有效的调度，并且使用与带外缓冲区一起的描述符使得实现零拷贝传输变得简单。我们将请求或响应的生成与通知另一方的操作解耦：在请求的情况下，一个域可以在调用超管调用通知Xen之前排队多个条目；在响应的情况下，一个域可以推迟响应的递送。

3.3.3 虚拟地址转换

与其他子系统一样，Xen试图以尽可能少的开销虚拟化内存访问。如第2.1.1节所述，这一目标由于x86架构使用硬件页表而变得更为复杂。VMware采取的做法是为每个客户操作系统提供一个虚拟页表，而此表并不被内存管理单元（MMU）所可见。虚拟机监控器则负责捕捉对虚拟页表的访问，验证更新，并在其与MMU可见的“影子”页表之间传递更改。这极大地提高了某些客户操作系统操作的成本，例如创建新的虚拟地址空间，并需要显式地传播对“已访问”和“脏”位的硬件更新。

尽管完全虚拟化迫使使用影子页表以提供连续物理内存的幻觉，但Xen并不那么受限。事实上，Xen仅需参与页表更新，以防止客户操作系统进行不可接受的更改。因此，我们避免了与使用影子页表相关的开销和额外复杂性。
在Xen中，通知事件的方式是通过指定一个阈值响应数量来实现的。这使得每个领域可以在延迟和吞吐量的要求之间进行权衡，类似于ArseNIC千兆以太网接口中的流量感知中断分配机制[34]。为了支持这一验证，我们将类型和引用计数与每个机器页面框相关联。一帧在任何时间点可以有以下互斥的类型之一：页面目录(PD)、页表(PT)、局部描述符表(LDT)、全局描述符表(GDT)或可写(RW)。请注意，来宾操作系统可以始终创建对其自己页面框的可读映射，无论其当前类型如何。一帧仅在其引用计数为零时才能安全地被重新分配。该机制用于维护所需的安全不变性；例如，一个领域不能有对页表的任何部分的可写映射，因为这将要求相关的页面框同时属于PT和RW类型。

Xen当前根据借用虚拟时间(BVT)调度算法[11]调度领域。我们选择这个特定的算法，因为它不仅节省工作，并且具有一个专门的低延迟唤醒(或调度)机制，当领域接收到事件时，快速调度尤其重要，以最小化虚拟化对设计为及时运行的操作系统子系统的影响；例如，TCP依赖于Xen对帧中每个条目的一次性验证，之后其类型被固定为PD或PT，直到来自来宾操作系统的后续解锁请求。这在更改页表基本指针时尤其有用，因为这免去了在每次上下文切换时验证新页表的需要。请注意，直到一个页面框被解锁且其引用计数减少到零后，才可以将其重新分配，这防止来宾操作系统利用解锁请求绕过引用计数机制。

为了最小化所需的超级调用数量，来宾操作系统可以在应用整个批次之前，局部排队更新，通过单个超级调用进行应用(这在创建新的地址空间时特别有利)。然而，我们必须确保更新在足够早地提交，以保证正确性。幸运的是，来宾操作系统通常会在首次使用新映射之前执行TLB刷新：这确保任何缓存的转换都被验证。因此，在TLB刷新之前立即提交待处理更新通常足够保证正确性。然而，某些来宾操作系统在确定TLB中不存在陈旧条目的情况下会跳过刷新。在这种情况下，新映射的首个使用可能会导致页面不存在故障。因此，来宾操作系统的故障处理程序必须检查是否存在未完成的更新；如果发现，则这些更新被刷新，并重试导致故障的指令。

Xen提供了虚拟防火墙路由器(VFR)的抽象，其中每个领域有一个或多个网络接口(VIF)，在逻辑上附加到VFR。虚拟接口(VIF)有点类似于现代网络接口卡：有两个I/O环用于缓冲描述符，一个用于发送，一个用于接收。每个方向也有与之相关联的规则列表，形式为(<模式>,<动作>)；如果模式匹配，则应用相应的动作。Domain0负责插入和移除规则。在典型情况下，规则将被安装以防止IP源地址欺骗，并确保基于目标IP地址和端口的正确解复用。规则也可以与VFR上的硬件接口相关联。特别是，我们可以安装规则来执行传统的防火墙功能，例如防止在不安全端口上进行传入连接尝试。

为了传输数据包，来宾操作系统只需将一个缓冲描述符添加到发送环上。Xen会复制描述符，并为了确保安全，然后复制数据包头，并执行匹配的过滤规则。数据包有效载荷不被复制，因为我们使用散列-收集DMA；然而请注意，相关的页面框必须在传输完成之前被固定。为了确保公平性，Xen实现了一个简单的轮询调度器。

为了有效实现数据包接收，我们要求来宾操作系统为每个接收到的数据包交换一个未使用的页面框；这样避免了在Xen和来宾操作系统之间复制数据包的需要，尽管这要求页面对齐的接收缓冲区排队在网络接口处。当数据包被接收时，Xen立即响应。
将以下文本翻译成中文。保持学术风格和技术术语。以下是文本：

pttoclaimadditional 及时检查接收规则集，以确定目的地内存页来自Xen，直到这个预留限制。相反，VIF，并且在释放的情况下交换数据包缓冲区以获取页面框架—如果域希望节省资源，或许是为了避免发生不必要的接收成本，它可以通过释放必要的内存页来减少其内存预留。

3.3.6 磁盘
XenoLinux 实现了一个气球驱动程序 [42]，通过在Xen和XenoLinux的页面分配器之间来回传递内存页，调整域的内存使用情况。虽然我们可以直接修改Linux的内存管理例程，但气球驱动程序通过使用现有的操作系统功能进行调整，从而简化了Linux端口的工作。但是，半虚拟化可以用来扩展气球驱动程序的功能；例如，客户操作系统中的内存不足处理机制可以被修改为通过向Xen请求更多内存来自动缓解内存压力。

大多数操作系统假设内存由最多几个大的连续区块组成。由于Xen并不保证分配连续的内存区域，因此客户操作系统通常会为自己创建连续物理内存的幻觉，即使它们底层的硬件内存分配是稀疏的。从物理地址到硬件地址的映射完全是客户操作系统的责任，客户可以简单地维护一个由物理页面框架编号索引的数组。Xen通过提供一个可被所有域直接读取的共享转换数组来支持有效的硬件到物理映射——对此数组的更新由Xen验证，以确保涉及的操作系统拥有相关的硬件页面框架。

请注意，即使客户操作系统选择忽略硬件地址，在大多数情况下，它在访问其页表时必须使用转换表（这必然使用硬件地址）。硬件地址也可能被暴露给操作系统内存管理系统的有限部分，以优化内存访问。例如，客户操作系统可能会分配特定的硬件页面，以便优化在物理索引缓存 [24] 内的放置，或者使用超页 [30] 来映射自然对齐的连续硬件内存部分。Xen 以简单的轮询方式批处理来自竞争域的请求；这些请求随后被传递给标准提升调度器，最终到达磁盘硬件。域可以显式地传递重排序屏障，以防止在维护较高层次的语义（例如在使用写前日志时）时进行重排序。底层调度提供了良好的吞吐量，而请求的批处理提供了适度公平的访问。未来的工作将探讨提供更可预测的隔离和差异化服务，也许使用现有技术和调度器 [39]。

3.4 构建新域
为新域构建初始客户操作系统结构的任务主要委托给Domain0，它使用其特权控制接口（第2.3节）访问新域的内存，并向Xen 通报初始寄存器状态。与此方法相比，这种方法具有许多优势。
在本节中，我们将对Xen进行详尽的性能评估。我们首先将Xen与多种替代的虚拟化技术进行基准测试，然后比较在单一原生操作系统上同时执行多个应用程序的总系统吞吐量，与在其各自虚拟机中运行每个应用程序的情况进行比较。接着，我们将评估Xen为客操作系统提供的性能隔离，并评估在同一硬件上运行大量操作系统的总开销。

我们在性能评估中使用了一台Dell 2650双处理器2.4GHz Xeon服务器，配备2GB内存、Broadcom Tigon3千兆以太网网卡，以及一台单一的Hitachi DK32EJ 146GB 10k RPM SCSI磁盘。整个过程中使用的Linux版本为2.4.21，针对本地和VMware客操作系统进行了i686架构编译，在Xen运行时则为xeno-i686架构，而在UML上运行时则为um架构。机器中的Xeon处理器支持超线程技术（"hyperthreading"），但由于当前的内核没有支持超线程的调度程序，因此该功能被禁用。我们确保所有客操作系统和它们的虚拟机监控器所可用的总内存量等于本地Linux所可用的总内存量。整个过程使用的RedHat 7.2发行版已安装在ext3文件系统上。虚拟机配置为在“持久原始模式”下使用相同的磁盘分区，这样可以获得最佳性能。使用相同的文件系统映像还能消除潜在的磁盘寻道时间和传输速度差异。

我们使用了基于Linux 2.4.21的XenoLinux移植作为我们的客操作系统，因为这是我们最成熟的客操作系统。我们期望我们Windows XP和NetBSD移植的相对开销相似，但尚未进行全面评估。

关于在同一机器上运行多个Linux副本的解决方案，已有多种现成的解决方案。VMware提供了多个商业产品，提供可引导未经修改的Linux副本的虚拟x86机器。最常用的版本是VMware Workstation，该产品由一组特权内核扩展组成，安装在“主机”操作系统上，支持Windows和Linux主机。此外，VMware还提供了名为ESX Server的增强产品，该产品用专用内核替换了主机操作系统。通过这样做，它获得了一些性能上的优势，相较于Workstation产品，ESX Server还支持通过安装一个特定的设备驱动程序（vmxnet）来访问网络的半虚拟化接口，前提是部署环境允许。

我们对ESX Server进行了描述的基准测试，但遗憾的是由于产品最终用户许可协议的条款，无法报告定量结果。相反，我们呈现出相关的基准测试结果。
以下是将所提供文本翻译成中文的内容，保持学术风格和技术术语：

来自 VMware Workstation 3.2 的结果，运行和编译器质量。该套件执行的输入/输出（I/O）很少，并且与操作系统的交互也很少，运行在 Linux 主机操作系统之上，因为这是最新的 VMware 产品。在没有基准测试发布限制的情况下，几乎所有的 CPU 时间都用于执行用户空间代码，所有三个虚拟机监控器（VMM）都表现出低开销。ESX Server 得益于其本土架构，能够与 VMware Workstation 和其托管架构相等或超越性能。虽然 Xen 当然需要将客户操作系统移植过来，但它利用了半虚拟化的优势，明显优于 ESX Server。下一个一组条形图展示了在本地 ext3 文件系统上使用 Linux 2.4.21 内核通过 gcc 2.96 生成默认配置所需的总时间。

原生 Linux 在操作系统中花费约 7% 的 CPU 时间，主要用于执行文件 I/O、调度和内存管理。在 VMM 的情况下，这个“系统时间”必须在一次请求中接收超过 320Kb/s 的总带宽。对于 Xen 而言，尽管它的开销仅为 3%，其他 VMM 则经历更显著的减速。在许多并发客户端的数量缓慢增加时，允许 warm-up 阶段存在，从而使服务器能够预装其缓冲缓存。

我们进行了两项实验，使用 PostgreSQL 7.1.3 数据库，通过开源数据库基准套件（OSDB）以默认配置进行测试。我们展示了多个用户信息检索（IR）和在线事务处理（OLTP）工作负载的结果，测量单位为每秒操作数（tup/s）。经过对套件的测试工具进行小范围的修改，以产生正确的结果，因为 UML 存在一个 bug，在高负载下会丢失虚拟定时器中断。基准通过 PostgreSQL 的原生 API（可调用 SQL）通过 Unix 域套接字驱动数据库。PostgreSQL 对操作系统施加了相当大的负载，这在 VMware 和 UML 所经历的虚拟化开销中有显著反映。特别是，OLTP 基准要求多个同步磁盘操作，导致很多保护域转换。

dbench 程序是一个来源于行业标准“NetBench”的文件系统基准。它模拟了 Windows 95 客户端对文件服务器施加的负载。在这里，我们检查单个客户端处理约 90,000 次系统操作的吞吐量。XenoLinux 表现良好，性能在原生 Linux 的 1% 以内。VMware 和 UML 都显得力不从心，支持的客户端数量不及原生 Linux 系统的三分之一。

SPECWEB99 是一个复杂的应用层基准，用于评估 Web 服务器及其承载的系统。工作负载是请求页面的复杂组合：30% 需要动态内容生成，16% 是 HTTP POST 操作，0.5% 执行 CGI 脚本。随着服务器运行，它生成访问和 POST 日志，因此磁盘负载并不是单纯的只读。因此，测量也反映了操作系统整体性能，包括文件系统和网络。

为了更精确地测量 Xen 和其他 VMM 中的开销区域，我们还进行了许多小型实验，针对特定的子系统。我们调查了 McVoy 的 lmbench 程序 [29] 测量的虚拟化开销。我们使用了版本 3.0-a3，因为它解决了 Seltzer 的 hbench [6] 中提出的工具准确性问题。操作系统性能子集显示了 l 系统的效率。
微基准测试套件包含37个微基准，这是所能支持的最大数量。SPECWEB99定义了模拟用户必须接收的最低服务质量，以便在计分中被视为“符合规范”。在本地Linux的案例中，我们给出了单处理器（L-UP）和对称多处理（L-SMP）内核的数字，因为在许多情况下，用户对SMP系统中由于额外锁定导致的性能开销感到惊讶。

表6：ttcp：带宽（以Mb/s为单位）

表3：lmbench：进程时间（以毫秒为单位）

在微基准测试中，我们发现实际情况下，L-SMP和L-UP的性能差异较小。例如，在某些配置下，由于XenoLinux中的缓存对齐偶然性，性能相似，这突显了过于认真对待微基准测试的潜在风险。

在网络性能的评估中，我们考察了Gigabit以太网局域网下的TCP性能。在所有实验中，我们使用了一个同样配置的SMP机器，该机器运行本地Linux，作为端点之一。这使我们能够独立测量接收和传输性能。通过ttcp基准测试进行这些测量。发送者和接收者应用程序的套接字缓冲区大小配置为128kB，因为我们发现这在所有测试系统中提供了最佳性能。所呈现的结果是9次实验的中位数，所有实验传输400MB。

表6展示了两组结果，一组使用默认以太网MTU为1500字节，另一组使用500字节的MTU（因其通常被拨号PPP客户端使用）。结果表明，XenoLinux虚拟网络驱动程序使用的页面翻转技术避免了数据复制的开销，从而实现了每字节开销非常低。使用500字节的MTU时，逐包开销占主导地位。额外的传输重排和接收解复用的复杂性对吞吐量产生负面影响，但仅影响14%。

在37个微基准中的24个案例中，XenoLinux的表现与本地Linux相似，紧随单处理器Linux内核的性能表现，超过了SMP内核。在表3至表5中，我们展示了结果，显示出测试系统之间有趣的性能变化；特别是对Xen的巨大惩罚以粗体字标出。

在微基准测试（表3）中，Xen在fork、exec和sh性能方面表现得比本地Linux更慢。这是可以预期的，因为这些操作需要大量的页表更新，而这些更新都必须经过Xen的验证。然而，半虚拟化的方法允许XenoLinux批量更新请求。创建新的页表是一个理想的情况：因为没有理由提前提交待处理的更新，XenoLinux可以将每次超调用平均分摊到2048次更新（即其批量缓冲区的最大大小）中。因此，每次更新超调用构建8MB的地址空间。

表4显示了不同工作集大小的进程之间的上下文切换时间。Xen的开销在1（毫秒）到3（毫秒）之间，因为它为每个SPEC WEB99的实例执行一个超调用来更改页表基址。然而，对于更大的工作集大小（也许更能代表真实应用）的上下文切换结果显示，相对于缓存效率而言，开销较小。

本节中，我们比较了在各自的客户操作系统中运行多个应用程序与将它们在同一本地操作系统上运行的性能。我们的重点是Xen的结果，但在适用的情况下，我们也会对其他虚拟机监控程序的性能进行评论。图4显示了在一台双CPU机器上并行运行1、2、4、8和16个SPEC WEB99基准的结果。原生Linux配置为SMP；在其上我们并行运行了多个Apache进程。在Xen的情况下，每个SPEC WEB99的实例都在其自己的单处理器Linux客户操作系统中运行（同时还有一个sshd和其他管理进程）。每个Web服务器使用不同的TCP端口号，以使这些副本能够并行运行。请注意
以下是文本的中文翻译：

异常地，VMware Workstation在这些同时连接所需的SPEC数据集上的表现不如UML（c 0.66），对于1000个连接，所需的内存大小大约为4.88MBytes或3.3GB。这足够大，以充分测试磁盘和缓冲区缓存子系统。

表5展示的mmap延迟和页面错误延迟的结果非常有趣，因为它们需要进入Xen页面的两个过渡：第一个是识别硬件错误并将详细信息传递给来宾操作系统，第二个是在来宾操作系统的基础上安装更新的页表条目。尽管如此，开销仍然相对较小。 

表3中的一个小异常是，XenLinux的信号处理延迟低于原生Linux。该基准测试完全不需要对Xen进行任何调用，0.75（30%）的加速是一个显著的结果。 

在单一Apache实例的情况下，增加一个第二CPU能够使原生Linux在第4.1节报告的得分上提高28%，达到662个合规客户。然而，当运行两个Apache实例时，可以获得最佳的整体吞吐量，这表明Apache 1.3.27可能存在一些SMP可扩展性问题。

当运行单个域时，由于缺乏对SMP来宾操作系统的支持，Xen的性能受到了限制。然而，Xen的中断负载平衡器识别空闲CPU并将所有中断处理转移到该CPU，从而使单个CPU的得分提高了9%。随着域数量的增加，Xen的性能在接近原生情况的情况下有所提升。

接下来，我们进行了一系列实验，运行多个PostgreSQL实例并通过OSDB套件进行测试。在单个Linux操作系统上运行多个PostgreSQL实例 proved困难，因为通常只运行单个PostgreSQL实例来支持多个数据库。然而，这将妨碍不同用户拥有独立的数据库配置。我们不得不使用chroot和软件补丁的组合来避免不同PostgreSQL实例之间的SysVIPC命名空间冲突。相反，Xen允许每个实例在自己的域中启动，从而简化了配置。

在图5中，我们展示了Xen在运行1、2、4和8个OSDB-IR和OSDB-OLTP实例时所获得的总吞吐量。当增加第二个域时，第二CPU的充分利用几乎使整体吞吐量翻倍。进一步增加域的数量虽然造成了整体吞吐量的某些减少，这可以归因于上下文切换和磁盘读头移动的增加。在单个Linux操作系统上运行多个PostgreSQL实例的综合得分比使用Xen的等效得分低25%-35%。原因尚不完全明确，但似乎PostgreSQL在SMP可扩展性方面存在问题，且对Linux块缓存的利用不足。

图5还展示了在8个域之间的性能差异。Xen的调度程序被配置为为每个域分配1到8之间的整数权重。每个域的吞吐量得分反映在条形图的不同带状上。在IR基准测试中，权重对吞吐量有精确的影响，每个段在其预期大小的4%内。 

在OLTP的情况下，获得更多资源份额的域并未获得成比例更高的得分：同步磁盘活动的高水平突显了我们当前磁盘调度算法的不足，导致其表现不佳。

4.4 性能隔离

为了展示Xen提供的性能隔离，我们希望进行一次“对决”，比较Xen和其他基于操作系统的性能隔离技术实现，如资源容器。然而，目前似乎没有基于Linux 2.4的实现可供下载。QLinux 2.4尚未发布，目标在于为多媒体应用提供QoS，而不是在服务器环境中提供完全的防御性隔离。Ensim的基于Linux的私人虚拟服务器产品似乎是最完整的实现， reportedly 包含CPU、磁盘、网络和物理内存资源的控制[14]。我们正在与Ensim进行讨论，希望能在以后的日期报告比较评估的结果。 

在无法并排比较的情况下...
为了比较，我们展示了结果，表明Xen的性能隔离功能在存在恶意负载的情况下仍然按预期工作。我们配置了四个领域，它们具有相等的资源分配，其中两个领域运行之前测量过的负载（PostgreSQL/OSDB-IR和SPEC WEB99），两个领域各自运行一对极具反社会性质的进程。第三个领域同时运行一个磁盘带宽占用者（持续dd）以及一个文件系统密集型负载，目标是大量在大目录内创建小文件。第四个领域在同一时间运行一个“分叉炸弹”，同时运行一个虚拟内存密集型应用程序，该应用程序尝试分配并访问3GB的虚拟内存，并在失败时释放每一页，然后重新启动。

我们发现OSDB-IR和SPEC WEB99的结果仅受到运行破坏性进程的两个领域行为的边际影响——分别比早期报告的结果低4%和2%。我们将此归因于额外上下文切换和缓存效应的开销。我们认为，在配置默认的5ms最大调度“时间片”时，当操作128个同时计算密集型进程在单个服务器上，这在我们的目标应用领域中可能并不常见，但Xen的表现相对较好：在运行128个领域的情况下，我们仅损失了相对于Linux的7.5%的总体吞吐量。

在这种极端负载下，我们测量了与运行SPEC CINT2000子集的一个领域的用户到用户的UDP延迟。我们测得平均响应时间为147ms（标准差97ms）。在针对一个闲置的第129个领域重复实验时，我们记录到了平均响应时间为5.4ms（标准差16ms）。这些数据非常令人鼓舞——尽管背景负载相当大，交互式领域仍然保持响应。

为了确定7.5%性能降低的原因，我们将Xen的调度“时间片”设置为50ms（ESXServer使用的默认值）。结果显示吞吐量曲线紧密跟踪本地Linux，几乎消除了性能差距。然而，如预期的那样，在高负载下，交互性能受到了不利的影响。

这些数据显示，尽管我们当前相对简单的磁盘调度算法可能有些幸运，但在这种情况下，似乎为基准测试提供了足够的隔离，以避免其他领域的页面交换和磁盘密集型活动的干扰。

5. 相关工作  
虚拟化技术在操作系统中应用了近三十年，既用于商业也用于研究。IBM VM/370首次利用虚拟化技术支持遗留代码的二进制兼容。VMware和Connectix都虚拟化了商品PC硬件，允许多个操作系统在单个主机上运行。所有这些示例都实现了对基础硬件（至少是子集）的完全虚拟化，而不是对称地虚拟化并呈现给客户操作系统。

在这一部分，我们将考察Xen在100个领域目标系统上的扩展能力。我们讨论多实例客户操作系统及相关应用程序运行的内存需求，并测量其执行的CPU性能开销。我们评估了在XenoLinux上引导的领域的最小物理内存需求，并运行RH7.2守护进程以及一个sshd和Apache web服务器。该领域在启动时被保留64MB的内存，限制了其最大增长尺寸。客户操作系统被指示通过将所有可能的页面返回给Xen来最小化其内存占用。在没有配置任何交换空间的情况下，该领域能够将其内存占用减少到6.2MB；允许使用交换设备进一步减少到4.2MB。一个静态领域能够维持在这个减少的状态，直到有一个HTTP请求或周期性事件到达。
Here is the translation of the provided text into Chinese, maintaining the academic style and technical terminology:

---

系统。
周期性服务会导致更多的内存需求。在这种情况下，除了 Denali，我们还知道另外两个利用低级虚拟化来构建分布式系统基础设施的努力。vMatrix [1] 项目是基于 VMware 的，旨在构建一个在不同机器之间移动代码的平台。由于 vMatrix 是在 VMware 之上开发的，他们更关注分布的高级问题，而非虚拟化本身的问题。此外，IBM 提供了一种“托管服务”，可以在 IBM 大型机上租用虚拟 Linux 实例。PlanetLab [33] 项目构建了一个分布式基础设施，旨在为地理分布式网络服务的研究和开发提供实验平台。该平台针对研究人员，试图将单个物理主机划分为小块，为所有用户提供同时的低级访问。当前的部署使用 VServers [17] 和 SILK [4] 来管理操作系统内部的资源共享。

我们与操作系统扩展性和主动网络社区有一些相同的动机。然而，在 Xen 上运行时，无需检查“安全”代码，也不需要保证终止（—无论哪种情况，受损的唯一一方是相关客户端）。因此，Xen 提供了一个更通用的解决方案：没有必要将托管代码数字签名由受信任的编译器（如 SPIN [5]），或伴随安全证明（如 PCC [31]），以特定语言编写（如在 SafetyNet [22] 或任何基于 Java 的系统中），或依赖于特定中间件（如移动代理系统）。这些其他技术当然可以继续在运行于 Xen 的客操作系统中使用。这对于具有更多瞬态任务的工作负载特别有用，因为它们不会提供机会来摊销启动新域的成本。

对于语言级虚拟机方法也可以提出类似的论点：虽然资源管理的 JVM [9] 当然能够托管不可信的应用程序，但这些应用程序必须被编译为 Java 字节码，并遵循该特定系统的安全模型。与此同时，Xen 可以轻松支持作为运行在客操作系统上的应用程序的语言级虚拟机。

6.2 结论
Xen 为部署各种网络中心服务提供了一个优秀的平台，如动态网页内容的本地镜像、媒体流的转码和分发、多人游戏和虚拟现实服务器，以及提供对瞬时连接设备的轻量级网络存在的“智能代理” [2]。Xen 直接解决了部署这种服务的最大障碍：当前无法在短期内以低启动成本托管瞬时服务器。通过允许 100 个操作系统在单一服务器上运行，我们将相关成本降低了两个数量级。此外，通过将每个操作系统的设置和配置转变为软件问题，我们促进了更小粒度的托管时间尺度。

6. 讨论与结论
我们展示了 Xen 内核，它将计算机的资源在运行来宾操作系统的域之间进行分配。我们的半虚拟化设计特别强调性能和资源管理。我们还描述并评估了 XenoLinux，一个完整特性的 Linux 2.4 内核的移植，运行于 Xen 上。我们正在进行的工作是将 BSD 和 Windows XP 内核移植到 Xen 上，这证实了 Xen 所暴露的接口的通用性。

6.1 未来工作
我们认为 Xen 和 XenoLinux 是足够完善的，以...

---

请注意，文本被翻译为中文的同时，保持了原有的学术和技术术语的准确性。
致谢  
本工作旨在使更广泛的受众受益，因此计划在不久的将来公开发布我们的软件。一个测试版本已经在选定的相关方进行评估；一旦这一阶段完成，我们将在我们的项目页面上宣布1.0版本的正式发布。  
这一工作得到了ESPCR资助（拨款编号GR/S01894/01）和微软公司的支持。我们要感谢Evangelos Kotsovinos、Anil Madhavapeddy、Russ Ross和James Scott的贡献。  

在初始发布之后，我们计划对Xen进行多项扩展和改进。为了提高虚拟块设备的效率，我们打算实施一个基于块内容索引的共享通用缓冲缓存。这将为我们的设计增加受控的数据共享，而不牺牲隔离性。为虚拟块设备添加写时复制语义将允许它们在不同的域之间安全地共享，同时仍允许不同的文件系统共存。  

为了提供更好的物理内存性能，我们计划实施一个最后机会页面缓存（LPC），它有效地作为一个系统级的自由页面列表，仅在机器内存不足时使用。LPC将在来宾操作系统的虚拟内存系统选择逐出一个干净页面时使用；而不是完全丢弃该页面，它可以被添加到自由页面列表的尾部。发生在该页面重新分配之前的故障，可以因此在不进行磁盘访问的情况下得以解决。  

Xen的一个重要作用是作为XenoServer项目的基础，该项目超越了单个机器，构建支持互联网规模计算基础设施所需的控制系统。我们设计的关键思想是，资源使用应被精确计算，并由该工作的赞助者支付——如果以真实现金支付，我们可以采用拥塞定价策略来处理超额需求，并利用超额收入支付额外的机器。这需要准确和及时的I/O调度，以对抗敌意负载。我们还计划通过为虚拟块设备创建租约，将会计纳入我们的块存储体系结构中。  

为了更好地支持XenoServers的管理和行政工作，我们正在整合更全面的审计和取证日志支持。我们也在开发相关功能。  

参考文献  
[1] A. Awadallah 和 M. Rosenblum. The vMatrix: A network of virtual machine monitors for dynamic content distribution. 在第七届国际网络内容缓存与分发研讨会（WCW2002）中，2002年8月。  
[2] A. Bakre 和 B. R. Badrinath. I-TCP: indirect TCP for mobile hosts. 在第十五届国际分布式计算系统会议（ICDCS1995）中，第136-143页，1995年6月。  
[3] G. Banga, P. Druschel, 和 J. C. Mogul. Resource containers: A new facility for resource management in server systems. 在第三届操作系统设计与实现研讨会（OSDI1999）中，第45-58页，1999年2月。  
[4] A. Bavier, T. Voigt, M. Wawrzoniak, L. Peterson, 和 P. Gunningberg. SILK: Scout paths in the Linux kernel. 技术报告2002-009，乌普萨拉大学信息技术系，2002年2月。  
[5] B. N. Bershad, S. Savage, P. Pardyak, E. G. Sirer, M. Fiuczynski, D. Becker, S. Eggers, 和 C. Chambers. Extensibility, safety and performance in the SPIN operating system. 在第十五届ACM操作系统原则研讨会（SIGOPS）中，ACM操作系统评论第29卷第5期，第267-284页，1995年12月。  
[6] A. Brown 和 M. Seltzer. Operating System Benchmarking in the Wake of Lmbench: A Case Study of the Performance of NetBSD on the Intel x86 Architecture. 在1997年ACM SIGMETRICS测量与建模计算机系统会议中，1997年6月。  
[7] E. Bugnion, S. Devine, K. Govil, 和 M. Rosenblum. Disco: Running commodity operating systems on scalable multiprocessors. 在第十六届ACM操作系统原则研讨会（SIGOPS）中，ACM操作系统评论第31卷第5期，第143-156页，1997年10月。  
[8] Connectix. Product Overview: Connectix Virtual Server, 2003. http://www.connectix.com/products/vs.html.  
[9] G. Czajkowski 和 L. Dayneś. Multitasking without compromise: a virtual machine evolution. ACM SIGPLAN Notices, 36(11):125-138, 2001年11月。  
[10] S. Devine, E. Bugnion, 和 M. Rosenblum. Virtualization system including a virtual machine monitor for a computer with a segmented architecture. 美国专利6397242，1998年10月。  
[11] K. J. Duda 和 D. R. Cheriton. Borrowed-Virtual-Time (BVT) scheduling: supporting latency-sensitive threads in a general-purpose scheduler. 在第十七届ACM操作系统原则研讨会（SIGOPS）中。
以下是将文本翻译成中文的结果：

Proceedings of the 5th
Operating Systems Principles，ACM Operating Systems Design and Implementation (OSDI) 研讨会，卷33(5)，页面261-276，Kiawah Island Resort，南卡罗来纳州，美国，2002年。ACM Operating Systems Review，冬季特别刊，1999年12月，页面89-104，马萨诸塞州波士顿，美国，2002年12月。
[12] G.W. Dunlap、S.T. King、S. Cinar、M. Basrai 及 P.M. Chen。 [31] G.C. Necula。带证明的代码。在POPL 1997会议记录中：第24届ACM SIGPLAN-SIGACT编程语言原理研讨会，页面106-119，1997年1月。
ReVirt：通过虚拟机日志和重放来启用入侵分析。在第五届操作系统设计与实施研讨会（OSDI 2002）论文集中，ACM Operating Systems Review，冬季特别刊，页面211-224，波士顿，马萨诸塞州，美国，2002年12月。
[13] D. Engler、S.K. Gupta 和 F. Kaashoek。AVM：应用层虚拟内存。在第五届热议操作系统研讨会的论文集中，页面72-77，1995年5月。
[14] Ensim。Ensim 虚拟专用服务器，2003年。http://www.ensim.com/products/materials/datasheet_vps_051003.pdf。
[15] K.A. Fraser、S.M. Hand、T.L. Harris、I.M. Leslie 和 I.A. Pratt。Xenoserver 计算基础设施。技术报告 UCAM-CL-TR-552，剑桥大学计算机实验室，2003年1月。
[16] T. Garfinkel、M. Rosenblum 及 D. Boneh。灵活的操作系统支持和用于可信计算的应用。在第9届热议操作系统研讨会论文集中，夏威夷考艾岛，2003年5月。
[17] J. Gelinas。虚拟专用服务器和安全上下文，2003年。http://www.solucorp.qc.ca/miscprj/s_context.hc，2000年8月。
[18] K. Govil、D. Teodosiu、Y. Huang 及 M. Rosenblum。Cellular Disco：在共享内存多处理器上使用虚拟集群进行资源管理。在第17届ACM SIGOPS操作系统原则研讨会的论文集中，卷33(5)，ACM Operating Systems Review，页面154-169，1999年12月。
[19] P.H. Gum。System/370扩展架构：虚拟机的设施。IBM研究与开发杂志，27(6)：530-544，1983年11月。
[20] S. Hand。Nemesis 操作系统中的自分页。在第3届操作系统设计和实施研讨会的论文集中，页面73-86，1999年10月。
[21] S. Hand、T.L. Harris、E. Kotsovinos 和 I. Pratt。控制XenoServer开放平台，2003年4月。
[22] A. Jeffrey 和 I. Wakeman。主动网络的语义技术调查，1997年11月。http://www.cogs.susx.ac.uk/projects/safetynet/。
[23] M.F. Kaashoek、D.R. Engler、G.R. Granger、H.M. Bricenço、R. Hunt、D. Mazières、T. Pinckney、R. Grimm、J. Jannotti 和 K. Mackenzie。Exokernel系统中的应用性能和灵活性。在第16届ACM SIGOPS 操作系统原则研讨会的论文集中，卷31(5)，ACM Operating Systems Review，页面52-65，1997年10月。
[24] R. Kessler 和 M. Hill。在大范围实索引缓存中的页面放置算法。ACM计算机系统交易，发布时间：27(6):530-544，1983年11月。
10(4):338-359, 1992年11月。[43] A. Whitaker, M. Shaw 和 S. D. Gribble。Denali：轻量级虚拟机支持分布式和网络应用的操作系统。在2003年美国USENIX年度技术会议的会议记录中，华盛顿大学，2002年6月。[44] A. Whitaker, M. Shaw 和 S. D. Gribble。Denali隔离内核中的规模和性能。[25] S. T. King, G. W. Dunlap 和 P. M. Chen。针对虚拟机的操作系统支持。在信息技术系统设计与实现的第五届研讨会（OSDI 2002）会议记录中，纽约州卡利库，2002年6月。[26] M. Kozuch 和 M. Satyanarayanan。互联网挂起/恢复。[27] I. M. Leslie, D. McAuley, R. Black, T. Roscoe, P. Barham, D. Evers。操作系统评论，2002年冬季特刊，页面195-210，美国马萨诸塞州波士顿，2002年12月。